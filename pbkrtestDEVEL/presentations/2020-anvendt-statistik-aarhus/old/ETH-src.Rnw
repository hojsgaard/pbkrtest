
<<include=FALSE>>=
library(knitr)
options(warn=-1)
opts_chunk$set(
fig.path='fig/eth-', tidy=FALSE, fig.height=4, size="tiny"
)
@

\def\R{\texttt{R}}
\def\pkg#1{{\bf #1}}

% \section{Outline}
% \label{sec:xxx}


\section{History}
\label{sec:history}

\begin{sframe}
\frametitle{Some history}
  \begin{itemize}
  \item Years ago, Ulrich Halekoh and SH colleagues at ``Danish Institute for Agricultural Sciences''
  \item That was SAS-country back then
  \item Many studies called for random effects models - and for \texttt{PROC MIXED}
  \item \texttt{PROC MIXED} reports (by default) $p$--values from asymptotic likelihood ratio test. 
  \item Main concern: Effects should be ``tested against'' the correct variance
    component in order not to make effects appear more significant than they
    really are.
  \end{itemize}
\end{sframe}

\begin{sframe}
\frametitle{Then \R\ came along...}
\begin{itemize}
  \item Common advice: Use Satterthwaite or Kenward-Roger approximation of denominator
    degrees of freedom in $F$-test -- in an attempt not to get things ``too wrong''.
\item Then \R\ came along; we advocated the use of \R.  
\item Random effects models were fitted with the \pkg{nlme} package -- but there
  was no Satterthwaite or Kenward-Roger approximation, so our common advice fell
  apart.
\end{itemize}
\end{sframe}

\subsection{The degree of freedom police...	}
\label{sec:degr-freed-polic}

\begin{sframe}{asdasd}
\frametitle{The degree of freedom police}

R-help - 2006: \texttt{[R] how calculation degrees freedom}

\url{https://stat.ethz.ch/pipermail/r-help/2006-January/087013.html}

SH: Along similar lines ...  probably in recognition of the degree of freedom problem. It could be nice, however, if anova() produced ... 

Doug Bates: I don't think the "degrees of freedom police" would find that to be a
suitable compromise. :-)

In reply to another question:

Doug Bates: I will defer to any of the "degrees of freedom police" who post to
this list to give you an explanation of why there should be different
degrees of freedom. 

\end{sframe}





\subsection{Motivation: Sugar beets - A split--plot experiment}
\label{sec:xxx}

\begin{sframe}
 \begin{itemize}
 \item Model how sugar percentage in sugar beets depends on
   harvest time and sowing time.
 \item Five sowing times ($s$) and two harvesting times ($h$).
 \item Experiment was laid out in three blocks ($b$).
 \end{itemize}

{\tiny
\begin{verbatim}
Experimental plan for sugar beets experiment

Sowing times:
 1: 4/4, 2: 12/4, 3: 21/4, 4: 29/4, 5: 18/5

Harvest times:
 1: 2/10, 2: 21/10

Plot allocation:
      |  Block 1           |  Block 2           |  Block 3           |
      +--------------------|--------------------|--------------------+
Plot  | h1  h1  h1  h1  h1 | h2  h2  h2  h2  h2 | h1  h1  h1  h1  h1 | Harvest time
1-15  | s3  s4  s5  s2  s1 | s3  s2  s4  s5  s1 | s5  s2  s3  s4  s1 | Sowing time
      |--------------------|--------------------|--------------------|
Plot  | h2  h2  h2  h2  h2 | h1  h1  h1  h1  h1 | h2  h2  h2  h2  h2 | Harvest time
16-30 | s2  s1  s5  s4  s3 | s4  s1  s3  s2  s5 | s1  s4  s3  s2  s5 | Sowing time
      +--------------------|--------------------|--------------------+
\end{verbatim}
}
\end{sframe}

\begin{sframe}
<< >>= 
data(beets, package='pbkrtest')
head(beets)
library(doBy)
library(lme4)
@    
\end{sframe}

\begin{sframe}

<< >>= 
par(mfrow=c(1,2))
with(beets, interaction.plot(sow, harvest, sugpct))
with(beets, interaction.plot(sow, harvest, yield))
@  
\end{sframe}


\begin{sframe}

% Let $h$ denote harvest time ($h=1,2$), $b$ denote block
% ($b=1,2,3$) and $s$ denote sowing time ($s=1,\dots,5$). Let $H=2$,
% $B=3$ and $S=5$.

  \begin{itemize}
  \item 
    For simplicity we assume that there is no interaction between sowing
    and harvesting times.

  \item A typical model for such an experiment would be:
    \begin{equation}
      \label{eq:beetsmodel1}
      y_{hbs} = \mu + \alpha_h + \beta_b + \gamma_s + U_{hb} + \epsilon_{hbs},
    \end{equation}
    where $U_{hb} \sim N(0,\omega^2)$ and $\epsilon_{hbs}\sim
    N(0,\sigma^2)$.
    

  \item Notice that $U_{hb}$ describes the random variation
    between whole--plots (within blocks).

  \end{itemize}

\end{sframe}



\begin{sframe}
As the design is balanced we may make F--tests for each of the effects
as:
<<>>=
beets$bh <- with(beets, interaction(block, harvest))
summary(aov(sugpct ~ block + sow + harvest + 
                Error(bh), data=beets))
@ %def

Notice: the F--statistics are $F_{1,2}$ for harvest time and $F_{4,20}$ for
sowing time.
\end{sframe}

\begin{sframe}

Using \code{lmer()} from \pkg{lme4} we can
fit the models and test for no effect of sowing and harvest time as follows:

<<echo=F>>=
options("digits"=4)
options("show.signif.stars"=F)
@ %def

<<tidy=FALSE>>=
beetLarge <- lmer(sugpct ~ block + sow + harvest + 
                      (1 | block:harvest), data=beets, REML=FALSE)
beet_no.harv <- update(beetLarge, .~. - harvest)
beet_no.sow  <- update(beetLarge, .~. - sow)
@ %def

\end{sframe}

\begin{sff}

The LRT based $p$--values are anti--conservative: the effect
of harvest appears stronger than it is.
<<>>= 
anova(beetLarge, beet_no.sow)  %>% as.data.frame
anova(beetLarge, beet_no.harv)  %>% as.data.frame
@
  
\end{sff}


\subsection{Motivation: A random regression problem}
\begin{sframe}
  \frametitle{Random coefficient model}

  The change with age  of the distance between two cranial
distances was
observed for 16 boys and 11 girls
from age 8 until age 14.
<<echo=FALSE>>=
library(lattice)
data(Orthodont,package='nlme')
pdf(file='fig/ortfig01.pdf',width=8,height=8/1.7)
xyplot(distance~age|Sex, group=Subject, data=Orthodont,type='l')
graphics.off()
@ %def

\begin{figure}[ht]
 \centering
 \includegraphics[width=0.7\textwidth]{fig/ortfig01}\\[-0.5cm]
%%\caption{Orthodont data}
 \label{fig:ortfig01}
\end{figure}
\end{sframe}

\begin{sframe}
  \frametitle{Random coefficient model}
  Plot suggests:
\[
dist_{[i]} = \alpha_{sex[i]}
+ \beta_{sex[i]} age_{[i]} + A_{Subj[i]} +
B_{Subj[i]} age_{[i]} + e_{[i]}
\]
with $(A,B) \sim N(0,\bm S)$.

ML-test of $\beta_{boy}=\beta_{girl}$:
<<tidy=FALSE>>=
ort1ML<- lmer(distance ~ age + Sex + age:Sex + (1 + age | Subject),
                  REML = FALSE, data=Orthodont)
ort2ML<- update(ort1ML, .~. - age:Sex)
as.data.frame(anova(ort1ML, ort2ML))
@
\end{sframe}


\subsection{Our contribtion}
\label{sec:packages-we-need}



\subsection{Our goal}
\label{sec:our-goal}

\begin{sframe}
\frametitle{Our goal...}
Our goal is to extend the tests provided by \lmer.

There are two issues here:

\begin{itemize}
\item  The choice of test statistic and
  
\item  The reference distribution in which the test statistic is evaluated.
  
\end{itemize}

Implement Kenward-Roger approximation.

Implement parametric bootstrap.

Implement Satterthwaite approximation (not yet released)

\end{sframe}


<<echo=F>>=
library(pbkrtest) ## The works
library(broom)    ## tidy, glance, ...
library(magrittr) ## %>% - the pipe
@  



\section{The Kenward--Roger approach}

\subsection{The Kenward--Roger modification of the $F$--statistic}

\begin{sframe}
\frametitle{Setting the scene}
For multivariate normal data
\[
  Y_{n\times 1} \sim  N(\bm X_{n\times p} \bm \beta_{p\times 1}, \bm \Sigma)
\]
we consider the test of the hypothesis
\[
  \Lb_{d \times p} \betab = \bm \beta_0
\]
where $\Lb$ is a regular matrix of estimable functions of $\bm \beta$.

\def\transp{^{\top}}
\def\inv{^{-1}}

With $\hat \betab \sim N_d(\betab, \bm\Phi)$, a Wald statistic for testing $\Lb \betab = \betab_0$ is
\begin{displaymath}
  W = [\Lb(\hat\betab - \betab_0)]\transp [L\bm\Phi L\transp]\inv [\Lb(\hat\betab - \betab_0)]
\end{displaymath}
which is asymptotically $W \sim \chi^2_d$ under the null hypothesis. 
\end{sframe}

\begin{sframe}
  A scaled version of $W$ is 
\begin{displaymath}
  F = \frac{1}{d} W
\end{displaymath}
which is asymptotically $F \sim \frac{1}{d} \chi^2_d$
under the null hypothesis -- which we can think of as the limiting distribution
of an $F_{d,m}$--distribution as $m\rightarrow \infty$

To account for the fact that $\bm\Phi$
is estimated from data, we must come up with a better estimate of the
denominator degrees of freedom $m$ (better than $m=\infty$).

That was what Kenward and Roger worked on... 
\end{sframe}

\begin{sframe}
  \frametitle{Kenward and Roger's setting}
The linear hypothesis $\Lb \betab = \betab_0$ 
can be tested via the  Wald-type  statistic
\begin{gather*}
F= \frac{1}{r}(\hat \betab - \betab_0)^\top \Lb^\top   (\Lb^\top \bm \Phi(\ssb) \Lb)^{-1}
 \Lb (\hat \betab - \betab_0)
\end{gather*}
\begin{itemize}
\item
$\bm \Phi (\sigmab) = (\bm X^\top \bm \Sigma(\sigmab) \bm X)^{-1} \approx
\cov(\hat \betab)$, $\hat  \betab$ REML estimate of $\betab$ 
\item
$\ssb$: vector of REML estimates of the elements of $\Sigmab$
\end{itemize}
\end{sframe}

\begin{sframe}
  \frametitle{Kenward and Roger's modification}
Kenward and Roger (1997) modify the test statistic
\begin{itemize}
\item
$\bm \Phi$ is replaced by an improved small sample approximation $\bm \Phi_A$
\end{itemize}
Furthermore
\begin{itemize}
\item   
the statistic $F$ is scaled by a factor $\lambda$,
\item
denominator degrees of freedom $m$ are determined
\end{itemize}
such that the approximate expectation and variance are those of a $F_{d,m}$ distribution.
\end{sframe}


\begin{sframe}
\frametitle{Restriction on covariance}
\begin{itemize}

\item 
Consider only situations where 
\begin{gather*}
\Sigmab= \sum_i \sigma_i \bm G_i, \quad \bm G_i \, \text{known matrices}
\end{gather*}


\item Variance component and random coefficient models satisfy this
restriction.


\item $\bm \Phi_A(\ssb)$  depends now only on the first  partial derivatives of $\bm \Sigma^{-1}$:
\begin{displaymath}
\frac{\partial \bm \Sigmab^{-1}}{\partial \sigma_i} =
-
\bm \Sigma^{-1}
\frac{\partial \bm \Sigmab}{\partial \sigma_i}
\bm \Sigma^{-1}.  
\end{displaymath}



\item $\bm \Phi_A(\ssb)$  depends also on $\var(\ssb)$.


\item Kenward and Roger propose to estimate
  $\var(\ssb)$ via the  inverse expected information matrix.

\end{itemize}

\end{sframe}

 

\begin{sframe}
  \frametitle{Properties of the Kenward--Roger adjustment}
The  modification of the F-statistic by Kenward and Roger
\begin{itemize}
\item
yields the exact F-statistic for balanced mixed classification nested models
or balanced split plot models (Alnosaier, 2007).
\item
Simulation studies (e.g. Spilke, J. et al.(2003)) indicate that the Kenward-Roger approach perform  mostly better than alternatives  (like Satterthwaite  or containment method) for blocked experiments even with missing data. 
\end{itemize}
\end{sframe}



\begin{sframe}
  \frametitle{{\R\ packages \pkg{lme4}} and \pkg{pbkrtest}}

  \pkg{lme4} (Bates, D., Maechler, M,  Bolker, B., Walker, S. 2014)
provides efficient estimation of linear mixed models.

\pkg{lme4} provides most  matrices and estimates needed 
to  implement a Kenward-Roger approach.

\pkg{pbkrtest} (Halekoh, U.,   H{\o}jsgaard, S., 2014)
provides a ``straight forward'' transcription of the
description in the article of Kenward and Roger,  1997.

%%
%%enumerate}
%%
%%lementation uses a  
%%
%%operations use sparse matrices representation.
%%
%%s are extracted from \verb+lmer+ objects via their slots (using \verb+@+).
%%umerate}
%%
\end{sframe}

 




\begin{sframe}
  \frametitle{Kenward--Roger: split-plot (sugar-beets)}
The Kenward--Roger approach yields the same results
as the anova-test:

<<>>=
beetLarge <- update(beetLarge, REML=TRUE)
beet_no.harv <- update(beet_no.harv, REML=TRUE)
@
Test for harvest effect:
<<>>=
KRmodcomp(beetLarge, beet_no.harv)
@
\end{sframe}



\begin{sframe}
  \frametitle{Kenward--Roger:  random regression (cranial change)}
For the cranial distances  data the
Kenward and Roger modified F-test yields
<<>>=
formula(ort1ML)
formula(ort2ML)
ort1<- update(ort1ML, .~., REML = TRUE)
ort2<- update(ort2ML, .~., REML = TRUE)
@ %def
\end{sframe}

\begin{sframe}
<< >>= 
KRmodcomp(ort1, ort2)
@  
The p-value form the  $\chi^2$-test was 0.0249.

\end{sframe}


\subsection{Shortcommings of Kenward-Roger}
\begin{sframe}
\frametitle{Shortcommings of Kenward-Roger}

\begin{itemize}
\item The Kenward--Roger approach is no panacea.
\item In the computations of the degrees of freedom we need to compute
  \begin{displaymath}
    G_j {\bm \Sigma}^{-1} G_j
  \end{displaymath}
  where $\Sigmab= \sum_i \sigma_i \bm G_i$. 
  Can be space and time consuming!
\item An alternative is a Sattherthwaite--kind approximation which is faster to
  compute. Will come out in next release of \pkg{pbkrtest} (code not tested yet). 
  Way faster...

\item What to do with generalized linear mixed models -- or even
  with generalized linear models.
  
\item 
  \pkg{pbkrtest} also provides the parametric bootstrap $p$-value. 
  
  Computationally somewhat demanding, but can be parallelized. 
\end{itemize}

  
\end{sframe}


\section{Parametric bootstrap}
\label{sec:parametric-bootstrap}
\begin{sframe}
\frametitle{Using parametric bootstrap}

We have two competing models; a large model $f_1(y; \theta)$
and a null model $f_0(y; \theta_0)$; the null model is a submodel of the large model. 

<< >>= 
lg <- update(beetLarge, REML=FALSE)
sm <- update(beet_no.harv, REML=FALSE)
t.obs <- 2*(logLik(lg)-logLik(sm))
t.obs
@

Idea is simple: Draw $B$ parametric bootstrap samples
$t^1, \dots, t^B$ under the fitted null model $\hat \theta_0$. 

That is; simulate $B$
datasets from the fitted null model; fit the large and the null model to each of
these datasets; calculate the LR-test statistic for each simulated data:
\end{sframe}

\begin{sframe}

<<cache=TRUE >>=
set.seed(121315)
t.sim <- PBrefdist(lg, sm, nsim=500)
@
  
The $p$-value
is the fraction of simulated test statistics that are larger or equal to the
observed one:

<< >>= 
head(t.sim)
sum( t.sim >= t.obs ) / length( t.sim )
@
\end{sframe}

\begin{sframe}

  Interesting to overlay limiting $\chi^2_1$
  distribution and simulated reference distribution:
  
<< >>= 
hist(t.sim, breaks=20, prob=T)
abline(v=t.obs, col="red", lwd=3)
f <- function(x){dchisq(x, df=1)}
curve(f, 0, 20, add=TRUE, col="green", lwd=2)
@
  
\end{sframe}


\begin{sframe}
Do the same for sowing time:

<< >>= 
lg <- update(beetLarge, REML=FALSE)
sm <- update(beet_no.sow, REML=FALSE)
t.obs <- 2*(logLik(lg)-logLik(sm))
t.obs
@


<<cache=TRUE >>=
set.seed(121315)
t.sim <- PBrefdist(lg, sm, nsim=500)
@

  
\end{sframe}

\begin{sframe}

  Interesting to overlay limiting $\chi^2_1$
  distribution and simulated reference distribution:
  
<< >>= 
hist(t.sim, breaks=20, prob=T)
abline(v=t.obs, col="red", lwd=3)
f <- function(x){dchisq(x, df=4)}
curve(f, 0, 20, add=TRUE, col="green", lwd=2)
@
  
\end{sframe}








\begin{sframe}

  This scheme is implemented as:

\begin{sblock}
<<cache=TRUE >>=
set.seed(121315)
pb <- PBmodcomp(beetLarge, beet_no.harv)
pb
@  
\end{sblock}  
\end{sframe}

\begin{sframe}

In addition we can get $p$-values
  
  \begin{enumerate}
\item
directly via the proportion of sampled $t_i$ exceeding $t_{obs}$, 
\item
  approximating the distribution of the scaled statistic $\frac{f}{\bar t}\cdot T$ by
  a  $\chi^2_f$ distribution (Bartlett type correction)\\
  ($\bar t$ is the sample average  and $f$ the difference in the number of parameters
  between the null and the alternative model)  
\item
  approximating the bootstrap distribution by a $\Gamma(\alpha,\beta)$ distribution
  which mean and variance match the moments of the bootstrap sample.
\item 
  approximating the bootstrap distribution by a $F_{d,m}$ distribution
  which mean is based on matching mean of  the bootstrap sample.
\end{enumerate}

\end{sframe}


\begin{sframe}
<< >>= 
summary(pb)
@  
  \end{sframe}


\subsection{Parallel computations}

\begin{sframe}
\frametitle{Parallel computations}

Parametric bootstrap is computationally demanding, but multiple cores can be exploited:

<<echo=F >>= 
library(parallel) 
nc <- detectCores()
if (!exists("clus"))
    clus <- makeCluster(rep("localhost", nc))
@
  
<< >>= 
library(parallel)
nc <- detectCores()
nc
@

<<eval=F >>= 
clus <- makeCluster(rep("localhost", nc))
@

\end{sframe}



\begin{sframe}
\begin{sblock}
<<cache=TRUE >>=
set.seed(121315)
pb1 <- PBmodcomp(beetLarge, beet_no.harv)
pb1
pb2 <- PBmodcomp(beetLarge, beet_no.harv, cl=clus)
pb2
@  
\end{sblock}  
\end{sframe}



\begin{sframe}
Results from sugar beets:
\input{pvalbeets.txt}

Results for cranial distance data:
\input{pvalorto.txt}
\end{sframe}


\begin{sframe}

  The above approaches are computationally intensive but there are
  possibilities for speedups:

  Instead of simulating a fixed number of values $t^1, \dots, t^M$ for
  determining the reference distribution used for finding $p^{PB}$
  we may instead introduce a stopping rule saying \emph{simulate until we
  have found, say 20 values $t^j$ larger than $t_{obs}$.} If $J$
  simulations are made then the reported $p$--value is $20/J$.

  Estimating tail--probabilities will require more samples than
  estimating the mean (and variance) of the reference
  distribution. Therefore the Bartlett and gamma approaches will
  require fewer simulations than needed for finding $p^{PB}$.

  The simulation of the reference distribution can be parallelized
  onto different processors.

\end{sframe}



\section{Small simulation study: A random regression problem}
\begin{sframe}
  \frametitle{Random coefficient model}
We consider the simulation from  a simple random coefficient model
(cf. Kenward  and Roger (1997, table 4)):
\begin{gather*}
y_{it}= \beta_0 + \beta_1 \cdot t_i  + A_{i} + B_i \cdot t_i + \epsilon_{it}
\end{gather*}
with $cov(A_i,B_{i})=
\left[
\begin{array}{rr}
 0.250 & -0.133\\
-0.133 &  0.250
\end{array}
\right] $
and $var(\epsilon_{it})=0.25$.

There are observed $i=1,\dots,24$ subjects divided in groups of 8.
For each group observations are
at the non overlapping times $t=0, 1, 2; t=3, 4, 5$ and $t=6, 7, 8$.
\end{sframe}



<<echo=F,results='hide'>>=
#generates the follow table
source('pvals-randcofSim.R')
@

\begin{sframe}
  \frametitle{Results from random coefficient model}
\input{pvalrandcof2.txt}
\end{sframe}
%% 
%% \section{Parametric bootstrap in generalized linear models}
%% \label{sec:param-bootstr-gener}
%% 
%% \begin{sframe}
%%   \frametitle{Parametric bootstrap and \texttt{glm()}}
%% << >>= 
%% data(lizard, package="gRbase")
%% lizard
%% lizard[1,1,1] <- 100
%% @
%% \end{sframe}
%% 
%% \begin{sframe}
%% 
%% Conditional independence between $D$ and $H$ given $S$: 
%% Independence between $D$ and $H$ for each value of $S$.
%% 
%% Usual tests (based on large sample asymptotics)
%% 
%% << >>= 
%% s1 <- chisq.test(lizard[ , , 1]); s1
%% s2 <- chisq.test(lizard[ , , 2]); s2
%% s1
%% s2
%% @
%% 
%% Simultaneous test 
%% << >>= 
%% 1-pchisq( s1$statistic + s2$statistic, df=2)
%% @
%% 
%% 
%% \end{sframe}
%% 
%% 
%% \begin{sframe}
%% Alternative based 
%%   
%% Log--linear model
%% \begin{eqnarray}
%%   \label{eq:1}
%%   log p(d,h,s) = \alpha(d,s) + \beta(h,s)
%% \end{eqnarray}
%% 
%%   
%% \end{sframe}
%% 
%% 
%% \begin{sframe}
%% << >>= 
%% lizardDF <- as.data.frame(lizard)
%% head(lizardDF, 4)
%% m1 <- glm(Freq ~ diam:height:species, family=poisson, data=lizardDF)
%% m0 <- glm(Freq ~ diam:species + height:species + diam:height, family=poisson, data=lizardDF)
%% @  
%%   
%% \end{sframe}
%% 
%% \begin{sframe}
%%  
%% <<cache=F >>= 
%% anova(m1, m0, test="Chisq")
%% pb <- PBmodcomp(m1, m0)
%% pb
%% @
%% 
%% \end{sframe}
%% 
%% \begin{sframe}
%% << >>= 
%% summary(pb)
%% @
%%   
%% \end{sframe}
%% 
%% \begin{sframe}
%% << >>= 
%% lizardDF2 <- lizardDF
%% lizardDF2$Freq <- round(lizardDF2$Freq/10)
%% m12 <- glm(Freq ~ diam:height:species, family=poisson, data=lizardDF2)
%% m02 <- glm(Freq ~ diam:species + height:species  + diam:height, family=poisson, data=lizardDF2)
%% @  
%% \end{sframe}
%% 
%% \begin{sframe}
%% <<cache=F >>= 
%% anova(m12, m02, test="Chisq")
%% pb <- PBmodcomp(m12, m02)
%% pb
%% @
%% 
%% 
%%   
%% \end{sframe}
%% 


\section{Final remarks}
\label{sec:final-remarks}

\begin{sframe}
  \frametitle{Summary}

  \begin{itemize}
  \item   The functions \verb'KRmodcomp()' and \verb'PBmodcomp()' 
    described here are available in the \verb+pbkrtest+ package.

    
  \item The Kenward--Roger approach requires fitting by REML; the parametric
  bootstrap approaches requires fitting by ML.


\item The required fitting scheme is set by the relevant functions, so the
  user needs not worry about this.


\item Parametric bootstrap is parallelized using the \verb'snow'
  package. 
  
  \end{itemize}
  

\end{sframe}


\begin{sframe}
  \frametitle{Literature}

\begin{itemize}
\item Halekoh, U., H{\o}jsgaard, S. (2014)
\textit{ 	A Kenward-Roger Approximation and Parametric Bootstrap Methods for Tests in Linear Mixed Models The R Package pbkrtest}
\item
Alnosaier, W. (2007) 
\textit{Kenward-Roger Approximate F Test for Fixed Effects in Mixed Linear Models}, Dissertation, Oregon State University  
\item
Bates, D., Maechler, M.  and Bolker, B., Walker, S. (2015)
\textit{lme4: Linear mixed-effects models using S4 classes},
R package version 0.999375-39.
\item
Kenward,  M. G. and Roger, J. H. (1997)
\textit{Small Sample Inference for Fixed Effects from Restricted Maximum Likelihood},
Biometrics, Vol. 53, pp. 983--997                    
\item
Spilke J., Piepho, H.-P. and Hu, X. Hu (2005)
\textit{A Simulation Study on Tests of Hypotheses
and Confidence Intervals for Fixed Effects in
Mixed Models for Blocked Experiments With
Missing Data}
Journal of Agricultural, Biological, and Environmental Statistics,
Vol. 10,p. 374-389
\end{itemize}
\end{sframe}

