In general the paper and the pbkrtest package are clearly written and
an excellent addition to the R ecosystem. 

Most of my comments are saved as notes on the PDF.  Here are some more
general points. 

  STATISTICAL:

 * What bias or other artifacts might be induced by discarding PB
 * samples on which lmer fails to converge? 
 * It surprises me that the PB results are _less_ accurate than the KR
 * results.  I would have expected that the PB results would be more
 * or less exactly right, for sufficiently large sample sizes, but in
 * general (according to Table 1) KR is closer to the nominal value. 
 * would it be possible to extend the usage of PB to compute confidence intervals and tests of specific contrasts, or is this too broad a scope for the package?

  COSMETIC/PACKAGING:

 * It would be nice to provide an original reference for the 'beets' data set if possible.
 * I dislike the default format of "PBmodcomp"; giving all of the different corrections at once seems rather "SAS"-ish and would seem to be an invitation to cherry-picking.  I would prefer a "method" argument that selected the method(s) to choose -- it could have an "all" option, I suppose, to restore the current behaviour -- but its default would preferably be set to a sensible default (maybe c("PBkd","PBtest")?)
 * is there any point in trying to connect the bootstrap results with the format and capabilities of the boot package?  (Although I will say (a) I find the format of "boot" objects slightly clunky and (b) the
adjusted confidence intervals in the boot package don't work very
easily with parametric bootstrap results (see http://tinyurl.com/bl6le26)
 * you could make the output of PBmodcomp slightly prettier by using printCoefmat(), and particularly using na.print="" or na.print="." to
eliminate the NA values of df and ddf where they aren't relevant.
 * would it make sense to replace calls to snow functions with calls to functions from the parallel package, which is (a) included as a core package (now) and (b) allows both snow-like and multicore functionality?
 * does it make sense to report p values as <(1/N+1), rather than 0?  More generally, I have sometimes been told (don't have a reference right now, sorry) that it is most appropriate
 to include the observed data as part of the ensemble for calculating p-values, i.e. a PB sample of size N with p values >= the observed
statistic would be given a p-value of (1+p)/(1+N)
 * A different and potentially more powerful way to to plot the reference distributions is with nominal vs observed p-values (see
attached plot)
* Could we have the simulation code in an appendix?

* I think it might be nice to convert Table 1 to a figure, for example (the colours here are pretty but superfluous -- it would be easy to do a black & white version)

ptab <- read.table("pbkrptab.txt",header=TRUE)
library(reshape2)
library(ggplot2)
library(grid)
zmargin <- opts(panel.margin=unit(0,"lines"))
pm <- melt(ptab,id.var=c("parm","alphapct"))
theme_update(theme_bw())
dd <- expand.grid(parm=c("beta1","beta2"),alphapct=c(1,5,10))
## order by average conservatism
pm$variable <- reorder(pm$variable,pm$value,mean)
ggplot(pm,aes(x=value,y=variable,colour=parm))+
    geom_point()+
    geom_vline(data=dd,aes(xintercept=alphapct),linetype=2,colour="pink")+
    facet_grid(parm~alphapct,scale="free_x",labeller=label_both)+zmargin
    
pbkrptab.txt
================
parm alphapct LR KR_R KR_SAS PBtest  PBkd Bartlett	Gamma	F
beta1 1   1.6 0.7 1.4 1.4 1.4 1.3 1.5 0.8
beta2 1   1.8 1.2 1.0 1.3 1.4 1.2 1.4 0.8
beta1 5   7.0 4.3 5.2 6.1 6.0 5.8 6.2 5.2
beta2 5   6.9 5.5 5.1 5.5 5.5 5.3 5.6 5.1
beta1 10 13.5 9.4 10.0 11.7 11.6 11.6 11.9 11.3
beta2 10 12.8 10.7 10.0 10.5 10.4 10.4 10.5 10.6
