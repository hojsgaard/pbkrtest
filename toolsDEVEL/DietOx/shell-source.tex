%%%
%%% Automatically generated file from  shell.tex - DO NOT EDIT MANUALLY
%%%
%%% Input from file PigGrowth.Rnw
%\newslide
\subsection{Pig growth -- \texttt{dietox}}
\label{sec:dietox}

The data used here are described by \cite{lauridsen:99} and contains growth data
for a pig feeding experiment. Data is available as the \texttt{dietox} data set
in the \texttt{doBy} package for \R. 

One of the questions asked in connection with the experiment was whether 
copper added to pig feed increase/decrease growth. 
Copper (hereafter abbreviated Cu) was used in three levels Cu=1: No copper,
Cu=2: 35 mg/kg feed and Cu=3: 175 mg/kg feed. Here we shall analyze data as if they were
layed out as a factorial experiment (even though the design was a (almost)
balanced incomplete block design -- because there is an issue of a litter
effect). 
The weight of slaughter pigs were measured weekly over a 12 week period. 


\section{Loading data}

Data can be loaded as:
\begin{Schunk}
\begin{Sinput}
>library(doBy)
>data(dietox)
>dietox[1:5, ]
\end{Sinput}
\begin{Soutput}
    Weight      Feed Time  Pig Evit Cu Litter
1 26.50000        NA    1 4601    1  1      1
2 27.59999  5.200005    2 4601    1  1      1
3 36.50000 17.600000    3 4601    1  1      1
4 40.29999 28.500000    4 4601    1  1      1
5 49.09998 45.200001    5 4601    1  1      1
\end{Soutput}
\end{Schunk}

Cu is coded with levels 1,2 and 3 meaning that \R\ will regard Cu as a numeric
variable, which it is not. To turn Cu into a factor we do:
\begin{Schunk}
\begin{Sinput}
>dietox$Cu <- as.factor(dietox$Cu)
\end{Sinput}
\end{Schunk}

Note: If instead data was saved as a comma--searated file (a .csv file) it could be
loaded as
\begin{Schunk}
\begin{Sinput}
>dietox <- read.csv("dietox.csv")
\end{Sinput}
\end{Schunk}

\newslide
\section{Looking at data}



\shfig{dietox01}{XXX}{height=4cm}

The weight as function of time is shown in  Figure~\ref{fig:dietox01}.  
which suggests
\begin{itemize}
\item Approximately linear growth curves
\item Some tendency for variance to increase with mean
\end{itemize}
 
This plot
is produced using the \texttt{plotBy()} function in the \texttt{doBy} package as follows:
First, make space for 1 row and 3 columns of plots:
\begin{Schunk}
\begin{Sinput}
>par(mfrow = c(1, 3))
\end{Sinput}
\end{Schunk}

Then call the \texttt{plotBy()} function:
\begin{Schunk}
\begin{Sinput}
>plotBy(Weight ~ Time, subject = Pig, group = Cu, title = "Cu=", 
     data = dietox, col = 1:100, lines = T)
\end{Sinput}
\end{Schunk}

\newslide

\newslide


\subsection{Looking at the growth curve}

Next we  calculate the mean and variance for each combination of
Cu and Time using the \texttt{summaryBy()} function, which is also in the
\texttt{doBy} package:
\begin{Schunk}
\begin{Sinput}
>m.dietox <- summaryBy(Weight ~ Cu + Time, data = dietox, 
     FUN = c(mean, var))
>m.dietox[1:5, ]
\end{Sinput}
\begin{Soutput}
  Cu Time mean.Weight var.Weight
1  1    1    25.34782  12.264414
2  2    1    25.50000   9.656658
3  3    1    26.14999  18.799118
4  1    2    29.48695  15.213879
5  2    2    29.33599  16.070716
\end{Soutput}
\end{Schunk}
\newslide

Figure~\ref{fig:dietox01mean} gives an idea of the growth curves.
Figure~\ref{fig:dietox01mean} suggests that
\begin{itemize}
\item Growth is not quite linear. The curves are curved (slightly S--shaped)!
\item If there is a treatment effect, then it is small!
\end{itemize}
The plot is produced by:

\begin{Schunk}
\begin{Sinput}
>par(mfrow = c(1, 1))
>plotBy(mean.Weight ~ Time, subject = Cu, data = m.dietox, 
     lines = T, col = c("black", "red", "green"), silent = F)
\end{Sinput}
\begin{Soutput}
  symbol colour group subject line
1      1  black  .by.       1    1
2      1    red  .by.       2    1
3      1  green  .by.       3    1
\end{Soutput}
\end{Schunk}

\shfig{dietox01mean}{XXX}{height=6cm,width=7cm}




\subsection{Modelling the mean structure}


Based on Figure \ref{fig:dietox01mean} we fit polynomial models to the means as
\begin{Schunk}
\begin{Sinput}
>lm1 <- lm(mean.Weight ~ Cu * Time, data = m.dietox)
>lm2 <- lm(mean.Weight ~ Cu * (Time + I(Time^2)), data = m.dietox)
>lm3 <- lm(mean.Weight ~ Cu * (Time + I(Time^2) + I(Time^3)), 
     data = m.dietox)
\end{Sinput}
\end{Schunk}
- and plot the residuals (Figure~\ref{fig:dietox-residplots01}):
\begin{Schunk}
\begin{Sinput}
>par(mfrow = c(1, 3))
>plotBy(resid(lm1) ~ Time, subject = Cu, data = m.dietox, 
     lines = T, col = 1:3)
>plotBy(resid(lm2) ~ Time, subject = Cu, data = m.dietox, 
     lines = T, col = 1:3)
>plotBy(resid(lm3) ~ Time, subject = Cu, data = m.dietox, 
     lines = T, col = 1:3)
\end{Sinput}
\end{Schunk}

\shfig{dietox-residplots01}{XXX}{height=6cm,width=14cm}

The residual plots comfirm the S--shaped curve: A 3rd degree polynomial is
needed to remove systematic patterns in the residuals.
Inspired by this we fit the same model to the original data:
\begin{Schunk}
\begin{Sinput}
>mf <- formula(Weight ~ Cu * (Time + I(Time^2) + I(Time^3)))
>lm4 <- lm(mf, data = dietox)
>plotBy(resid(lm4) ~ Time, subject = Pig, data = dietox, 
     lines = T, col = 1:3)
\end{Sinput}
\end{Schunk}
\shfig{dietox-residplots02}{XXX}{height=6cm,width=7cm}

Figure~\ref{fig:dietox-residplots02} shows that a 3rd degree polynomial seems to
remove all systematic effects, but also that the variance increases with time
(and hence with the mean). Finally the plot shows that measurements on the same
animal tend to be positively correlated.


\subsection{Investigating the relationship between the mean and variance}

Next we can plot the log variance against the log mean and fit a straight line
to these data (see Figure~\ref{fig:dietox-010}):
\begin{Schunk}
\begin{Sinput}
>plot(log(var.Weight) ~ log(mean.Weight), data = m.dietox)
>l <- lm(log(var.Weight) ~ log(mean.Weight), data = m.dietox)
>abline(l, lwd = 2, col = "red")
>l
\end{Sinput}
\begin{Soutput}
Call:
lm(formula = log(var.Weight) ~ log(mean.Weight), data = m.dietox)

Coefficients:
     (Intercept)  log(mean.Weight)  
          -1.134             1.212  
\end{Soutput}
\end{Schunk}
The plot suggests that 
\begin{equation}
  \label{eq:vm1}
 \log Var(y) \approx a + b \log E(y) 
\end{equation}
and hence
$$
Var(y) \approx e^a \cdot E(y)^b
$$
The slope $b$ is about one suggesting that the variance is approximately
proportional to the mean.

\shfig{dietox-010}{XXX}{height=6cm,width=7cm}

Note that we can calculate the mean and variance for each combination of time
and Cu in this example since there are many observations for each combination of
\texttt{Time x Cu}. In situations where this is not the case an alternative idea
can be used: We found that a 3rd degree polynomial seems to remove practically
all systematic variation from data. Let $\hat\mu$ and $e$ denote the fitted
values and the residuals under the 3rd degree model \texttt{lm4}. Then we can
plot $\log e^2$ against $\log \hat\mu$:
\begin{Schunk}
\begin{Sinput}
>plot(log(fitted(lm4)), log(resid(lm4)^2))
>l <- lm(log(resid(lm4)^2) ~ log(fitted(lm4)))
>abline(l, col = "red")
>l
\end{Sinput}
\begin{Soutput}
Call:
lm(formula = log(resid(lm4)^2) ~ log(fitted(lm4)))

Coefficients:
     (Intercept)  log(fitted(lm4))  
          -2.611             1.188  
\end{Soutput}
\end{Schunk}

The idea behind doing so is as follows: The residuals $e_i = y_i -\hat\mu_i$
have mean $E(e_i)=0$ and the variance is $Var(e_i)\approx Var(y_i)$. Now since
$E(e_i)=0$ we have $Var(e_i) =E(e_i^2)$. A simple estimate of $E(e_i^2)$ is
$e_i^2$. So discovering an (approximately) linear relationship between 
$\log e_i^2$ and $\log \hat\mu_i$ suggests the variance function in
\eqref{eq:vm1}. Above we find that the slope is aboout 1 in accordance with the
previous findings. The plot is in Figure~\ref{fig:dietox-varmeanplots01}.  

\shfig{dietox-varmeanplots01}{XXX}{height=6cm,width=10cm}



\newslide





\section{Fitting a gee model}

Based on the previous findings interest is in fitting a model which:
\begin{enumerate}
\item Describes the mean structure (and we have already seen that a 3rd degree
  polynomial could be a good starting point).
\item Accounts for that the variance is approximately proportional to the mean.
\item Accounts for that there are repeated measurements on the same animal
\end{enumerate}


One way to meet these three requirements is to fit a gee model:
\begin{Schunk}
\begin{Sinput}
>gee1 <- geeglm(mf, data = dietox, id = Pig, family = poisson("identity"), 
     corstr = "ar1")
\end{Sinput}
\end{Schunk}

For comparison we fit a quasi--poisson model which only accounts for 1. and
2. above as
\begin{Schunk}
\begin{Sinput}
>qpo1 <- glm(mf, data = dietox, family = quasipoisson("identity"))
\end{Sinput}
\end{Schunk}


It is informative to compare the regression coefficients and the
models and the
standard errors of the 1) gee model, 2) the
quasi--poisson model and 3) the linear model:

\begin{Schunk}
\begin{Sinput}
>sgee1 <- summary(gee1)
>sqpo1 <- summary(qpo1)
>slm4 <- summary(lm4)
>Egee <- sgee1$coef[, 1]
>Eqpo <- sqpo1$coef[, 1]
>Elm <- slm4$coef[, 1]
>SEgee <- sgee1$coef[, 2]
>SEqpo <- sqpo1$coef[, 2]
>SElm <- slm4$coef[, 2]
>Rgee <- sgee1$coef[, 2]/slm4$coef[, 2]
>Rqpo <- sqpo1$coef[, 2]/slm4$coef[, 2]
>round(cbind(Egee, Eqpo, Elm, SEgee, SEqpo, SElm, Rgee, 
     Rqpo), 3)
\end{Sinput}
\begin{Soutput}
                Egee   Eqpo    Elm SEgee SEqpo  SElm  Rgee  Rqpo
(Intercept)   21.858 21.140 21.152 0.693 1.726 2.395 0.289 0.721
Cu2            0.527  0.879  0.699 0.941 2.389 3.316 0.284 0.720
Cu3            0.043 -0.004 -0.042 1.036 2.433 3.350 0.309 0.726
Time           2.885  3.560  3.553 0.360 1.249 1.535 0.235 0.814
I(Time^2)      0.614  0.471  0.472 0.070 0.237 0.270 0.260 0.880
I(Time^3)     -0.026 -0.018 -0.018 0.004 0.013 0.014 0.279 0.931
Cu2:Time      -0.405 -0.804 -0.662 0.637 1.724 2.123 0.300 0.812
Cu3:Time       0.857  0.888  0.918 0.609 1.761 2.146 0.284 0.820
Cu2:I(Time^2)  0.018  0.119  0.093 0.115 0.327 0.372 0.310 0.878
Cu3:I(Time^2) -0.096 -0.099 -0.105 0.109 0.334 0.377 0.291 0.887
Cu2:I(Time^3)  0.001 -0.006 -0.005 0.006 0.018 0.019 0.317 0.928
Cu3:I(Time^3)  0.003  0.003  0.003 0.006 0.018 0.019 0.294 0.938
\end{Soutput}
\end{Schunk}

The output shows that 1) the parameter estimates are almost identical whereas 2)
the standard errors differ quite a bit. \texttt{Rgee} gives the ratio between
the standard errors for the gee model and the linear model while \texttt{Rqpo}
gives the ratio between the standard errors for the quasi poisson model and the
linear model. The gee model reduces the standard error to about 0.3 of the
standard errors for the linear model whereas the quasi poisson model reduces the
standard error to about 0.8 of the standard errors for the linear model.

So what can be concluded from this: We have three different models (or perhaps more
appropriately: three different estimation methods) which produce
practically the same parameter estimates but with markedly different
standard errors of the estimates?

We claim that the standard errors for the gee model are the most reliable, i.e.\
the other two models overestimate the standard error.  The reason being that the
gee model adjusts for the fact that measurements at two different time points on
the same pig tend to be more alike than on two different pigs. We shall return
to this in Section~\ref{why}.


\section{Model selection}


We proceed by adding terms sequentially to the models
\begin{Schunk}
\begin{Sinput}
>anovalm4 <- anova(lm4)
>anovaqpo1 <- anova(qpo1, test = "F")
>anovagee1 <- anova(gee1)
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
>anovalm4
\end{Sinput}
\begin{Soutput}
Analysis of Variance Table

Response: Weight
              Df Sum Sq Mean Sq    F value    Pr(>F)    
Cu             2    980     490     9.9751 5.224e-05 ***
Time           1 492513  492513 10028.0637 < 2.2e-16 ***
I(Time^2)      1   1014    1014    20.6385 6.350e-06 ***
I(Time^3)      1    294     294     5.9771   0.01470 *  
Cu:Time        2     35      17     0.3547   0.70152    
Cu:I(Time^2)   2     52      26     0.5266   0.59080    
Cu:I(Time^3)   2      8       4     0.0828   0.92057    
Residuals    849  41697      49                         
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
\end{Soutput}
\begin{Sinput}
>anovaqpo1
\end{Sinput}
\begin{Soutput}
Analysis of Deviance Table

Model: quasipoisson, link: identity

Response: Weight

Terms added sequentially (first to last)


              Df Deviance Resid. Df Resid. Dev          F    Pr(>F)
NULL                            860     9105.9                     
Cu             2     16.1       858     9089.8    10.2481 4.001e-05
Time           1   8362.8       857      727.0 10656.8815 < 2.2e-16
I(Time^2)      1     27.8       856      699.2    35.4091 3.906e-09
I(Time^3)      1      5.8       855      693.4     7.3664   0.00678
Cu:Time        2      1.2       853      692.2     0.7735   0.46171
Cu:I(Time^2)   2      1.2       851      691.0     0.7794   0.45903
Cu:I(Time^3)   2      0.2       849      690.8     0.1281   0.87979
                
NULL            
Cu           ***
Time         ***
I(Time^2)    ***
I(Time^3)    ** 
Cu:Time         
Cu:I(Time^2)    
Cu:I(Time^3)    
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
\end{Soutput}
\begin{Sinput}
>anovagee1
\end{Sinput}
\begin{Soutput}
Analysis table for GEE models

                  X2.stat DF Pr(>|X^2|)
Cu              1.5221015  2    0.46718
Time         6905.6249513  1    0.00000
I(Time^2)      89.0758061  1    0.00000
I(Time^3)      61.6253666  1    0.00000
Cu:Time         0.9430137  2    0.62406
Cu:I(Time^2)    5.8956159  2    0.05245
Cu:I(Time^3)    2.1170065  2    0.34697
\end{Soutput}
\end{Schunk}

We see that only under the gee model any treatment effect comes near to being
significant. This is closely related to that the standard errors of the
parameter estimates are smallest under the gee model. \cite{lauridsen:99} find,
by a different analysis (random regression) that there is a statistically
significant effect of Cu -- but the effect is clearly very small.

Dropping the highest order term from the model gives the model gee2 below, and
this model will be our focus in the following:
\begin{Schunk}
\begin{Sinput}
>gee2 <- update(gee1, . ~ . - Cu:I(Time^3))
>summary(gee2)
\end{Sinput}
\begin{Soutput}
Call:
geeglm(formula = Weight ~ Cu + Time + I(Time^2) + I(Time^3) + 
    Cu:Time + Cu:I(Time^2), family = poisson("identity"), data = dietox, 
    id = Pig, corstr = "ar1")

 Coefficients:
                 Estimate    RobustSE          X^2    P(>X^2)
(Intercept)   21.78946075 0.709163599 944.06107981 0.00000000
Cu2            0.56048295 0.926072249   0.36629844 0.54502888
Cu3            0.21255942 1.033674186   0.04228569 0.83707617
Time           2.96637338 0.304312314  95.01946433 0.00000000
I(Time^2)      0.59484762 0.047701492 155.50613493 0.00000000
I(Time^3)     -0.02522423 0.002456949 105.40078602 0.00000000
Cu2:Time      -0.44639624 0.419359426   1.13310002 0.28711506
Cu3:Time       0.65091898 0.379920208   2.93541210 0.08665655
Cu2:I(Time^2)  0.02803206 0.030587932   0.83986545 0.35943526
Cu3:I(Time^2) -0.04752989 0.027632755   2.95859257 0.08542227

 Estimated Scale Parameters:
             estimate    san.se     wald            p
(Intercept) 0.7752835 0.1434884 29.19360 6.549508e-08

Correlation Structure:     ar1 

Estimated Correlation Parameters:
       estimate      san.se     wald p
alpha 0.9572027 0.009545219 10056.25 0
Number of clusters:   72   Maximum cluster size: 12 
\end{Soutput}
\end{Schunk}



\section{Estimating contrasts}

Suppose first we want to estimate the predicted value for Cu=2 and Cu=3 at Time=7.
The traditional way of doing this in \R\ is by using the \texttt{predict()} function:
\begin{Schunk}
\begin{Sinput}
>dnew <- data.frame(Time = c(7, 7), Cu = as.factor(c(2, 
     3)))
>predict(gee2, dnew)
\end{Sinput}
\begin{Soutput}
       1        2 
61.85898 65.48972 
\end{Soutput}
\end{Schunk}

The difficulty arises when wanting to estimate the difference in those predicted
values. A solution to this problem is provided by 
 the \texttt{esticon()} function in the package
XXXX as follows. Define
\begin{Schunk}
\begin{Sinput}
>L.Cu2 <- c(1, 1, 0, 7, 7^2, 7^3, 7, 0, 7^2, 0)
>L.Cu3 <- c(1, 0, 1, 7, 7^2, 7^3, 0, 7, 0, 7^2)
>L.Cu2
\end{Sinput}
\begin{Soutput}
 [1]   1   1   0   7  49 343   7   0  49   0
\end{Soutput}
\begin{Sinput}
>L.Cu3
\end{Sinput}
\begin{Soutput}
 [1]   1   0   1   7  49 343   0   7   0  49
\end{Soutput}
\end{Schunk}

Then the predicted values and the difference between them is 
\begin{Schunk}
\begin{Sinput}
>sum(gee2$coef * L.Cu2)
\end{Sinput}
\begin{Soutput}
[1] 61.85898
\end{Soutput}
\begin{Sinput}
>sum(gee2$coef * L.Cu3)
\end{Sinput}
\begin{Soutput}
[1] 65.48972
\end{Soutput}
\begin{Sinput}
>sum(gee2$coef * (L.Cu3 - L.Cu2))
\end{Sinput}
\begin{Soutput}
[1] 3.630748
\end{Soutput}
\end{Schunk}

The problem in practice is that when estimating such functions (contrasts) we
always want an estimate of the standard error and often we want to test the
hypothesis that the contrast is equal to a specifed value. To obtain this we
use the \texttt{esticon()} function
\begin{Schunk}
\begin{Sinput}
>L <- rbind(L.Cu2, L.Cu3, diff = L.Cu3 - L.Cu2)
>L
\end{Sinput}
\begin{Soutput}
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
L.Cu2    1    1    0    7   49  343    7    0   49     0
L.Cu3    1    0    1    7   49  343    0    7    0    49
diff     0   -1    1    0    0    0   -7    7  -49    49
\end{Soutput}
\begin{Sinput}
>esticon(gee2, L)
\end{Sinput}
\begin{Soutput}
      beta0  Estimate Std.Error    X2.value DF Pr(>|X^2|)
L.Cu2     0 61.858975  1.376609 2019.223943  1   0.000000
L.Cu3     0 65.489723  1.809338 1310.107187  1   0.000000
diff      0  3.630748  2.284166    2.526602  1   0.111941
\end{Soutput}
\end{Schunk}

So the estimated difference is not significantly different from
\texttt{beta0=0}. 


\section{LSmeans}

We conclude by showing how to calculate LSmeans. This is done using the
\texttt{lsmean()} function in the \texttt{pda} package. One has to be a little
bit careful when using \texttt{lsmean()} as illustrated in the following:

Consider
\begin{Schunk}
\begin{Sinput}
>lsmean(gee2)
\end{Sinput}
\begin{Soutput}
  Cu     Time     pred       se
1  1 6.480836 41.01404 1.983820
2  2 6.480836 39.85888 1.947625
3  3 6.480836 43.44879 2.292466
\end{Soutput}
\end{Schunk}

The values in the \texttt{pred} column are ``wrong'', (and it is the hope that
this error in the \texttt{pda} package will be fixed). To get the right answer
one needs to define the quadratic and cubic terms directly and rewrite the model
in terms of these as: 
\begin{Schunk}
\begin{Sinput}
>dietox2 <- dietox
>dietox2$Time2 <- dietox$Time^2
>dietox2$Time3 <- dietox$Time^3
>gee3 <- geeglm(Weight ~ Cu * Time + Cu * Time2 + Time3, 
     data = dietox2, id = Pig, family = poisson("identity"), 
     corstr = "ar1")
>lsmean(gee3)
\end{Sinput}
\begin{Soutput}
  Cu     Time    Time2    Time3     pred       se
1  1 6.480836 53.85366 502.7456 60.36739 1.025467
2  2 6.480836 53.85366 502.7456 59.54448 1.176289
3  3 6.480836 53.85366 502.7456 62.23879 1.649893
\end{Soutput}
\end{Schunk}

For completeness we write out the details of what the specific LSmeans are in
this case:
\begin{Schunk}
\begin{Sinput}
>mT <- mean(dietox$Time)
>mT2 <- mean(dietox$Time^2)
>mT3 <- mean(dietox$Time^3)
>L1 <- c(1, 0, 0, mT, mT2, mT3, 0, 0, 0, 0)
>L2 <- c(1, 1, 0, mT, mT2, mT3, mT, 0, mT2, 0)
>L3 <- c(1, 0, 1, mT, mT2, mT3, 0, mT, 0, mT2)
>esticon(gee2, rbind(L1, L2, L3))
\end{Sinput}
\begin{Soutput}
   beta0 Estimate Std.Error X2.value DF Pr(>|X^2|)
L1     0 60.36739  1.025467 3465.462  1          0
L2     0 59.54448  1.176289 2562.445  1          0
L3     0 62.23879  1.649893 1423.018  1          0
\end{Soutput}
\end{Schunk}


 


\appendix

\section{On why the gee model fits best}
\label{why}


To justify the claim that the standard errors produced from the gee model are
the most appropriate ones, reconsider the notion of variance of an estimator: Let
$\hat\theta(y)$ be an estimator of a parameter $\theta$ based on a sample of
data $y=(y_1, \dots, y_n)$.  The variance $Var(\hat\theta)$ is a measure of how
much $\hat\theta(y)$ will vary if the experiment is repeated under identical
conditions a large number of times. To be specific let $y^1=(y^1_1,
\dots,y^1_n), \dots, y^R=(y^R_1, \dots,y^R_n)$ denote the samples which are
obtained after repeating the experiment $R$ times. Let $\hat\theta(y^1), \dots,
\hat\theta(y^R)$ be the corresponding estimates calculated for each of the $R$
experiments. Then the variance of $\hat\theta(y^1), \dots, \hat\theta(y^R)$ is
(when $R\rightarrow \infty$) equal to $Var(\hat\theta)$.

In practice $R$ is finite.  Letting $\bar{ \hat\theta}$ denote the average of
$\hat\theta(y^1), \dots, \hat\theta(y^R)$. Then 
$$
\var(\hat\theta) \approx
\frac 1 {R-1} \sum_r (\hat\theta(y_r)- \bar{ \hat\theta} )^2 
$$

In real life the experiment can not be repeated, but there is a statistical
technique called ``jacknife'' by which one can mimic the replication of an
experiment. We will not go into details about the method but refer to e.g.\
\cite{efron:82}. Instead we will show that the jacknife technique is very simple to
implement in practice:
\begin{Schunk}
\begin{Sinput}
>d <- unique(dietox$Pig)
>v1 <- v2 <- v3 <- NULL
>for (i in 1:length(d)) {
     print(c(i, d[i]))
     dsub <- subset(dietox, Pig != d[i])
     gee1 <- geeglm(mf, data = dsub, id = Pig, family = poisson("identity"), 
         corstr = "ar1")
     qpo1 <- glm(mf, data = dsub, family = quasipoisson("identity"))
     lm4 <- lm(mf, data = dsub)
     v1 <- rbind(v1, summary(gee1)$coef[, 1])
     v2 <- rbind(v2, summary(qpo1)$coef[, 1])
     v3 <- rbind(v3, summary(lm4)$coef[, 1])
 }
>SEgeej <- sqrt(apply(v1, 2, var) * (nrow(v1) - 1))
>SEqpoj <- sqrt(apply(v2, 2, var) * (nrow(v2) - 1))
>SElmj <- sqrt(apply(v3, 2, var) * (nrow(v3) - 1))
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
>round(cbind(SEqpoj, SEgeej, SElmj, SEgee, SEqpo, SElm), 
     3)
\end{Sinput}
\begin{Soutput}
              SEqpoj SEgeej SElmj SEgee SEqpo  SElm
(Intercept)    0.724  0.722 0.801 0.693 1.726 2.395
Cu2            0.966  0.978 1.072 0.941 2.389 3.316
Cu3            1.058  1.062 1.165 1.036 2.433 3.350
Time           0.342  0.375 0.398 0.360 1.249 1.535
I(Time^2)      0.059  0.074 0.059 0.070 0.237 0.270
I(Time^3)      0.003  0.004 0.003 0.004 0.013 0.014
Cu2:Time       0.606  0.663 0.703 0.637 1.724 2.123
Cu3:Time       0.691  0.641 0.729 0.609 1.761 2.146
Cu2:I(Time^2)  0.098  0.121 0.105 0.115 0.327 0.372
Cu3:I(Time^2)  0.126  0.116 0.127 0.109 0.334 0.377
Cu2:I(Time^3)  0.005  0.006 0.005 0.006 0.018 0.019
Cu3:I(Time^3)  0.007  0.006 0.007 0.006 0.018 0.019
\end{Soutput}
\end{Schunk}

The first three columns contain the jacknife estimates of the standard errors
under the three different estimation methods. They are practically identical and
represent an approximation to the true standard error which one would find when
repeating the experiment a large number of times. The next three columns contain
the standard errors estimated using the different models, and we see that the
gee model produces the standard errors which are closest to the ``true'' ones.








% \section{Comparison: Fitting a mixed model}



% @ 
% <<>>=
% library(nlme)
% lme1 <- lme(Weight~Cu*(Time+I(Time^2)+I(Time^3)), data = dietox, random=~1|Pig)
% summary(lme1)$tTable
% anova(lme1)
% @ %def 

% @ 
% <<>>=
% cbind(SElme=summary(lme1)$tTable[,2],
% SEgee=sgee1$coef[,2],SEqpo=sqpo1$coef[,2],SElm=slm4$coef[,2]) 
% @ %def 






% \newslide

% \subsubsection{Linear regression}

% Let $c$: Cu, $p$: pig (within treatment), $t$: time.

% Simple regression model:
% $$
% y_{cpt} = \alpha+ \beta t+ \alpha_c + \beta_c t + e_{cpt}; \ \ e_{cpt}\sim N(0,\sigma^2)
% $$
% Written shortly as:
% $$
% [y] = 1 + \underline{time} + Cu + Cu * \underline{time} + [e]
% $$


% \newslide
% \subsubsection{In \SAS\ and \R}

% \begin{verbatim}
% proc mixed data=dietox noinfo noclprint;
%   class cu pig;
%   model weight = time cu cu * time / solution htype=1;
% run;
% \end{verbatim}


% @ 
% <<eval=T>>=
% fm0 <- lm (Weight ~ Time + Cu + Cu * Time, data = dietox)
% @ %def 


% To plot the residuals and the fitted values, one can do:
% @ 
% <<>>=
% par(mfrow=c(2,3))
% plotBy(resid(fm0)~Time,subject=Pig,group=Cu, data=dietox, lines=T, col=1:100)
% plotBy(fitted(fm0)~Time,subject=Pig,group=Cu, data=dietox, lines=T, col=1:3)
% @ %def 

% \newslide
% \subsubsection{Figures}
% \shfig{dietox02}{XXX}{}

% If model is appropriate, residuals should fluctuate randomly around zero. That
% is clearly not so. 

% \newslide
% \subsubsection{Adding a random intercept term}

% Pigs who start above (below) average tend to keep that position throughout the
% experiment. 

% This suggests to add a pig--specific (random) intercept term:

% $$
% y_{cpt} = \alpha+ \beta t+ \alpha_c + \beta_c t +{\color{red}U_{cp}}+ e_{cpt}; \ \ 
% %e_{cpt}\sim N(0,\sigma_e^2), U_{cp}\sim N(0,\sigma_U^2), 
% $$
% Written shortly as:
% $$
% [y] = 1 + \underline{time} + Cu + Cu * \underline{time} + {\color{red}[Cu*Pig]}+[e]
% $$




% \newslide
% \subsubsection{In \SAS\ and \R}

% \begin{verbatim}
% proc mixed data=dietox noinfo noclprint;
%   class cu pig;
%   model weight = time cu cu * time / solution htype=1;
%   random int / subject=cu*pig;
% run;
% \end{verbatim}

% @ 
% <<eval=T>>=
% library(nlme)
% fm1 <- lme(Weight ~ Time + Cu + Cu * Time, data = dietox, random=~1|Pig)
% @ %def 


% \newslide
% @ 
% <<echo=F,eval=F>>=
% anova(fm1)
% @ %def 

% \newslide
% \subsubsection{Figures}

% \shfig{dietox03}{XXX}{}

% Residuals still problematic: Residuals on some pigs steadily increasing, others
% steadily decreasing.


% \newslide
% \subsubsection{Adding a random slope term}

% To account for this phenomenon, one can add a pig--specific (random) slope term:
% $$
% y_{cpt} = \alpha+ \beta t+ \alpha_c + \beta_c t +U_{cp}+ {\color{red}W_{cp} t} + e_{cpt}; \ \ 
% %e_{cpt}\sim N(0,\sigma_e^2), U_{cp}\sim N(0,\sigma_U^2), W_{cp}\sim N(0,\sigma_W^2), 
% $$
% Written shortly as:
% $$
% [y] = 1 + \underline{time} + Cu + Cu * \underline{time} + [CU*Pig] + 
% {\color{red}[CU*Pig]*\underline{time}}+[e]
% $$

% Such a model is called a \com{random regression model}.

% \newslide
% \subsubsection{In \SAS\ and \R}

% \begin{verbatim}
% data dietox; set dietox; 
%   timec = time;

% proc mixed data=dietox noinfo noclprint;
%   class cu pig timec;
%   model weight = time cu cu * time / solution htype=1;
%   random int time/ subject=cu*pig;
% run;
% \end{verbatim}

% @ 
% <<>>=
% fm2 <- lme(Weight ~ Cu * Time, data = dietox, random = ~ 1 + Time| Pig)
% @ %def 
% \newslide
% @ 
% <<echo=F,eval=F>>=
% anova(fm2)
% @ %def 

%%% End of input from file PigGrowth.Rnw
