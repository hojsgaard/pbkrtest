




## Exercise (Lactation)


```{r}
library(ggplot2)
library(modelr)
library(dplyr)
library(broom.mixed)
library(doBy)
library(lmbayes)
options("digits"=2)
```

In this exercise you must fit a linear model and a Bayesian linear model to lactation data. 

```{r}
milkman <- doBy::milkman
milkman <- milkman |>
    filter(dfc>2 & ampm==1, race=="RDM", lactno==1) |>
    dplyr::select(my, dfc, cowno) |> na.omit() |> mutate(hfc=dfc*24)



cl <- milkman$cowno |> unique()
```

```{r}
plotit <- function(dat){
    dat |> ggplot(aes(x = dfc, y = my, group = cowno)) +
        geom_point() +
        facet_wrap(~cowno, ncol = 4)    
}

idx <- 0+(1:12)
milkman |> filter(cowno %in% cl[idx]) |> plotit()
```

```{r}
idx <- 12+(1:12)
milkman |> filter(cowno %in% cl[idx]) |> plotit()
```

```{r}
idx <- 24+(1:12)
milkman |> filter(cowno %in% cl[idx]) |> plotit()
```

```{r}
idx <- 36+(1:12)
milkman |> filter(cowno %in% cl[idx]) |> plotit()
```

```{r}
idx <- 48+(1:12)
milkman |> filter(cowno %in% cl[idx]) |> plotit()
```


```{r}
idx <- 0+(1:12)
milk <- milkman |> filter(cowno %in% cl[idx])  |> na.omit()

m.lst <- milk |> lm_by(log(my) ~ log(dfc) + dfc | cowno)
coef_df <- m.lst |> sapply(coef) |> t()
m <- colMeans(coef_df)
C <- cov(coef_df)
ss <- m.lst |> sapply(sigma)
sd <- mean(ss)
```


```{r}
dat1 <- milkman |> filter(cowno==906)
dat2 <- milkman |> filter(cowno==836)
dat3 <- milkman |> filter(cowno==838)

dat1 |> dim()
dat2 |> dim()
dat3 |> dim()

pl1 <- dat1 |> ggplot(aes(x=dfc, y=my)) + geom_point()
pl2 <- dat2 |> ggplot(aes(x=dfc, y=my)) + geom_point()
pl3 <- dat3 |> ggplot(aes(x=dfc, y=my)) + geom_point()
cowplot::plot_grid(pl1, pl2, pl3, nrow=1)
```

Fit lm to all data in dat2:

```{r}
tidy(lmf0)
```

```{r}
dat1 |> ggplot(aes(x = dfc, y = my)) +
  geom_point() +
  geom_line(aes(y = exp(predict(lmf0))))
```

```{r}
update_geom_defaults("line", list(linewidth = 1.75))
this.dat <- dat1 
dfc0 <- 40
dat.train <- filter(this.dat, dfc < dfc0)
##


## Prior from other cows
load_all("_lmbayes/")
prior0 <- list(m=m, C=C, sd=sd)
prior1 <- list(m=m, C=C, sd=sd * 100^2)
prior2 <- list(m=m, C=C/100, sd=sd)
prior3 <- list(m=m, C=diag(diag(C)), sd=sd)
##
lmf0 <- lm(log(my)  ~ log(dfc) + dfc, data=this.dat)
lmf1 <- lm(log(my)  ~ log(dfc) + dfc, data=dat.train)
lmb0 <- lmb(log(my) ~ log(dfc) + dfc, data=dat.train, prior=prior0)
lmb1 <- lmb(log(my) ~ log(dfc) + dfc, data=dat.train, prior=prior1)
lmb2 <- lmb(log(my) ~ log(dfc) + dfc, data=dat.train, prior=prior2)
lmb3 <- lmb(log(my) ~ log(dfc) + dfc, data=dat.train, prior=prior3)
##
this.dat$predf0 <- exp(predict(lmf0, newdata = this.dat))
this.dat$predf1 <- exp(predict(lmf1, newdata = this.dat))
this.dat$predb0 <- exp(predict(lmb0, newdata = this.dat))
this.dat$predb1 <- exp(predict(lmb1, newdata = this.dat))
this.dat$predb2 <- exp(predict(lmb2, newdata = this.dat))
this.dat$predb3 <- exp(predict(lmb3, newdata = this.dat))

##
pl1 <- this.dat |>
    ggplot(aes(dfc, my)) +
    geom_point(alpha=.3) +
    geom_vline(xintercept = dfc0) + 
    geom_line(aes(y=predf0), color="darkgreen", linetype=2) +
    geom_line(aes(y=predf1), color="brown", linetype=2) +
    ylim(c(3, 20))
pl1

##
pl1 + 
    geom_line(aes(y=predb0), color="red") +
    geom_line(aes(y=predb1), color="orange") +
    geom_line(aes(y=predb2), color="blue") +
    geom_line(aes(y=predb3), color="cyan") 






pl1 <- this.dat |>
    ggplot(aes(dfc, my)) +
    geom_point(alpha=.3) +
    geom_vline(xintercept = dfc0) + 
    geom_line(aes(y=predf0), color="darkgreen", linetype=2, linewidth=2) +
    geom_line(aes(y=predf1), color="brown", linetype=2) +
    geom_line(aes(y=predb0), color="red") +
    geom_line(aes(y=predb1), color="orange") +
    ## geom_line(aes(y=predb2), color="purple") +
    geom_line(aes(y=predb3), color="blue") +
    ylim(c(3, 20)) 
pl1

l.lst <- list(lmf0, lmf1, lmb0, lmb1, lmb2, lmb3)

my_time <- function(object){
    b <- coef(object)
    - b[2] / b[3]    
}

l.lst |> sapply(my_time)

##
lmf0 |> coef()
lmf1 |> coef()
lmb1 |> coef()
lmb2 |> coef()

dat2$predf <- exp(predict(lmf1, newdata = dat2))
dat2$predb <- exp(predict(lmb1, newdata = dat2))

pl1 <- dat1 |> ggplot(aes(dfc, my)) + geom_point() +
    geom_line(aes(y=predf, color="red")) +
    geom_line(aes(y=predb, color="blue")) +
    ylim(c(2, 15))
pl2 <- dat2 |> ggplot(aes(dfc, my)) + geom_point() +
    geom_line(aes(y=predf, color="red")) +
    geom_line(aes(y=predb, color="blue")) +
    ylim(c(2, 15))
cowplot::plot_grid(pl1, pl2, nrow=1)

```



A function that describes these data well is the Woods curve

$$
y = a * t^b * \exp(-c * t)
$$

Can be fitted with `nls()`. An alternative is to consider $z = \log y$. This function can be fitted with `lm()`

### Classical approach

First focus on data set `dat1`: 

1. Derive the expression for the time for maximum milk yield.

1. Fit model to all data and plot fitted curve for all data on top of
   the observed data points. Does the curve describe the data
   reasonably well? Estimate the time for maximum milk yield.
   
1. Fit model to data for the first 10 days plot fitted curve for all data on top of
   the observed data points. Does the curve describe the data
   reasonably well? Estimate the time for maximum milk yield.
   
1. Repeat the step above but this time use data for the first 20 days,
   the first 40 days, the first 60 days etc. How many training data do
   you need to optain a fitted curve for the full curve that fits well
   to the observed data?
   
The real test is to try to predict the yield for the data set `dat2`.

1. Redo all the steps above: Fit model on (subsets of) `dat1` and predict `dat2`.

1. Comment on the results. 


### Bayesian approach

Next, we consider a Bayesian linear model where we need to specify

1. a (conditional) standard deviation $\sigma$ for the observables,

1. a prior mean $m$ for the regression parameters $(a, b, c)$ and

1. a prior variance $C$ for the regression parameters.

Having dat1 so repeat the steps above. In doing so you are more than welcome to use my `lmbayes` package available 
on github: [https://github.com/hojsgaard/lmbayes](https://github.com/hojsgaard/lmbayes)

Hint: there is a vignette in the `lmbayes` package that explains the usage.

```{r}

## ## lmfit to first observations
## dat.train <- filter(dat1, dfc < dfc0)
## lmf1 <- lm(log(my)  ~ log(dfc) + dfc, data=dat.train)
## ## lmb fit to first observations; diffuse prior

## prior <- lm2prior(lm0, 1000)
## lmb3 <- lmb(log(my) ~ log(dfc) + dfc, data=dat.train, prior=prior)

## prior <- lm2prior(lm0, 5)
## lmb1 <- lmb(log(my) ~ log(dfc) + dfc, data=dat.train, prior=prior)
## ##
## prior <- lm2prior(lm0, 50)
## lmb2 <- lmb(log(my) ~ log(dfc) + dfc, data=dat.train, prior=prior)
## ##

## dat1$predf0  <- exp(predict(lmf0, newdata = dat1))
## dat1$predf1  <- exp(predict(lmf1, newdata = dat1))
## dat1$predb1 <- exp(predict(lmb1, newdata = dat1))
## dat1$predb2 <- exp(predict(lmb2, newdata = dat1))
## dat1$predb3 <- exp(predict(lmb3, newdata = dat1))

## pl1 <- dat1 |> ggplot(aes(dfc, my)) + geom_point() +
##     geom_vline(xintercept = dfc0) + 
##     geom_line(aes(y=predf0),  colour="darkgreen", linetype=2) +
##     geom_line(aes(y=predf1),  colour="green", linetype=2) +
##     geom_line(aes(y=predb3), color="brown") +
##     geom_line(aes(y=predb1), color="blue") +
##     geom_line(aes(y=predb2), color="purple") +
##     ylim(c(2, 20))
## pl1


```


<!-- ```{r} -->
<!-- # Step 1: Sample covariance of estimated coefficients -->
<!-- sample_cov <- cov(coef_df) -->

<!-- # Step 2: Extract vcov matrices and compute their average -->
<!-- vcov_matrices <- lapply(m.lst, vcov) -->

<!-- # Check that all vcov matrices are 4x4 (intercept + 3 coefs) -->
<!-- # We'll assume interest is only in the 3 slope coefficients (excluding intercept) -->

<!-- vcov_3x3 <- lapply(vcov_matrices, function(mat) { -->
<!--   mat[2:4, 2:4]  # remove intercept row and column -->
<!-- }) -->

<!-- # Compute average of these matrices -->
<!-- avg_within_cow_cov <- Reduce("+", vcov_matrices) / length(vcov_matrices) -->

<!-- # Step 3: Corrected prior covariance -->
<!-- corrected_cov <- sample_cov - avg_within_cow_cov -->

<!-- # Output -->
<!-- cat("Sample covariance matrix:\n") -->
<!-- print(sample_cov) -->
<!-- cat("\nAverage within-cow estimation covariance matrix:\n") -->
<!-- print(avg_within_cow_cov) -->
<!-- cat("\nCorrected prior covariance matrix:\n") -->
<!-- print(corrected_cov) -->
<!-- ``` -->
