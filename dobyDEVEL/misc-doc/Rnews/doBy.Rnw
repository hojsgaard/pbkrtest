\documentclass[a4paper]{report}
\usepackage{Rnews}
\usepackage[round]{natbib}
\bibliographystyle{abbrvnat}

\begin{document}

\RecustomVerbatimEnvironment{Sinput}{Verbatim}%
    {fontsize=\small,frame=single,framerule=0pt}

\RecustomVerbatimEnvironment{Soutput}{Verbatim}%
    {fontsize=\scriptsize, frame=single,framerule=0pt}

@ 
<<echo=FALSE>>=
oopt <- options()
options("digits"=4)
options("width"=40)
@ %def 


\def\doby{\pkg{doBy}}
\def\code#1{\texttt{#1}}

\title{The doBy package}
\author{by Søren Højsgaard}

\maketitle

This article is not about rocket science; in fact it is not about
science at all. It is a description of yet another package with
utility functions.

I have used R in connection with teaching generalized linear models
and related topics to Ph.d.\ students within areas like agronomy,
biology, and veterinary science at the Danish Institute of
Agricultural Sciences.

These students, of which many are familiar with the SAS system, have
almost all come to appreciate R very quickly. However, they have also
from time to time complained that certain standard tasks are hard to
do in R -- and certainly harder than in SAS.  The \doby\ package is an
attempt to make some of these standard tasks easier.


\section*{Airquality data} 

The presentation of the package is based on the \code{airquality}
dataset which contains  air quality measurements in New York, May to
September 1973. (Note that months are coded as $5,\dots,9$).  

@ 
<<>>=
head(airquality)
#names(airquality) <- c("Oze","Sor","Wnd", "Tmp","Mth", "Day")
@ %def 

\section*{The \code{summaryBy} function}
\label{summaryBy}

With the summary procedure of SAS (\code{PROC SUMMARY}) one
can easily calculate things like ``the mean and variance of $x$ for
each combination of two factors $A$ and $B$''. To calculate mean and
variance of Ozone and Wind for each combination of Month, do:
@ 
<<>>=
summaryBy(Ozone+Wind~Month, data=airquality,FUN=c(mean,var),
          prefix=c("m","v"),  na.rm=TRUE)
@ %def 

The result above can clearly be obtained in other ways. For example by
using
the \code{aggregate} function, the \code{summarize} function in the
\texttt{Hmisc} package or by
@ 
<<results=hide>>=
a<-by(airquality, airquality$Month, function(d){
  c(mean(d[,c("Ozone","Wind")],na.rm=T), diag(var(d[,c("Ozone","Wind")],na.rm=T)))})
do.call("rbind",a)
@ %def 
However, my students have found this somewhat cumbersome!


\section*{The \code{orderBy} function} 
\label{orderBy}

Ordering (or sorting) a data frame is possible with the \code{orderBy}
function. 
Suppose we want to order the dataframe by \code{Temp} and by
\code{Month} (within \code{Temp}) and that the ordering should be
decreasing. This can be achieved by:
@ 
<<results=hide>>=
x<-orderBy(~Temp+Month, data=airquality,decreasing=T)
@ %def 
The first lines of the result are:
@ 
<<echo=F>>=
head(x)
@ %def  


Again, this can clearly be achieved in other ways, but presumably not
with so few commands as above. 


\section*{The \code{splitBy} function} 
\label{splitBy}

Suppose we want to split data into a list of dataframes, e.g.\ one
dataframe for each month. This can be achieved by:
@ 
<<results=hide>>=
x<-splitBy(~Month, data=airquality)
@ %def 

Information about the grouping is stored as a dataframe 
in an attribute called \code{groupid}:
<<>>=
attr(x,"groupid")
@ %def 


\section*{The \code{sampleBy} function} 
\label{sampleBy}

Suppose we want a random sample of 50 \% of the observations from a
dataframe. This can be achieved with:
@ 
<<results=hide>>=
sampleBy(~1, frac=0.5, data=airquality)
@ %def 

Suppose instead that we want a  systematic sample of  every fifth
observation within each month. This is achieved with:
@ 
<<results=hide>>=
sampleBy(~Month, frac=0.2, data=airquality,systematic=T)
@ %def 



\section*{The \code{subsetBy} function} 
\label{subsetBy}

Suppose we want to take out those rows within each month for which the the
wind speed is larger than the mean wind speed (within the month). This
is achieved by:
@ 
<<results=hide>>=
subsetBy(~Month, subset='Wind>mean(Wind)', data=airquality)
@ %def 
Note that the statement \verb+"Wind>mean(Wind)"+ is evaluated within
each month.

\section*{The \code{esticon} function} 
\label{esticon}

Consider a linear model which explains \code{Ozone} as a linear
function of \code{Month} and \code{Wind}:
@ 
<<>>=
airquality <- transform(airquality, Month=factor(Month))
m<-lm(Ozone~Month*Wind, data=airquality)
coefficients(m)
@ %def 

When a parameter vector $\beta$ of (systematic) effects have been
estimated, interest is often in a particular estimable function, i.e.\
linear combination $\lambda^\top \beta$ and/or testing the hypothesis 
$H_0: \lambda^\top \beta=\beta_0$ where $\lambda$ is a specific vector
defined by the user.

Suppose for example we want to calculate the expected difference in
ozone between consequtive months at wind speed 10 mph (which is about
the average wind speed over the whole period).

The \code{esticon} function provides a way of doing so. 
 We can specify several $\lambda$ vectors at the same time
by row--binding of $\lambda^\top$: 
@ 
<<>>=
Lambda <- rbind(
  c(0,-1,0,0,0,0,-10,0,0,0),
  c(0,1,-1,0,0,0,10,-10,0,0),
  c(0,0,1,-1,0,0,0,10,-10,0),
  c(0,0,0,1,-1,0,0,0,10,-10)
  )
esticon(m, Lambda
)
@ %def 

In other cases, interest is in testing a hypothesis of a contrast
$H_0: \Lambda \beta=\beta_0$ where $\Lambda$ is a matrix. For example
a test of no interaction between \code{Month} and \code{Wind} can be
made by testing jointly that the last four parameters in \code{m} are
zero (observe that the test is a Wald test): 
@ 
<<>>=
Lambda <- rbind(
  c(0,0,0,0,0,0,1,0,0,0),
  c(0,0,0,0,0,0,0,1,0,0),
  c(0,0,0,0,0,0,0,0,1,0),
  c(0,0,0,0,0,0,0,0,0,1)
  )
esticon(m, Lambda, joint.test=T)
@ %def 

For a linear normal model, one would typically prefer to do a
likelihood ratio test instead. However, for generalized estimating
equations of glm--type (as dealt with in the packages \pkg{geepack}
and \pkg{gee}) there is no likelihood. In this case \code{esticon}
function provides an operational alternative.

Observe that another function for calculating contrasts as above is the
\code{contrast} function in the \pkg{Design} package but it applies to
a narrower range of models than \code{esticon} does.



% Consider:
% @ 
% <<>>=
% data(iris)
% lm1  <- lm(Sepal.Length~Sepal.Width*Species, data=iris)
% #coef(summary(lm1))[,1]
% names(coef(lm1))
% @ %def 

% Suppose we want to 1) estimate the intercept for versicolor

\section*{Final remarks} 
\label{discussion}

The ease in using the data oriented functions lies in that 1) the
formula language is used in the specification of both the variables
and the grouping and 2) the functions take a \code{data} argument. My
``biologically oriented'' students (and ditto collegues) seem to
appreciate that.

On their wishlist are facilities along the line of the \verb+ESTIMATE+
and \verb+LSMEANS+ statements available in many SAS procedures
(including \code{GLM}, \code{MIXED} and \code{GENMOD}) for easy
specification of various contrasts (\verb+LSMEANS+ is sometimes also
denoted ``population means''). While \verb+LSMEANS+ are often misused
(and certainly misinterpreted) such facilities would be nice to have
in R.  I would like to encourage anyone who has implemented such
facilities in a reasonable level of generality to come forward.

% In this connection I would like to raise another issue: There are
% several packages with various ``general purpose'' utilities on
% CRAN already -- and \doby\ is yet another one.  Perhaps it would be an
% idea to think of way of combining such packages in one way or
% another. 


@ 
<<echo=F>>=
options(oopt)
@ %def 


Søren Højsgaard \\
Statistics and Decision Analysis Unit\\
Department of Genetics and Biotechnology\\
Danish Institute of Agricultural Sciences\\
DK--8830 Tjele, Denmark\\
sorenh@agrsci.dk
\end{article}

\end{document}
