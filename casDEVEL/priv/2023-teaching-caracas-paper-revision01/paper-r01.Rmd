---
title: >
  Computer Algebra in R Bridges a Gap Between Mathematics and Data in the Teaching of Statistics and Data Science
date: "2023-08-11"
abstract: >
  The capability of R to do symbolic mathematics is enhanced by the `caracas` package. 
  This package uses the Python computer algebra library SymPy as a back-end 
  but `caracas` is tightly integrated in the R environment. This enables the R user 
  with symbolic mathematics within R at a high abstraction level rather than using text strings and 
  text string manipulation as  the case would be if using SymPy from R directly. 
  We demonstrate how mathematics and statistics can benefit from 
  bridging computer algebra and data via R. 
  This is done thought a number of examples and we propose some topics 
  for small student projects.
  The `caracas` package integrates well with e.g. `Rmarkdown`, 
  and as such creation of scientific reports and teaching is supported. 
draft: true
author:  
  - name: Mikkel Meyer Andersen
    affiliation: Department of Mathematical Sciences, Aalborg University, Denmark
    address:
    - Skjernvej 4A
    - 9220 Aalborg Ø, Denmark
    email:  mikl@math.aau.dk
    orcid: 0000-0002-0234-0266
  - name: Søren Højsgaard
    affiliation: Department of Mathematical Sciences, Aalborg University, Denmark
    address:
    - Skjernvej 4A
    - 9220 Aalborg Ø, Denmark
    email:  sorenh@math.aau.dk
    orcid: 0000-0002-3269-9552
always_allow_html: true
type: package
output: 
  rjtools::rjournal_pdf_article:
    self_contained: yes
    toc: yes
bibliography: RJreferences.bib
---




```{r setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
knitr::opts_chunk$set(echo = TRUE, cache = !TRUE, message = FALSE,
                      fig.height = 3, fig.width = 5, prompt = TRUE)
library(caracas)
options(caracas.print.method = "prettyascii")
options(caracas.prompt = "[c]: ")
options("digits" = 3)

library(knitr)
library(kableExtra)
```

```{r, echo=FALSE}
inline_code <- function(x) {
  x
}

texdot <- function(x) {
  #tex(x, zero_as_dot = TRUE, matstr = c("matrix", "r"))
  tex(x, zero_as_dot = TRUE)
}
```

# Introduction

The \CRANpkg{caracas} package [@caracas:21] and the \CRANpkg{Ryacas}
package [@ryacas] enhance the capability of R to handle
symbolic mathematics. In this paper we will illustrate the use of the
`caracas` package (version 2.0.1) in connection with teaching mathematics and
statistics.  Focus is on 1) treating statistical models symbolically,
2) on bridging the gap between symbolic mathematics and numerical
computations and 3) on preparing teaching material in a reproducible
framework (provided by, e.g. \CRANpkg{rmarkdown} [@rmarkdown;
@RMarkdownDefinitiveGuide; @RMarkdownCookbook]). The `caracas` package
is available from CRAN.  The open-source development version of
`caracas` is available at <https://github.com/r-cas/caracas> and
readers are recommended to study the online documentation at
<https://r-cas.github.io/caracas/>.  The `caracas` package provides an
interface from R to the Python package SymPy [@sympy]. This
means that SymPy is "running under the hood" of R via the
`reticulate` package [@reticulate].  The SymPy package is mature and
robust with many users and developers.

The benefit of using `caracas` instead of using SymPy via `reticulate` 
is that work is performed at a higher abstraction level in a session of 
operations and not coding at a lower-level using text strings and 
text string manipulation. 

Neither `caracas` nor `Ryacas` are as powerful as some
of the larger commercial computer algebra systems (CAS).  The virtue of
`caracas` and `Ryacas` lie elsewhere:
(1) Mathematical tools like equation solving, summation, limits, symbolic linear
   algebra, outputting in tex format etc. are directly available from
   within R.
(2) The packages enable working with the same language and in the same
   environment as the user does for statistical analyses.
(3) Symbolic mathematics can easily be combined with data which is
   helpful in e.g. numerical optimization.
(4) The packages are open-source and  therefore support e.g. education - also for people
   with limited economical means and thus contributing to United
   Nations sustainable development goals [@UN17].

The paper is organized in the following sections: The section
[Introducing `caracas`] briefly introduces
the `caracas` package and its syntax, including how `caracas` can be
used in connection with preparing texts, e.g. teaching material.  More
details are provided in [Appendix].
Several vignettes illustrating `caracas` are provided and they are
also available online, see <https://r-cas.github.io/caracas/>.  The
section [Statistics examples] is the main section of the paper and
here we present a sample of statistical models where we believe that a
symbolic treatment is a valuable supplement to a numerical in
connection with teaching.  The section [Possible topics to study]
contains suggestions about hand-on activities for students.  Lastly,
the section [Discussion and future work] contains a discussion of the
paper.


# Introducing `caracas` 

Introduce key concepts and show functionality subsequently needed in the section [Statistics examples].


## Documents with mathematical content

A LaTeX rendering of a `caracas` symbol, say `x` is obtained by typing
`` $$x = `r knitr::inline_expr("tex(x)")`$$ ``. This feature is useful
when creating documents with a mathematical content and has been used
extensively throughout this paper (looks nice and saves space).



## Symbols

A `caracas` symbol is a list with a `pyobj` slot and the class
`caracas_symbol`.  The `pyobj` is a Python object (often a SymPy
object).  As such, a `caracas` symbol (in R) provides a handle to a
Python object.  In the design of `caracas` we have tried to make
this distinction something the user should not be concerned with, but
it is worthwhile being aware of the distinction. Whenever we refer to
a symbol we mean a `caracas` symbol.  Two functions that create
symbols are `def_sym()` and `as_sym()`; these and other functions that
create symbols will be illustrated below.


## Linear algebra

We create a symbolic matrix from an R object and a symbolic
vector directly. A vector is a one-column matrix which is printed as
its transpose to save space. Matrix products are computed using the
`%*%` operator:


```{r}
M0 <- toeplitz(c("a", "b"))  ## Character matrix
M  <- as_sym(M0)             ## as_sym() converts to a caracas symbol
v  <- vector_sym(2, "v")     ## vector_sym creates symbolic vector
y  <- M %*% v
Minv <- inv(M) %>% simplify()
v2 <- Minv %*% y  |> simplify()
```

Default printing of `M` is

```{r}
M
```

while the LaTeX rendering of the symbols above are:

$$
M = `r tex(M)`; \; 
v = `r tex(v)`; \;
y = `r tex(y)`; \;
M^{-1} = `r tex(Minv)`; \;
v2 = `r tex(v2)` . 
$$




The determinant of $M$, $det(M)=`r det(M)`$, can be factored out of
the matrix by dividing each entry with the determinant and multiplying
the new matrix by the determinant which simplifies the appearance of
the matrix:


```{r}
Minv_fact <- as_factor_list(1 / det(M), simplify(det(M) * Minv))
```

Hence we have in LaTeX format:

$$
\quad M^{-1} = `r tex(Minv_fact)` = `r tex(Minv)` .
$$


A `caracas` symbol can be coerced to an R expression
using `as_expr()`. 
Symbols can be substituted with other symbols or with numerical values
using `subs()`:

```{r}
as_expr(M)
def_sym(a) ## This creates the symbol 'a'
a
M2 <- subs(M, "b", "a^2")
M3 <- subs(M2, a, 2)
```

$$
M2 = `r tex(M2)`; \quad
M3 = `r tex(M3)`.
$$






## Calculus

Next, we define a `caracas` symbol `x` and 
subsequently a `caracas` polynomial `p` in `x` (`p` becomes a symbol because `x` is):

```{r}
def_sym(x)  
p <- 1 - x^2 + x^3 + x^4/4 - 3 * x^5 / 5 + x^6 / 6
```

We investigate `p` further by finding the gradient and Hessian of `p`. The gradient factors which shows that the stationary
points are $-1$, $0$, $1$ and $2$:

```{r}
g <- der(p, x) 
g2 <- factor_(g)
h <- der2(p, x)
```

Notice here: Several functions have a postfix underscore as a simple
way of distinguishing them from R functions with a different
meaning.

$$
 \texttt{g}  = `r tex(g)`; \quad 
 \texttt{g2}  = `r tex(g2)`.
$$

In a more general setting we can find the stationary points by equating the gradient to zero:
The output `sol` is a list of solutions in which each solution is a list of `caracas` symbols. 

```{r}
sol <- solve_sys(lhs = g, rhs = 0, vars = x)
sol
sol_expr <- sapply(sol, sapply, as_expr) |> unname()
sol_expr
```


A `caracas` symbol can be turned into an R function for subsequent
numerical evaluation using `as_func()`, see
Fig. \@ref(fig:calculus). 
The stationary points are  indicated in the plots.

```{r, eval=T}
p_fn <- as_func(p)
p_fn
g_fn <- as_func(g)
h_fn <- as_func(h)
h_fn(sol_expr)
```

The sign of the Hessian in the stationary points shows that $-1$ and
$2$ are local minima, $0$ is a local maximum and $1$ is an inflection
point.



```{r calculus, echo = TRUE, fig.cap="Left: A polynomium. Center: The gradient. Right: The Hessian.", echo=FALSE, fig.height=1.3, fig.width=5, layout = "l-body-outset"}
dat <- data.frame(x=seq(-1.2,2.2,0.1))
dat$p <- p_fn(dat$x)
dat$g <- g_fn(dat$x)
dat$h <- h_fn(dat$x)
stationary_points <- c(-1, 0, 1, 2)
library(ggplot2)
theme_set(theme_bw())
p1 <- dat |> ggplot(aes(x, p)) + geom_line() + geom_vline(xintercept = stationary_points, col='red', linetype=4)
p2 <- dat |> ggplot(aes(x, g)) + geom_line() + geom_vline(xintercept = stationary_points, col='red', linetype=4)
p3 <- dat |> ggplot(aes(x, h)) + geom_line() + geom_vline(xintercept = stationary_points, col='red', linetype=4)
cowplot::plot_grid(p1, p2, p3, nrow=1)
```



## Integration

The unit circle is given by $x^2 + y^2 = 1$ so the area of the upper
half of the unit circle is $\int_{-1}^1 \sqrt{1-x^2}\; dx$ (which is
known to be $\pi/2$).  This result is produced by `caracas` while the
`integrate` function in R produces the approximate result $1.57$.

```{r}
x <- as_sym("x")
half_circle_ <- sqrt(1-x^2)
ad <- int(half_circle_, "x")          ## Anti derivative
area <- int(half_circle_, "x", -1, 1) ## Definite integral
```

$$
\texttt{ad} = `r tex(ad)`; \quad
\texttt{area} = `r tex(area)`.
$$ 


<!-- ## Rosenbrock function -->

<!-- [FIXME: More fun than polynomium??] -->

<!-- ```{r, echo=T, eval=T} -->
<!-- def_sym(x, y, a, b) -->
<!-- rosen <- (a - x)^2 + b*(y-x^2)^2 -->
<!-- grad <- der(rosen, c(x, y)) -->
<!-- ## gradient(rosen, c(x, y)) ?? STRANGE -->
<!-- hess <- der2(rosen, c(x, y)) -->
<!-- sol <- solve_sys(grad, c(x,y)) -->
<!-- sol -->
<!-- sol_expr <- sapply(sol, sapply, as_expr) |> unname() -->
<!-- sol_expr -->
<!-- ```  -->

## Unevaluated expressions


Finally, we illustrate creation of unevaluated expressions:

```{r}
def_sym(x, n)
y <- (1 + x/n)^n
l <- lim(y, n, Inf, doit = FALSE)
l_2 <- doit(l)
```

$$
l = `r tex(l)`; \quad l_2 = `r tex(l_2)`
$$


Several functions have the `doit` argument, e.g. `lim()`, `int()` and `sum_()`. 
Unevaluated expressions help making reproducible documents where the changes 
in code appears automatically in the generated formulas. 






















# Statistics examples

In this section we examine larger statistical examples and 
demonstrate how `caracas` can help improve understanding of the models.

## Example: Linear models

A matrix algebra approach to e.g. linear models is very clear and
concise. On the other hand, it can also be argued that matrix algebra
obscures what is being computed. Numerical examples are useful for
some aspects of the computations but not for other. In this respect
symbolic computations can be enlightening. 

Consider a two-way analysis of variance (ANOVA) with one observation
per group, see Table \@ref(tab:anova-two-way-table).

```{r anova-two-way-table, echo=F}
tt <- structure(c("$y_{11}$", "$y_{21}$", "$y_{12}$", "$y_{22}$"), 
                dim = c(2L, 2L), 
                dimnames = list(NULL, NULL))
tt %>%
  kable(col.names = NULL, escape = FALSE, caption = "Two-by-two layout of data.") %>%
  kable_styling(bootstrap_options = "bordered",
                latex_options = "hold_position",
                full_width = FALSE) %>%
  column_spec(column = 1,
              border_left = TRUE) %>%
  column_spec(column = 2,
              border_right = TRUE) 
```

```{r}
nr <- 2
nc <- 2
y  <- as_sym(c("y_11", "y_21", "y_12", "y_22"))
dat <- expand.grid(r = factor(1:nr), s = factor(1:nc))
X <- model.matrix(~ r + s, data = dat) |> as_sym()
b <- vector_sym(ncol(X), "b")
mu <- X %*% b
```

For the specific model we have random variables $y=(y_{ij})$. All
$y_{ij}$s are assumed independent and $y_{ij}\sim N(\mu_{ij}, v)$.
The corresponding mean vector $\mu$ has the form given below:

$$
y = `r tex(y)`, \quad X=`r texdot(X)`, \quad b=`r tex(b)`, \quad  \mu = X b = `r tex(mu)` .
$$

Above and elsewhere, dots represent zero. This is obtained with the `zero_as_dot` argument to the `tex()` function. 
The least squares estimate of $b$ is the  vector $\hat{b}$ that minimizes $||y-X
b||^2$ which leads to the normal equations $(X^\top X)b =
X^\top y$ to be solved. If $X$ has full rank, the unique solution to the normal
equations is $\hat{b} = (X^\top X)^{-1} X^\top y$. Hence the
estimated mean vector is $\hat \mu = X\hat{b}=X(X^\top X)^{-1} X^\top
y$. Symbolic computations are
not needed for quantities involving only the model matrix $X$, but
when it comes to computations involving $y$, a symbolic treatment of
$y$ is useful:

```{r}
XtX <- t(X) %*% X
XtXinv <- inv(XtX)
Xty <- t(X) %*% y
b_hat <- XtXinv %*% Xty
```


\begin{align}
X^\top y &= `r tex(Xty)`; \quad 
\quad
\hat{b} = `r tex(mat_factor_div(b_hat, 2))`.
\end{align}



Hence $X^\top y$ (a sufficient reduction of data if the variance is
known) consists of the sum of all observations, the sum of
observations in the second row and the sum of observations in the
second column. For $\hat{b}$, the second component is, apart from a
scaling, the sum of the second row minus the sum of the first
row. Likewise, the third component is the sum of the second column
minus the sum of the first column. Hence, for example the second
component of $\hat{b}$ is the difference in mean between the first and
second column in Table \@ref(tab:anova-two-way-table).




## Example: Logistic regression

In the following we go through details of a logistic regression model,
see e.g. @mccullagh for a classical description of logistic
regression.

As an example, consider the `budworm` data from the \CRANpkg{doBy} package [@doBy].
The data shows the number of killed moth tobacco budworm
\emph{Heliothis virescens}.  Batches of 20 moths of each sex were
exposed for three days to the pyrethroid and the number in each batch
that were dead or knocked down was recorded. 
Below we focus only on male budworms and the mortality is illustrated
in Figure \@ref(fig:budworm) (produced with \CRANpkg{ggplot2} [@ggplot2]).  On the $y$-axis we have the empirical
logits, i.e. $\log((\text{ndead} + 0.5)/(\text{ntotal}-\text{ndead} +
0.5))$. The figure suggests that logit grows linearly with log dose.


```{r}
data(budworm, package = "doBy")
bud <- subset(budworm, sex == "male")
bud
```

```{r budworm, echo = TRUE, fig.cap="Insecticide mortality of the moth tobacco budworm.", echo=FALSE, fig.height=2.0, fig.width=5, layout = "l-body-outset"}
library(ggplot2)
theme_set(theme_bw())
p1<-bud  %>% ggplot(aes(x=dose, y=log((ndead+0.5) / (ntotal - ndead + 0.5)))) +
    geom_point() + geom_line() + labs(y = "Empirical logits")
p2<-bud  %>% ggplot(aes(x=log2(dose), y=log((ndead + 0.5) / (ntotal - ndead + 0.5)))) +
    geom_point() + geom_line() + labs(y = "Empirical logits")
cowplot::plot_grid(p1, p2, nrow=1)
```







Observables are binomially distributed, $y_i \sim \text{bin}(p_i,
n_i)$. The probability $p_i$ is connected to a $q$-vector of
covariates $x_i=(x_{i1}, \dots, x_{iq})$ and a $q$-vector of
regression coefficients $b=(b_1, \dots, b_q)$ as follows: The term
$s_i = x_i \cdot b$ is denoted the \emph{linear predictor}. The
probability $p_i$ can be linked to $s_i$ in different ways, but the
most commonly employed is via the \emph{logit link function} which is
$\text{logit}(p_i) = \log(p_i/(1-p_i))$ so here $\text{logit}(p_i) =
s_i$.  Based on Figure \@ref(fig:budworm), we consider the specific
model with $s_i = b_1 + b_2 \log2(dose_i)$. For later use, we define the data matrix below:

```{r}
DM <- cbind(model.matrix(~log2(dose), data=bud),
            bud[, c("ndead", "ntotal")])  |> as.matrix()
DM |> head(3)
```

### Each component of the likelihood

The log-likelihood is $\log L=\sum_i y_i \log(p_i) + (n_i-y_i)
\log(1-p_i) = \sum_i \log L_i$, say. 
Consider the contribution to the total log-likelihood from the $i$th
observation which is $\log L_i = l_i = y_i \log(p_i) + (n_i-y_i) \log(1-p_i)$.
Since we are focusing on one observation only, we shall ignore the
subscript $i$ in this section. First notice that with 
$s = \log(p/(1-p))$ we can find $p$ as:

```{r}
def_sym(s, p)
sol_ <- solve_sys(lhs = log(p / (1 - p)), rhs = s, vars = p)
p_s <- sol_[[1]]$p
```

$$
\texttt{p\_s} = `r tex(p_s)`
$$


Next, find the likelihood as a function of $p$, as a function of $s$
and as a function of $b$.  The underscore in `logLb_` and elsewhere
indicates that this expression is defined in terms of other
symbols. The log-likelihood can be maximized using e.g. Newton-Rapson
(see e.g. @nocedal) and in this connection we need the score function,
$S$, and the Hessian, $H$:



```{r}
def_sym(y, n)
b  <- vector_sym(2, "b")
x  <- vector_sym(2, "x")
logLp_ <- y * log(p) + (n - y) * log(1 - p) ## logL as fn of p
s_b <- sum(x * b)                           ## s as fn of b
p_b <- subs(p_s, s, s_b)                    ## p as fn of b
logLb_ <- subs(logLp_, p, p_b)              ## logL as fn of b
Sb_ <- score(logLb_, b) |> simplify()
Hb_ <- hessian(logLb_, b) |> simplify()
```

\begin{align}
\texttt{p\_b}   &= `r tex(p_b)`, \\
\texttt{logLb}\_ &= `r texdot(logLb_)`, \\
\texttt{Sb}\_    &= `r texdot(Sb_)`, \\
\texttt{Hb}\_    &= `r texdot(Hb_)` . 
\end{align}


There are various possible approaches from here when it comes
maximizing the total log likelihood. One is to insert data case by
case into the symbolic log likelihood. This yields a list of new `caracas`
symbol which depends on the unknown regression parameters:

```{r}
nms <- c("x1", "x2", "y", "n")
DM_lst <- doBy::split_byrow(DM)
logLb_lst <- lapply(DM_lst, function(vls) {
    subs(logLb_, nms, vls)
})
```

For example, the contribution from the third observation to the total log likelihood is:

\begin{align}
\texttt{logLb\_lst[[3]]}  &= `r tex(logLb_lst[[3]])`.
\end{align}

These
symbols can be added up and the sum can be maximized either e.g.\
using SymPy (not pursued here) or by converting the sum to an R
function which can be maximized using one of R's internal
optimization procedures:

```{r}
logLb_tot <- Reduce(`+`, logLb_lst) 
logLb_fn  <- as_func(logLb_tot, vec_arg = TRUE)
opt <- optim(c(b1=0, b2=0), logLb_fn, control = list(fnscale = -1), hessian = TRUE)
opt$par
```

The same model can be fitted e.g. using R's `glm()` function as follows (output omitted):

```{r, eval=T}
m <- glm(cbind(ndead, ntotal - ndead) ~ log2(dose), family=binomial(), data=bud)
m |> coef()
```




### The total likelihood symbolically

We conclude this section by illustrating that the log-likelihood for the entire dataset 
can be constructed in a few steps (output is omitted to save space):

```{r}
N <- 6; q <- 2
X <- matrix_sym(N, q, "x")
n <- vector_sym(N, "n")
y <- vector_sym(N, "y")
p <- vector_sym(N, "p")
s <- vector_sym(N, "s")
b <- vector_sym(q, "b")
```

$$
 X=`r tex(X)`, \quad
 n=`r texdot(n)`, \quad
 y=`r texdot(y)` .
$$


The symbolic computations are as follows: We express the linear predictor $s$ as function of the regression coefficients $b$ and express the probability $p$ as function of the linear predictor:

```{r}
logLp <- sum(y * log(p) + (n - y) * log(1 - p)) ## logL as fn of p
p_s <- exp(s) / (exp(s) + 1)                    ## p as fn of s
s_b <- X %*% b                                  ## s as fn of b
p_b <- subs(p_s, s, s_b)                        ## p as fn of b
logLb_ <- subs(logLp, p, p_b)                   ## logL as fn of b
```

Next step could be to go from symbolic to numerical computations by
inserting numerical values. From here, one may proceed by computing
the score function and the Hessian matrix and solve the score
equation, using e.g. Newton-Rapson. Alternatively, one might create an
R function based on the log-likelihood, and maximize this function
using one of R's optimization methods (see the example in the
previous section):


```{r, eval=T}
logLb <- subs(logLb_, cbind(X, y, n), DM)
logLb_fn <- as_func(logLb, vec_arg = TRUE)
opt <- optim(c(b1=0, b2=0), logLb_fn, control = list(fnscale = -1), hessian = TRUE)
opt$par
```

<!-- An alternative would have been to define `logLp` above in terms of -->
<!-- `n.` and `y.` and similarly define `s_` in terms of `X.` If doing so,  -->
<!-- the last step above where numerical values are inserted could have been -->
<!-- avoided. -->


<!-- ```{r} -->
<!-- s_ <- X. %*% b -->
<!-- logLp <- sum(y. * log(p) + (n. - y.) * log(1 - p)) -->
<!-- logLs <- subs(logLp, p, p_) -->
<!-- logLb <- subs(logLs, s, s_) -->
<!-- ``` -->



## Example: Constrained maximum likelihood


In this section we illustrate constrained optimization using Lagrange multipliers. 
This is demonstrated for the independence model for a two-way contingency table.
Consider a $2 \times 2$ contingency table with cell 
counts $y_{ij}$ and cell probabilities $p_{ij}$ for $i=1,2$ and $j=1,2$, 
where $i$ refers to row and $j$ to column as 
illustrated in Table \@ref(tab:anova-two-way-table).

Under multinomial sampling, the log likelihood is
$$
 l = \log L = \sum_{ij} y_{ij} \log(p_{ij}).
$$

Under the assumption of independence between rows and columns, the cell
probabilities have the form, (see e.g. @hojsgaard, p. 32)
$$
p_{ij}=u \cdot r_i \cdot s_j.
$$ 

To make the parameters $(u, r_i, s_j)$ identifiable, constraints
must be imposed. One possibility is to require that $r_1=s_1=1$. The
task is then to estimate $u$, $r_2$, $s_2$ by maximizing the log likelihood
under the constraint that $\sum_{ij} p_{ij} = 1$.  These constraints
can be
imposed using a Lagrange multiplier where we  solve the
unconstrained optimization problem $\max_p Lag(p)$ where
\begin{align}
  Lag(p) &= -l(p) + \lambda g(p) \quad \text{under the constraint that} \\
  g(p) &= \sum_{ij} p_{ij} - 1 = 0 ,
\end{align}
where $\lambda$ is a Lagrange multiplier. 
In SymPy, `lambda` is a reserved symbol. Hence the underscore as postfix below: 

```{r}
def_sym(u, r2, s2, lambda_)
y  <- as_sym(c("y_11", "y_21", "y_12", "y_22"))
p  <- as_sym(c("u", "u*r2", "u*s2", "u*r2*s2"))
logL <- sum(y * log(p))
Lag  <- -logL + lambda_ * (sum(p) - 1) 
vars <- list(u, r2, s2, lambda_)
gLag <- der(Lag, vars)
sol  <- solve_sys(gLag, vars)
print(sol, method = "ascii")
sol <- sol[[1]]
```

There is only one critical point. Fitted cell probabilities $\hat p_{ij}$ are:

```{r}
p11 <- sol$u
p21 <- sol$u * sol$r2
p12 <- sol$u * sol$s2
p22 <- sol$u * sol$r2 * sol$s2
p.hat <- matrix_(c(p11, p21, p12, p22), nrow = 2)
```

\[
\hat p = `r tex(mat_factor_div(p.hat, denominator(p.hat[1,1])))`
\]

To verify that the maximum likelihood estimate has been found, we compute the Hessian matrix 
which is negative definite (the Hessian matrix is diagonal so the eigenvalues are the diagonal entries and these are all negative), output omitted:

```{r}
H <- hessian(logL, list(u, r2, s2)) |> simplify()
```

<!--
FIXME: eval = FALSE for faster compilation
-->
```{r, echo=FALSE, eval=FALSE}
rv <- vector_sym(2, "r")
sv <- vector_sym(2, "s")
m <- rv %*% t(sv)
p <- u*m |> vec()
l <- sum(n * log(p))
```

<!--
FIXME: eval = FALSE for faster compilation
-->
```{r, echo=FALSE, eval=FALSE}
def_sym(a, a1, a2, u, r1, r2, s1, s2)
p <- as_sym(c("u*r1*s1", "u*r2*s1", "u*r1*s2", "u*r2*s2"))
l  <- sum(y * log(p))
L  <- -l + a * (sum(p) - 1) + a1 * (r1 - 1) + a2 * (s1 - 1)
vars <- list(u, r2, s2, a, a1, a2)
gL <- der(L, vars)
sol <- solve_sys(gL, vars)
sol <- sol[[1]]
```


## Example: An auto regression model 

### Symbolic computations

```{r, echo=F}
e <- as_sym(paste0("e", 0:3))
x <- as_sym(paste0("x", 0:3))
u <- vector_sym(3, "u")
y <- vector_sym(3, "y")
eu <- rbind(e, u)
xy <- rbind(x, y)
```

```{r, echo=FALSE}
L <- diff_mat(4, "-a")
```

In this section we study the auto regressive model of order $1$ (an AR(1) model), see
e.g. @shumway:etal:16, p. 75 ff. for details: 
Consider random variables $x_1, x_2, \dots, x_n$ following a stationary zero mean AR(1) process: 

\begin{equation}
  x_i = a x_{i-1} + e_i; \quad i=2, \dots, n,
  (\#eq:ar1)
\end{equation}

where $e_i \sim N(0, v)$ and all $e_i$s are independent. Note that $v$ denotes the variance.
The marginal distribution of $x_1$ is also assumed normal, and for the process to be stationary 
we must have that the variance $\mathbf{Var}(x_1) = v / (1-a^2)$. 
Hence we can write $x_1 = \frac 1 {\sqrt{1-a^2}} e_1$. 

```{r, echo=F}
n <- 4
def_sym(a)
x <- vector_sym(n, "x")
e <- vector_sym(n, "e")
L <- diff_mat(n, "-a")
L[1, 1] <- sqrt(1-a^2)
```

For simplicity of exposition, we set $n=4$. All terms $e_1, \dots,
e_4$ are independent and $N(0, v)$ distributed.  Let $e=(e_1, \dots,
e_4)$ and $x=(x_1, \dots x_4)$. Hence $e \sim N(0, v I)$.  Isolating
error terms in \@ref(eq:ar1) gives

$$
  e= `r inline_code(tex(e))` = `r inline_code(texdot(L))` `r inline_code(tex(x))` = L x  .
$$

Since
$\mathbf{Var}(e)=v I$ we have $\mathbf{Var}(e)=v I=L \mathbf{Var}(x)
L^\top$ so the covariance matrix of $x$ is $V=\mathbf{Var}(x) = v L^-
(L^-)^\top$ while the concentration matrix (the inverse covariance 
matrix) is $K=v^{-1}L^\top L$: 

```{r}
def_sym(a, v)
n <- 4
L <- diff_mat(n, "-a")
L[1, 1] <- sqrt(1-a^2)
Linv <- inv(L)
K <- crossprod_(L) / v
V <- tcrossprod_(Linv) * v
```

```{r, echo=F, results="asis"}
cat(
  "\\begin{align} 
    L^{-1} &= ", texdot(Linv), " , \\\\ 
    K &= ", texdot(mat_factor_div(K, v)), " , \\\\ 
    V &= ", texdot(mat_factor_mult(V, v)), "  .
  \\end{align}", sep = "")
```

The zeros in the concentration matrix $K$ implies a conditional
independence restriction: If the $ij$th element of a concentration
matrix is zero then $x_i$ and $x_j$ are conditionally independent
given all other variables, see e.g. @hojsgaard, p. 84 for
details.

Next, we take the step from symbolic computations to numerical
evaluations.  The joint distribution of $x$ is multivariate normal
distribution, $x\sim N(0, K^{-1})$. Let $W=x x^\top$ denote the
matrix of (cross) products.  The log-likelihood is therefore (ignoring
additive constants)
$$ 
\log L = \frac n 2 (\log \mathbf{det}(K) - x^\top K x) = \frac n 2 (\log \mathbf{det}(K) - \mathbf{tr}(K W)), 
$$ 
where we note that $\mathbf{tr}(KW)$ is the
sum of the elementwise products of $K$ and $W$ since both matrices are
symmetric. Ignoring the constant $\frac n 2$, 
this can be written symbolically to obtain the expression in 
this particular case:

```{r}
x <- vector_sym(n, "x")
logL <- log(det(K)) - sum(K * (x %*% t(x))) |> simplify()
```

$$
\log L = `r tex(logL)` .
$$


### Numerical evaluation 

Next we illustrate how bridge the gap from symbolic computations to numerical computations based on a dataset:
For a specific data vector we get:

```{r}
xt <- c(0.1, -0.9, 0.4, 0.0)
logL. <- subs(logL, x, xt) 
```

$$
\log L = `r tex(logL.)` .
$$


We can use R for numerical maximization of the likelihood and constraints on the 
parameter values can be imposed e.g. in the `optim()` function:

```{r}
logL_wrap <- as_func(logL., vec_arg = TRUE)
eps <- 0.01
par <- optim(c(a=0, v=1), logL_wrap, 
             lower=c(-(1-eps), eps), upper=c((1-eps), 10),
             method="L-BFGS-B", control=list(fnscale=-1))$par
par
```

The same model can be fitted e.g. using R's `arima()` function as follows (output omitted):

```{r, eval=FALSE}
arima(xt, order = c(1, 0, 0), include.mean = FALSE, method = "ML")
```

It is less trivial to do the optimization in `caracas` by solving the score equations. 
There are some possibilities for putting assumptions on variables
in `caracas` (see the "Reference" vignette), but 
it is not possible to restrict the parameter $a$ to only take values in $(-1, 1)$. 



## Example: Variance of average of correlated variables

```{r echo=FALSE}
n <- 3
```

```{r echo=FALSE}
R <- as_sym(toeplitz(c(1, rep("r", n-1))))
# FIXME: SH? mikl: EXERCISE!
#R <- as_sym(toeplitz(rep(paste0("r^", 0:3))))
```

Consider random
variables $x_1,\dots, x_n$ where $\mathbf{Var}(x_i)=v$ and $\mathbf{Cov}(x_i,
x_j)=v r$ for $i\not = j$, where $0 \le |r| \le1$. 
For $n=`r n`$, the covariance matrix of $(x_1,\dots, x_n)$ is therefore

\begin{equation}
  \label{eq:1}
  V = v R = v `r tex(R)`. 
\end{equation}


Let $\bar x = \sum_i x_i / n$ denote the average.  Suppose interest is
in the variance of the average, $\mathbf{Var}(\bar x)$, when $n$ goes to
infinity. One approach is as follow: Let $1$ denote an $n$-vector of
$1$'s and let $V$ be an $n \times n$ matrix with $v$ on the diagonal
and $v r$ outside the diagonal.  Then $\mathbf{Var}(\bar x)=\frac 1 {n^2}
1^\top V 1$. The answer lies in studying the limiting behaviour of
this expression when $n \rightarrow \infty$. 
First, we must calculate variance of a sum $x. = \sum_i x_i$ 
which is $\mathbf{Var}(x.) = \sum_i \mathbf{Var}(x_i) + 2 \sum_{ij:i<j} \mathbf{Cov}(x_i,
x_j)$ (i.e., the sum of the elements of the covariance matrix). 
We can do this in `caracas` as follows:

```{r}
def_sym(v, r, n, j, i) 
var_sum <- v * (n + 2 * sum_(sum_(r, j, i + 1, n), i, 1, n - 1)) |> simplify()
var_avg <- var_sum / n^2
```

$$
\mathbf{Var}(x.) = `r tex(var_sum)`,
\quad
\mathbf{Var}(\bar x) = `r tex(var_avg)`.
$$

From hereof, we can study the limiting behavior of the variance
$\mathbf{Var}(\bar x)$ in different situations:

```{r}
l_1 <- lim(var_avg, n, Inf)         ## when sample size n goes to infinity
l_2 <- lim(var_avg, r, 0, dir='+')  ## when correlation r goes to zero
l_3 <- lim(var_avg, r, 1, dir='-')  ## when correlation r goes to one
```

Moreover, for a given correlation $r$ it is instructive to investigate
how many independent variables, say $k_n$ the $n$ correlated variables
correspond to (in the sense of the same variance of the average),
because then $k_n$ can be seen as a measure of the amount of information
in data.  Moreover, one might study how $k_n$ behaves as function of $n$
when $n \rightarrow \infty$.  That is we must (1) solve $v (1 +
(n-1)r)/n = v/k$ for $k$ and (2) find the limit $l_k = \lim_{n\rightarrow\infty} k_n$:

```{r}
def_sym(k_n)
sol <- solve_sys(var_avg - v / k_n, k_n)
k_n <- sol[[1]]$k_n
l_k <- lim(k_n, n, Inf)
```

The findings above are:
\[
l_1 = `r tex(l_1)`, \quad
l_2 = `r tex(l_2)`, \quad
l_3 = `r tex(l_3)`, \quad
k_n = `r tex(k_n)`, \quad 
l_k = `r tex(l_k)` .
\]

With respect to $k_n$, it is illustrative to supplement the symbolic
computations above with numerical evaluations, which shows that
even a moderate correlation reduces the effective sample size substantially:

```{r}
dat <- expand.grid(r=c(.1, .2, .5), n=c(10, 50))
k_fn <- as_func(k_n)
dat$k_n <- k_fn(r=dat$r, n=dat$n)
dat$l_k <- 1 / dat$r
dat
```


# Possible topics and projects for students

```{r echo=FALSE}
Rex5 <- as_sym(toeplitz(c(1, "r", rep(0, 3-2))))
```

1. Related to Section [Linear models]: 
   a) The orthogonal projection
      matrix onto the span of the model matrix $X$ is $P=X (X^\top
      X)^{-1}X^\top$. The residuals are $r=(I-P)y$. From this one may
      verify that these are not all independent.
   b) If one of the factors
      is ignored, then the model becomes a one-way analysis of variance
      model, at it is illustrative to redo the computations in Section
      [Linear models] in this setting. 
   c) Likewise if an interaction between the two factors 
      is included in the model. What are the residuals in this case? 
1. Related to Section [Logistic regression]: 
   a) In [Each component of the
      likelihood], Newton-Rapson can be implemented to solve the likelihood
      equations and compared to the output from `glm()`. 
      Note how sensitive Newton-Rapson is to starting point. 
      This can be solved by another optimisation scheme, e.g. 
      Nelder-Mead (optimising the log likelihood) or BFGS 
      (finding extreme for the score function).
   b) The example is done as logistic regression with the logit 
      link function. Try other link functions such as cloglog (complementary log-log).
1. Related to Section [Maximum likelihood under constraints]:
   a) Identifiability of the parameters was handled by not including
      $r_1$ and $s_1$ in the specification of $p_{ij}$. An alternative is
      to impose the restrictions $r_1=1$ and $s_1=1$, and this can also
      be handled via Lagrange multipliers. Another alternative is to regard
      the model as a log-linear model where $\log p_{ij} = \log u + \log
      r_i + \log s_j = \tilde{u} + \tilde{r}_i + \tilde{s}_j$. This model
      is similar in its structure to the two-way ANOVA for Section [Linear
      models]. This model can be fitted as a generalized linear model
      with a Poisson likelihood and $\log$ as link function. Hence, one
      may modify the results in Section [Logistic regression] to
      provide an alternative way of fitting the model.
   b) A simpler task is
      to consider a multinomial distribution with four categories,
      counts $y_i$ and cell probabilities $p_i$, $i=1,2,3,4$ where $\sum_i
      p_i=1$. For this model, find the maximum likelihood estimate for
      $p_i$ (use the Hessian to verify that the critical point is a maximum).
1. Related to Section [An $AR(1)$ model]: 
   a) Compare the estimated parameter values with those obtained from 
      the `arima()` function.
   b) Modify the model in Equation \@ref(eq:ar1) by 
     setting $x_1 = a x_n + e_1$ ("wrapping around") and see what happens
     to the pattern of zeros in the concentration matrix. 
   c) Extend the
     $AR(1)$ model to an $AR(2)$ model ("wrapping around") and 
     investigate this model along the same lines. Specifically, 
     where are the conditional independencies (try at least $n=6$)?
1. Related to Section [Variance of the average of correlated data]: It
   is illustrative to study such behaviours for other covariance
   functions. 
   Replicate the calculations for the covariance matrix of the form
\begin{equation}
  \label{eq:ex5}
  V = v R = v `r tex(Rex5)`,
\end{equation}
  i.e., a special case of a Toeplitz matrix. 
  How many independent variables, $k$, do 
  the $n$ correlated variables correspond to?


# Discussion and future work

We have presented the `caracas` package and argued that the
package extends the functionality of R significantly with respect to
symbolic mathematics.  One practical virtue of `caracas` is
that the package integrates nicely with `Rmarkdown`,
@rmarkdown, (e.g. with the `tex()` functionality) and thus
supports creating of scientific documents and teaching material. As
for the usability in practice we await feedback from users.

Another related package we mentioned in the introduction is `Ryacas`.
This package has existed for many years and is still of relevance.
`Ryacas` probably has fewer features than `caracas`. On the other
hand, `Ryacas` does not require Python (it is compiled), is faster for
some computations (like matrix inversion). Finally, the Yacas language
[@Pinkus2002; @yacas] is extendable (see e.g. the vignette
"User-defined yacas rules" in the `Ryacas` package).

One possible future development could be an R package which is
designed without a view towards the underlying engine (SymPy or Yacas)
and which then draws more freely from SymPy and Yacas. 
In this connection we mention that there are additional resources 
on CRAN such as \CRANpkg{calculus} [@JSSv104i05].

Lastly, with respect to freely available resources in a CAS context, we would
like to draw attention to `WolframAlpha`, see
<https://www.wolframalpha.com/>, which provides an online service for
answering (mathematical) queries.


# Acknowledgements

We would like to thank the R Consortium for financial support for
creating the `caracas` package, users for pin pointing aspects 
that can be improved in `caracas` and Ege Rubak (Aalborg
University, Denmark), Malte Bødkergaard Nielsen (Aalborg
University, Denmark), Poul Svante Eriksen (Aalborg 
University, Denmark), and reviewers
for constructive comments.



# Appendix

## Installation 


The `caracas` package is available on CRAN and can be installed as usual with `install.packages('caracas')`.
Please ensure that you have SymPy installed, or else install it:

```r
if (!caracas::has_sympy()) {
  caracas::install_sympy()
}
```

The `caracas` package uses the `reticulate` package (to run Python code). 
Thus, if you wish to configure your Python environment, you need to 
first load `reticulate`, then configure the Python environment, and at 
last load `caracas`. 
The Python environment can be configured as 
in `reticulate`'s "Python Version Configuration" vignette. 
Again, configuring the Python environment needs to be done before 
loading `caracas`. 
Please find further details in `reticulate`'s documentation. 

## Low-level access to engines


Since `caracas` provides essentially an R interface to SymPy using
the `reticulate` package [@reticulate], everything that can be done
with `caracas` can also be done directly in Python. In this
connection, it is recommended to refer to SymPy's elaborate
documentation at <https://docs.sympy.org>. We illustrate calling Sympy
directly in connection with finding the roots of a polynomial
derivative as done in the [calculus] section (the power operator in
Python is `**` and not `^` as in R):

```{r} 
library(reticulate)
s <- import("sympy") 
py_run_string("from sympy import *")
py_run_string("x = symbols('x')")
p <- py_eval("1 - x**2 + x**3 + x**4/4 - 3 * x**5 / 5 + x**6 / 6")
p$evalf(subs = list(x = 1))
sol <- s$solve(s$diff(p, "x"), "x")
```

We can obtain `p` in LaTeX format with the command
```{r, eval=FALSE}
s$latex(p)
```

The same can be achieved using standard R syntax with `caracas` as:

```{r, eval=FALSE}
def_sym(x)
p <- 1 - x^2 + x^3 + x^4/4 - 3 * x^5 / 5 + x^6 / 6
sol <- solve_sys(der(p, x), x)
```

Another example is from linear algebra: For a matrix $X$ find $(X^\top X)^{-1}$:

```{r}
library(reticulate)
s <- import("sympy") 
py_run_string("from sympy import *")
py_run_string("a = symbols('a')")
X_str <- "Matrix([[a, 1],[a, 1],[1, 0]])"
X <- py_eval(X_str, convert = FALSE) 
(X$T * X)$inv()
```

The same can be achieved using standard R syntax with `caracas` as:


```{r, eval=FALSE}
X <- matrix_(c("a", "1", "a", "1", "1", "0"), nrow = 3, byrow=TRUE)
(t(X) %*% X) |> inv() 
```

As seen, using SymPy directly can be powerful, but it also involves
using text strings and knowing more about Python and SymPy internals
and these requirements steepens the learning curve for both writing
and reading the code.


## Extending `caracas`

It is possible to easily extend `caracas` with additional
functionality from SymPy which we illustrate below.  This example
illustrates how to use SymPy's `diff()` function to perform univariate
differentiations multiple times. The partial derivative of $\sin(xy)$
with respect to $x$ and $y$ is found with `diff` in SymPy:

```{r}
sympy <- get_sympy()
sympy$diff("sin(x * y)", "x", "y")
```

One the other hand, the `der()` function in `caracas` finds the gradient: 

```{r}
def_sym(x, y)
f <- sin(x * y) 
der(f, list(x, y))
```

This is a design-choice in `caracas`. If we want to obtain 
the functionality from SymPy 
we can write a new function that invokes `diff` in SymPy using the 
`sympy_func()` function in `caracas`:

```{r}
der_diff <- function(expr, ...){
	 sympy_func(expr, "diff", ...)
}
der_diff(sin(x * y), x, y)
```

This latter function is especially useful if we need to find the higher-order 
derivative with respect to the same variable:

```{r, eval=FALSE}
sympy$diff("sin(x * y)", "x", 100L)
der_diff(sin(x * y), x, 100L)
```


