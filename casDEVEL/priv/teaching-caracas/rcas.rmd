---
author: Mikkel Meyer Andersen and Søren Højsgaard
title:
 "Bridging a gap between mathematics and data in teaching data science and statistics with the `R` package `caracas` for computer algebra"
header-includes:
  - \usepackage{amsmath}
  - \usepackage{natbib}
  - \usepackage{fancyvrb}
  - \DeclareMathOperator{\logit}{logit}
  - \DeclareMathOperator{\bin}{bin}
output: 
  pdf_document:
    fig_caption: yes
    keep_tex: true
    toc: no
    toc_depth: 3
    number_sections: true
    citation_package: natbib
bibliography: references.bib
fontsize: 10pt
---

<!-- \tableofcontents -->

<!---

  - name: Mikkel Meyer Andersen
    affiliation: Department of Mathematical Sciences, Aalborg University, Denmark
    address:
    - Skjernvej 4A
    - 9220 Aalborg Ø, Denmark
    email:  mikl@math.aau.dk
  - name: Søren Højsgaard
    affiliation: Department of Mathematical Sciences, Aalborg University, Denmark
    address:
    - Skjernvej 4A
    - 9220 Aalborg Ø, Denmark
    email:  sorenh@math.aau.dk

abstract: >
  The capability of \proglang{R} to do symbolic mathematics is enhanced by the \pkg{caracas} package. 
  This package uses the \proglang{Python} computer algebra library SymPy as a back-end 
  but \pkg{caracas} is tightly integrated in the \proglang{R} environment, thereby enabling the \proglang{R} user 
  with symbolic mathematics within \proglang{R}. 
  Key components of the \pkg{caracas} package are illustrated in this paper. 
  Examples are taken from statistics and mathematics. The \pkg{caracas} package integrates well with e.g. \pkg{Rmarkdown}, and as such creation of scientific reports and teaching is supported. 

keywords:
  formatted: [differentiation, factor analysis, Hessian matrix, integration, Lagrange multiplier, limit, linear algebra, principal component analysis, score function, symbolic mathematics, Taylor expansion, teaching]
  plain:     [differentiation, factor analysis, Hessian matrix, integration, Lagrange multiplier, limit, linear algebra, principal component analysis, score function, symbolic mathematics, Taylor expansion, teaching]

  # If you use tex in the formatted title, also supply version without
  plain:     "Computer Algebra in R with caracas"
  # For running headers, if needed
  short:     "\\pkg{caracas}: Computer Algebra in \\proglang{R}"

\usepackage{boxedminipage}

  rticles::jss_article:
    latex_engine: xelatex
    fig_caption: yes
    keep_tex: true
    toc: true
    toc_depth: 4
    number_sections: true
--->



<!-- \renewenvironment{Schunk}{\begin{center} -->
<!--     \begin{boxedminipage}{0.95\textwidth}\openup-1pt}{\end{boxedminipage}\end{center}} -->
<!-- \RecustomVerbatimEnvironment{Sinput}{Verbatim} -->
<!--     {fontsize=\small,xleftmargin=5mm,formatcom=\color{black},frame=single,framerule=0.1pt,numbers=left} -->
<!-- \RecustomVerbatimEnvironment{Soutput}{Verbatim} -->
<!--     {fontsize=\scriptsize,xleftmargin=5mm,formatcom=\color{black},frame=single,framerule=0.1pt,numbers=left} -->

<!-- \newlength{\fancyvrbtopsep} -->
<!-- \newlength{\fancyvrbpartopsep} -->
<!-- \makeatletter -->
<!-- \FV@AddToHook{\FV@ListParameterHook}{\topsep=\fancyvrbtopsep\partopsep=\fancyvrbpartopsep} -->
<!-- \makeatother -->

<!-- \setlength{\fancyvrbtopsep}{0pt} -->
<!-- \setlength{\fancyvrbpartopsep}{-1pt} -->


<!---
BEFORE SUBMISSION
--->
<!-- \RecustomVerbatimEnvironment{Sinput}{Verbatim}{fontsize=\scriptsize,xleftmargin=5mm,formatcom=\color{blue},frame=single,framerule=0.1pt} -->

<!-- \RecustomVerbatimEnvironment{Soutput}{Verbatim}{fontsize=\scriptsize,xleftmargin=5mm,formatcom=\color{violet},frame=single,framerule=0.1pt} -->

<!---
\RecustomVerbatimEnvironment{Sinput}{Verbatim}{xleftmargin=3mm,formatcom=\color{black}}

\RecustomVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=4mm,formatcom=\color{black}}
--->

\def\EE{\mathbf{E}}
\def\var{\mathbf{Var}}
\def\cov{\mathbf{Cov}}
\def\trace{\mathbf{tr}}
\def\det{\mathbf{det}}
\def\diag{\mathbf{diag}}
\def\proglang#1{\texttt{#1}}
\def\pkg#1{\texttt{#1}}

\def\sympy{\texttt{SymPy}}
\def\python{\texttt{Python}}
\def\caracas{\texttt{caracas}}
\def\r{\texttt{R}}


\def\bb{{b}}

<!-- \def\citet#1{\texttt{#1}} -->
<!-- \def\citep#1{\texttt{#1}} -->

\def\inv{^{-1}}
\def\transp{^\top}
\def\cip{\perp\!\!\perp}

\newcommand{\matrxr}[1]
{\left(
    \begin{array}{rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}
      #1 \\
    \end{array}
  \right)}
  
\newcommand{\matrxc}[1]
{\left(
    \begin{array}{cccccccccccccccccccccccccccccccccccc}
      #1 \\
    \end{array}
  \right)}  

\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother

\parindent0pt

```{r setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
knitr::opts_chunk$set(echo = TRUE, cache = !TRUE, message = FALSE,
                      fig.height = 3, fig.width = 5, prompt = TRUE)
library(caracas)
options(caracas.print.method = "prettyascii")
#options(caracas.prompt = "c: ")
options("digits" = 3)
```

<!-- Obsolete? -->
```{r, echo=FALSE}
inline_code <- function(x) {
  x
}

texr <- function(x) {
  tex(x, zero_as_dot = TRUE, matstr = c("matrix", "r"))
}
```


# Introduction
\label{sec:introduction}

The capability of \proglang{R} [@R] to handle symbolic mathematics is
enhanced by two add-on packages: The \pkg{caracas} package
[@caracas:21] and the \pkg{Ryacas} package [@ryacas].  In this paper
we will illustrate the use of the \pkg{caracas} package in connection with 
teaching mathematics and statistics, where symbolic
mathematics is helpful, strongly aided by the packages' ability to
enter in a reproducible framework (provided by, e.g.\ \pkg{Rmarkdown}
[@rmarkdown; @RMarkdownDefinitiveGuide; @RMarkdownCookbook]).  Focus
is on 1) treating statistical models symbolically, 2) on bridging the
gap between symbolic mathematics and numerical computations and 3) on
preparing teaching material.
The \pkg{caracas} package is available from CRAN [@R].
The open-source development version of \pkg{caracas} is available at
<https://github.com/r-cas/caracas>.  

Neither \pkg{caracas} nor \pkg{Ryacas} are as powerful as some
of the larger commercial computer algebra systems (CAS).  The virtue of
\pkg{caracas} and \pkg{Ryacas} lie elsewhere:
(1) Mathematical tools like equation solving, summation, limits, symbolic linear
   algebra, outputting in tex format etc.\ are directly available from
   within \proglang{R}.
(2) The packages enable working with the same language and in the same
   environment as the user does for statistical analyses.
(3) Symbolic mathematics can easily be combined with data which is
   helpful in e.g. numerical optimization.
(4) The packages are open-source and  therefore support e.g.\ education - also for people
   with limited economical means and thus contributing to United
   Nations sustainable development goals, cfr. [@UN17].

The paper is organized as follows:\footnote{FINISH LATER} 
Sec. \ref{sec:mathintro} introduces the `caracas` package and its syntax, including 
how `caracas` can be used in connection with preparing texts, e.g.\ teaching
material. 
More details are provided in the appendix (Sec. \ref{sec:primer}). 
Several vignettes illustrating
\pkg{caracas} are provided and they are also available online, see
<https://r-cas.github.io/caracas/>.
Sec. \ref{sec:statistics} is the main section of the
paper and here we present a sample of statistical models where we
believe that a symbolic treatment is a valuable supplement to a
numerical in connection with teaching. 
Sec. \ref{sec:topicsstudents} contains suggestions about 
hand-on activities for students. 
Lastly,
Sec. \ref{sec:discussion} contains a discussion of the paper.


<!-- Verifying and helping with math derivations and computations -->

<!-- Statistics, calculus and linear algebra -->

<!-- Gradients; Hessians; Newton-rapson. -->

<!-- Gradient descent; compute gradient as alternative to finite difference approach. -->


# Mathematics and documents containing mathematics {#sec:mathintro}

We start by introducing the `caracas` syntax on familiar topics within calculus 
and linear algebra. 

## Calculus 


<!-- # https://math.stackexchange.com/questions/918305/exercise-problem-with-one-maximum-two-minima-and-one-saddle-points/918344#918344 -->

First we define a `caracas` symbol `x` (see Sec. \ref{sec:primer}) and subsequently `p` to be a polynomial in `x` (`p` becomes a symbol because `x` is)
```{r}
library(caracas)
def_sym(x) ## Declares 'x' as a symbol
p <- 1 - x^2 + x^3 + x^4/4 - 3 * x^5 / 5 + x^6 / 6
p
```

The gradient of `p` is

```{r}
grad <- der(p, x) ## der is shorthand for derivative
grad
```

Stationary points of $p$ can be found by finding roots of the gradient. In this simple case we can factor the gradient
```{r}
factor_(grad)
```

which shows that stationary points are $-1$, $0$, $1$ and $2$. To
investigate if extreme points are local minima, local maxima or saddle
points, we compute the Hessian and evaluate the Hessian in the
stationary points.

```{r}
hess <- der2(p, x)
hess
hess_ <- as_func(hess)
hess_
stationary_points <- c(-1, 0, 1, 2)
hess_(stationary_points)
```

The sign of the Hessian in these points gives that $x=-1$ and $x=12$
are local minima, $x=0$ is a local maximum and $x=1$ is a saddle
point.

In general we can find the stationary symbolically and evaluate the Hessian (output omitted)
```{r, results='hide'}
sol <- solve_sys(grad, x) ## finds roots by default
subs(hess, sol[[1]]) ## the first solution
lapply(sol, function(s) subs(hess, s)) ## iterate over all solutions
```

## Linear algebra

Create a symbolic matrix and its inverse:
```{r}
M <- matrix_sym(nrow = 2, ncol = 2, entry = "m")
Minv <- inv(M)
```

Default printing of `M` is 
```{r}
M
```

Matrix products are computed using the `%*%` operator:
```{r}
simplify(M %*% Minv)
```

We need to verify the matrix and vector dimensions:

```{r}
v <- vector_sym(2, "v")
v ## Not the transpose, v is a column vector
dim(v)
M %*% v
```


## Preparing mathematical documents {#sec:teaching}



The packages `Sweave` [@leisch:02] and `Rmarkdown` [@rmarkdown] provide
integration of \LaTeX\ and other text formatting systems into \r\ helping to produce 
text document with \r\ content. In a similar vein, \caracas\ provides an integration
of computer algebra into \r\ and in addition, \caracas\ also facilitates creation
of documents with mathematical content without e.g.\ typing tedious
\LaTeX\ instructions.

A \LaTeX\ rendering of the \caracas\ symbol `p` is obtained by typing
 `` $$p(x) = `r knitr::inline_expr("tex(p)")`$$ `` 
$$
p(x) = `r tex(p)`
$$

but typing `` $$M = `r knitr::inline_expr("tex(M)")`$$ ``
produces the result 
$$
M = `r tex(M)`
$$

The inverse of $M$ contains the determinant of $M$ as denominator in each entry. This can be exploited as
```{r}
Minv_fact <- as_factor_list(1 / det(M), simplify(Minv * det(M)))
Minv_fact
```
Typing `` $$M^{-1} = `r knitr::inline_expr("tex(Minv_fact)")`$$ `` produces this:

$$
M\inv = `r tex(Minv_fact)`
$$

Finally we illustrate creation of additional mathematical expressions:

```{r}
def_sym(x, n)
y <- (1 + x/n)^n
lim(y, n, Inf)
```

Typing `` $$y = `r knitr::inline_expr("tex(y)")`$$ `` etc.\ gives
$$
y = `r tex(y)`, \lim_{n->\infty} y = `r lim(y, n, Inf)`
$$

We can also prepare unevaluated expressions using the `doit` argument. That makes output easier and more robust:

```{r}
l <- lim(y, n, Inf, doit = FALSE)
l
doit(l)
```
Typing `` $$`r knitr::inline_expr("tex(l)")` = `r knitr::inline_expr("tex(doit(l))")`$$ `` gives
$$
`r tex(l)` = `r tex(doit(l))`
$$
Several functions have the `doit` argument, e.g. `lim()`, `int()` and `sum_()`. 


# Statistics examples {#sec:statistics}

In this section we examine larger statistical examples and 
demonstrate how `caracas` can help improve understanding of the models.

## Linear models -- one way analysis of variance {#one-way}


A matrix algebra approach to e.g.\ linear models is very clear and
concise. On the other hand, it can also be argued that matrix algebra
obscures what is being computed. Numerical examples are useful for
some aspects of the computations but not for other. In this respect
symbolic computations can be enlightening. 


Consider one-way analysis of variance (ANOVA) with 
three groups and two replicates per group.

```{r}
n_grp <- 3 # Number of groups
n_rpg <- 2 # Number of replicates per group
dat <- expand.grid(rep=1:n_rpg, grp=paste0("g", 1:n_grp))
X <- as_sym(model.matrix(~ grp, data = dat))
y <- vector_sym(nrow(X), "y")
b <- vector_sym(n_grp, "b")
mu <- X %*% b
```

For the specific model we have random variables $y_1,\dots y_n$ where
$n=`r n_grp * n_rpg`$. All $y_i$s are assumed independent and $y_i\sim
N(\mu_i, v)$. The mean vector $\mu=(\mu_1,\dots,\mu_n)$ has the form given below (dots represent zero).

$$
y = `r tex(y)`, \quad X=`r texr(X)`, \quad b=`r tex(b)`, \quad  \mu = X b = `r tex(mu)`
$$

Above and elsewhere, dots represent zero. 
The least squares estimate of $b$ is the  vector $\hat \bb$ that minimizes $||y-X
\bb||^2$ which leads to the normal equations $(X\transp X)\bb =
X\transp y$ to be solved. If $X$ has full rank, the unique solution to the normal
equations is $\hat \bb = (X^\top X)^{-1} X^\top y$. Hence the
estimated mean vector is $\hat \mu = X\hat\bb=X(X^\top X)^{-1} X^\top
y$. Symbolic computations are
not needed for quantities involving only the model matrix $X$, but
when it comes to computations involving $y$, a symbolic treatment of
$y$ is useful:

```{r}
XtX <- t(X) %*% X
XtXinv <- inv(XtX)
Xty <- t(X) %*% y
b_hat <- XtXinv %*% Xty
y_hat <- X %*% b_hat
```

\[
X^\top y = `r tex(Xty)`,
\quad
\hat{\bb} = `r tex(mat_factor_div(b_hat, n_rpg))` ,
\quad
\hat{y} = `r tex(mat_factor_div(y_hat, n_rpg))`,
\]

Hence $X^\top y$ consists of the sum of all observations, the sum of
observations in group 2 and the sum of observations in group 3.
Similarly, $\hat\bb$ consists of the average in group 1, the average
in group 2 minus the average in group 1 and the average in group 3
minus the average in group 1. Fitted values are simply group
averages. 
The orthogonal projection matrix onto the column space 
of $X$ is:
```{r}
P <- X %*% XtXinv %*% t(X)
```

<!-- \[ -->
<!-- P = `r texr(mat_factor_div(P, n_rpg))` -->
<!-- \] -->


<!--
### Possible topics for students

1. Related to Sec. \ref{one-way}: Verify that the residuals $r=(I-P)y$ are not all
	independent and that the correlation between is small and becomes
	smaller as the number of subjects per group increase. Verify that
	$P X = X$ and thus $(I - P)X = 0$. Verify also that the rank of
	$P$ equals the number of groups, which is `r n_grp`. A model matrix
	also spanning $L$ is `X_2 = model.matrix(~ -1 + f)`. Investigate how
	the quantities above look for this choice of model matrix.

1. Construct a balanced two way analysis of variance (two-way anova), 
   first only with main effects and then with an interaction and compare the 
   estimates.
-->



## Logistic regression {#sec:logistic}

In the following we go through details of a logistic regression model,
see e.g.\ [@mccullagh:etal:89] for a classical description of logistic
regression.  Observables are binomially distributed, $y_i \sim
\bin(p_i, n_i)$. The probability $p_i$ is connected to a a $q$-vector
of covariates $x_i=(x_{i1}, \dots, x_{iq})$ and a $q$-vector of
regression coefficients $b=(b_1, \dots, b_q)$ as follows: Define $s_i
= x_i \cdot b$ to be the linear predictor. The probability $p_i$ can
be related to $s_i$ in different ways, but the most commonly employed
is as $\logit(p_i) = \log(p_i/(1-p_i)) = s_i$.

As an example, consider the `budworm` data from the `doBy` package.
The data shows the number of killed moth tobacco budworm
\emph{Heliothis virescens}.  Batches of 20 moths of each sex were
exposed for three days to the pyrethroid and the number in each batch
that were dead or knocked down was recorded.


```{r}
data(budworm, package = "doBy")
bud <- subset(budworm, sex == "male")
bud
```

Below we focus only on male budworms and the mortality is illustrated
in Fig. \ref{fig:budworm}.  On the $y$-axis we have the empirical
logits, i.e. $\log(\text{ndead} + 0.5 /(\text{ntotal}-\text{ndead} + 0.5))$.

```{r budworm, fig.cap="\\label{fig:budworm}Insecticide mortality of the moth tobacco budworm.", echo=F, fig.height=2, fig.width=6}
library(ggplot2)
theme_set(theme_bw())
p1<-bud  %>% ggplot(aes(x=dose, y=log((ndead+0.5) / (ntotal - ndead + 0.5)))) +
    geom_point() + geom_line() + labs(y = "Empirical logits")
p2<-bud  %>% ggplot(aes(x=log2(dose), y=log((ndead + 0.5) / (ntotal - ndead + 0.5)))) +
    geom_point() + geom_line() + labs(y = "Empirical logits")
cowplot::plot_grid(p1, p2, nrow=1)
```

### Each component of the likelihood {#sec:logistic-each-component}

The log-likelihood is $\log L=\sum_i y_i \log(p_i) + (n_i-y_i)
\log(1-p_i) = \sum_i \log L_i$, say. With $\log(p_i/(1-p_i)) = s_i$ we
have $p_i=1 / (1+ \exp(-s_i))$ and $\frac d {ds_i} p_i = \frac{\exp(-
s_i)}{\left(1 + \exp(- s_i)\right)^{2}}$. With $s_i = x_i\cdot b$, we
have $\frac d {db} s_i = x_i$.

Consider the contribution to the total log-likelihood from the $i$th
observation which is $l_i = y_i \log(p_i) + (n_i-y_i) \log(1-p_i)$.
Since we are focusing on one observation only, we shall ignore the
subscript $i$ in this section. The log-likelihood and its derivative
are:

```{r}
def_sym(y, n, p, x, s, b)
logL_ <- y * log(p) + (n - y) * log(1 - p)
gp_ <- der(logL_, p)
gp_
```

The underscore in `logL_` and elsewhere indicates that this expression 
is defined in terms of other symbols. This is in contrast 
to the free variables, e.g.\ `y`, `p`, and `n`.
With $s = \log(p/(1-p))$ we can find $p$ as:
```{r}
sol_ <- solve_sys(log(p / (1 - p)), s, p)
p_ <- sol_[[1]]$p
p_
```

<!-- Again, notice that `p_` depends on the free symbol `s`,  -->
<!-- hence the underscore (`_`) in its name. -->
The log-likelihood and its derivative as
functions of $s$ become:

```{r}
logL2_ <- subs(logL_, p, p_)
logL2_
gs_ <- der(logL2_, s) |> simplify()
gs_
```

Lastly we connect $s$ to the regression coefficients $b$ and 
compute the score function, $S$, and the Hessian, $H$:

```{r}
s_ <- sum(x * b)
logL3_ <- subs(logL2_, s, s_)
```

```{r}
S_ <- score(logL3_, b) |> simplify()
H_ <- hessian(logL3_, b) |> simplify()
S_
H_
```

Since $x$ and $\bb$ are vectors, the term `x*b` above should be read
 as the inner product $x \cdot \bb$ (or as $x\transp \bb$ in matrix
 notation). Also, since $x$ is a vector, the term `x^2` above should
 be read as the outer product $x \otimes x$ (or as $x x\transp$ in
 matrix notation).  More insight in the structure is obtained by
 letting $b$ and $x$ be $2$-vectors. (to save space, only the score
 function is shown in the following):

```{r}
b <- vector_sym(2, "b")
x <- vector_sym(2, "x")
s_ <- sum(x * b)
logL3_ <- subs(logL2_, s, s_)
```

Again, we compute the score function $S$ by differentiation with respect to the regression parameters. This gives a vector valued funtion of the regression parameters and data.

```{r}
S_ <- score(logL3_, b) |> simplify()
```

Next, insert data, e.g.\ $x_{1}=1$, $x_{2}=2$, $y=9$, $n=20$ to obtain a function of the regression parameters only:
```{r}
nms <- c("x1", "x2", "y", "n")
vls <- c(1, 2, 9, 20)
S. <- subs(S_, nms, vls)
```

Note how the expression depending on other symbols, `S_`, is 
named `S.` to indiciate that data has been inserted:

\begin{align}
\texttt{S\_} = `r texr(S_)`, \quad S. = `r texr(S.)`
\end{align}

An alternative is to create \r\ functions and subsequently set default values
```{r}
S.. <- as_func(S_, order = c("b1", "b2", "x1", "x2", "y", "n"))
S... <- function(b1, b2) {
  S..(b1, b2, x1 = vls[1], x2 = vls[2], y = vls[3], n = vls[4])
}
S...(1, 1)
```


### The total likelihood numerically

The score and Hessian for a full data set is the sum of
such terms and it is a straight forward \proglang{R} task to construct these
sums.  In either case the result is a function of the regression coefficients
which can be used in connection with numerical optimization. This
could be a Newton-Rapson algorithm (which would also require the
Hessian as function) or a coordinate descent or similar method.

```{r}
Sv <- Vectorize(S.., vectorize.args = c("x1", "x2", "y", "n"))
Sv_wrap <- function(b1, b2) {
  Sv(b1, b2, x1 = rep(1, nrow(bud)), x2 = log2(bud$dose), y = bud$ndead, n = bud$ntotal)
}
Sv_wrap(1, 1)
apply(Sv_wrap(1, 1), 1, sum)
apply(Sv_wrap(2, 2), 1, sum)
```

<!---
```{r}
# Sv <- Vectorize(S.., vectorize.args = c("x1", "x2", "y", "n"))
# Sv(b1 = 1, b2 = 1, x1=rep(1,nrow(bud)), x2=log2(bud$dose), y=bud$ndead, n=bud$ntotal)
# 
# aa <- list(x1=rep(1,nrow(bud)), x2=log2(bud$dose), y=bud$ndead, n=bud$ntotal)
# Sv2 <- doBy::set_default(Sv, aa)
# Sv2(1, 1)
# 
# Sv2 <- doBy::section_fun(Sv, aa, method = "env")
# Sv2(1, 1)
```
--->


<!--


### Bridging the gap - towards numerical evaluation

For illustration, we prepare the full score function, $S$, for 
all observations. 
One option is to substitute values into a `caracas` expresion:
```{r}
S.list <- lapply(seq_len(nrow(bud)), function(r){
    vls <- c(1, log2(bud$dose[r]), bud$ndead[r], bud$ntotal[r])
    subs(S_, nms, vls)
})

S. <- Reduce(`+`, S.list)
S_fun1 <- as_func(S.)
```

Another option is to substitute values into \r\ functions:

```{r}
S.list <- lapply(seq_len(nrow(bud)), function(r){
    vls <- c(1, log2(bud$dose[r]), bud$ndead[r], bud$ntotal[r])
    doBy::set_default(S.., nms, vls)
})

S_fun2 <- function(b1, b2){
  lapply(S.list, function(s) s(b1, b2)) |> Reduce(`+`, x=_)
}
```

```{r}
S_fun1(1,2) |> as.numeric()
S_fun2(1,2) |> as.numeric()
```
-->


### The total likelihood symbolically {#sec:logistic-all-likelihood}

An alternative to the approach above is to specify the full likelihood directly:

```{r}
N <- 6 ## Number of rows in dataset
q <- 2 ## Number of explanatory variables
X <- matrix_sym(N, q, "x")
y <- vector_sym(N, "y")
n <- vector_sym(N, "n")
p <- vector_sym(N, "p")
s <- vector_sym(N, "s")
b <- vector_sym(q, "b")
```

$$
 X=`r texr(as_sym(X))`, \quad
 n=`r texr(as_sym(n))`, \quad
 y=`r texr(as_sym(y))`
$$


The symbolic computations are as follows:

```{r}
## log-likelihood:
logL_  <- sum(y * log(p) + (n-y) * log(1-p))
## connecting p and s:
p_ <- 1 / (1 + exp(-s))
## log-likelihood as function of linear predictor:
logL2_ <- subs(logL_, p, p_)
## linear predictor as function of regression coefficients:
s_  <- X %*% b
## log-Likelihood as function of regression coefficients:
logL3_ <- subs(logL2_, s, s_)
## Score and Hessian:
S_ <- score(logL3_, b) 
H_ <- hessian(logL3_, b)
```

Above we have analysed the logistic regression model with the logit link. 
If we instead used the complementary log log link, $\eta = \log(-\log(1-p))$
and find the inverse 
```{r, eval=FALSE}
sol_ <- solve_sys(log(-log(1-p)), s, p)
sol_
```
which can be used in the above by specifying
```{r}
p_ <- 1 - exp(-exp(s))
```

[FIXME: SH: Above]



```{r, include=FALSE, echo=FALSE}
# S_1 <- S_
# S_1 <- subs(S_1, X[, 1], rep(1, N))
# S_1 <- subs(S_1, X[, 2], log2(bud$dose))
# S_1 <- subs(S_1, y, bud$ndead)
# S_1 <- subs(S_1, n, bud$ntotal)
# S_1 - S.
# simplify(S_1 - S.)
```








<!---
### Numerical evaluation

Substitute data into score and Hessian:

```{r
X. <- cbind(1, log2(bud$dose))
S. <- subs(S_, cbind(n, y, X), cbind(bud$ntotal, bud$ndead, X.))
H. <- subs(H_, cbind(n, y, X), cbind(bud$ntotal, bud$ndead, X.))
```

```{r
nr <- newton_rapson(c(-3, 1), S., H.)
nr$b
-solve(nr$H)
```
--->

<!--
### Possible topics for students {#sec:students-logit}


1. Related to Sec.\ \ref{sec:logistic-each-component}: Implement
   Newton-Rapson to solve the likelihood equations and compare your
   solution to the output from `glm()`.

1. Related to Sec.\ \ref{sec:logistic-all-likelihood}: Used the above
   symbolic computations and substitute data in directly. 
   Compare to `S.` and `H.` from Sec. \ref{sec:logistic-each-component}.
-->


## Auto regressive models {#sec:ar1}

### An $AR(1)$ model 

```{r, echo=F}
e <- as_sym(paste0("e", 0:3))
x <- as_sym(paste0("x", 0:3))
u <- vector_sym(3, "u")
y <- vector_sym(3, "y")
eu <- rbind(e, u)
xy <- rbind(x, y)
```

```{r, echo=FALSE}
L <- diff_mat(4, "-a")
```

In this section we study the auto regressive model of order $1$ (an AR(1) model), see
e.g. [@shumway:etal:16], p.\ 75 ff.\ for details: 
Consider random variables $x_1, x_2, \dots, x_n$ following a stationary zero mean AR(1) process

\begin{equation}
  \label{eq:ar1}
  x_i = a x_{i-1} + e_i; \quad i=2, \dots, n
\end{equation}

where $e_i \sim N(0, v)$ and all $e_i$s are independent. Note that $v$ denotes the variance.
The marginal distribution of $x_1$ is also assumed normal, and for the process to be stationary we must have $\var(x_1) = v / (1-a^2)$. Hence we can write $x_1 = \frac 1 {\sqrt{1-a^2}} e_1$. 

```{r, ar_setup, echo=F}
n <- 4
def_sym(a)
x <- vector_sym(n, "x")
e <- vector_sym(n, "e")
L <- diff_mat(n, "-a")
L[1, 1] <- sqrt(1-a^2)
```

For simplicity of
exposition, we set $n=4$. All terms $e_1, \dots, e_4$ are independent
and $N(0, v)$ distributed.  Let
$e=(e_1, \dots, e_4)$ and $x=(x_1, \dots x_4)$. Hence $e \sim N(0, v
I)$.
Isolating error terms gives

\begin{displaymath}
  e= `r inline_code(tex(e))` = `r inline_code(texr(L))` `r inline_code(tex(x))` = L x 
\end{displaymath}

Since
$\mathbf{Var}(e)=v I$ we have $\mathbf{Var}(e)=v I=L \mathbf{Var}(x)
L'$ so the covariance matrix of $x$ is $V=\mathbf{Var}(x) = v L^-
(L^-)\transp$ while the concentration matrix (the inverse covariances
matrix) is $K=v^{-1}L\transp L$.

```{r, ar_setup}
```

```{r, include=FALSE}
#v <- symbol("v", real = TRUE, positive = TRUE)
## ask(v, "positive")
## ask(v, "real")
```


```{r}
def_sym(v)
Linv <- inv(L)
K <- crossprod_(L) / v
V <- tcrossprod_(Linv) * v
```

```{r, echo=F, results="asis"}
cat(
  "\\begin{align} 
    L\\inv &= ", texr(Linv), " \\\\ 
    K &= ", texr(mat_factor_div(K, v)), " \\\\ 
    V &= ", texr(mat_factor_mult(V, v)), " 
  \\end{align}", sep = "")
```

The zeros in the concentration matrix $K$ implies a conditional
independence restrstriction: If the $ij$th element of a concentration
matrix is zero then $x_i$ and $x_j$ are conditionally independent
given all other variables, see e.g. [@hojsgaard:etal:12], chap.\ 4 for
details.\footnote{BETTER REFERENCE; Handbook of graphical models
perhaps?}









Next, we take the step from symbolic computations to numerical
evaluations.  The joint distribution of $x$ is multivariate normal
distribution, $x\sim N(0, K\inv)$. Let $W=x x\transp$ denote the
matrix of (cross) products.  The log-likelihood is therefore (ignoring
multiplicative constants) 
$$ 
\log L = \log \det(K) - x\transp K x = \log \det(K) - \trace(K W), 
$$ 
where we note that $\trace(KW)$ is the
sum of the elementwise products of $K$ and $W$ since both matrices are
symmetrical.

```{r}
logL <- log(det(K)) - sum(K * (x %*% t(x))) %>% simplify()
```

$$
\log L = `r tex(logL)`
$$


### Bridging the gap - towards numerical evaluation 

Next we illustrate how bridge the gap  from symbolic computations to numerical computations based on a dataset:
For a specific data vector we get

```{r}
xt <- c(0.1, -0.9, 0.4, .0)
logL. <- subs(logL, x, xt) 
```

$$
\log L = `r tex(logL.)`
$$


We can use \r\ for numerical maximization of the likelihood and constraints on the 
parameter values can be imposed e.g. in the `optim()` function:
```{r}
f_wrap <- as_func(logL., vec_arg = TRUE)
eps <- 0.01
par <- optim(c(a=0, v=1), f_wrap, lower=c(-(1-eps), eps), upper=c((1-eps), 10),
             method="L-BFGS-B", control=list(fnscale=-1))$par
par
```

The same model can be fitted e.g.\ using \r's `arima()` function as follows (output omitted):
```{r, eval=FALSE}
arima(xt, order = c(1, 0, 0), include.mean = FALSE, method = "ML")
```

It is less trivial to do the optimization in `caracas` by solving the score equations. 
There are some possibilities for putting assumptions on variables
in `caracas` (see the "Reference" vignette), but 
it is not possible to restrict variables to only take values in $(-1, 1)$. 

<!---
The score function is
```{r}
parm <- list(a, v)
S <- der(logL., parm) |> simplify()
```

$$
S = `r tex(S)`
$$


```{r}
sol <- solve_sys(S, parm)
sol

```

There are some possibilities for putting assumptions on variables
in `caracas` (see the "Reference" vignette), but 
it is not possible to restrict variables to only take values in $(-1, 1)$. 
Hence, here we do that manually by only considering the solution with $|a|<1$:
```{r}
valid_sol <- sapply(sol, function(s) abs(as_expr(s$a)) < 1)
sol <- sol[[which(valid_sol)]]
sol
```
--->



<!---
```{r, include=FALSE, eval=FALSE}
# parm <- list(a, v)
# S <- der(logL., parm) |> simplify()
# # To get |a| < 1
# S2 <- subs(S, a, "(exp(b)-1)/(exp(b)+1)") |> simplify()
# sol <- solve_sys(S2, c("b", "v"))
# sol
# 
# #b <- log((a-1)/(a+1))
# #sol <- solve_sys(b, a)
# 
# 
# #sol <- solve_sys(S, parm)
# sol <- solve_sys(Sb, c("b", "v"))
# sol
```
--->




## Maximum likelihood under constraints - independence model for two-way contingency table {#sec:cont_tab}


```{r echo=F}
n_ <- c("n11", "n21", "n12", "n22")
n  <- as_sym(n_)
tab <- cbind(expand.grid(r = 1:2, c = 1:2), n = n_)
tt  <- xtabs(~ r + c, data = tab)
tt[] <- n_
```

In this section we illustrate constrained optimization using Lagrange multipliers.
Consider a $2 \times 2$ contingency table with cell 
counts $n_{ij}$ and cell probabilities $p_{ij}$ for $i=1,2$ and $j=1,2$. 
```{r, echo=FALSE}
tt
```

Under multinomial sampling, the log likelihood is
$$
 l = \log L = \sum_{ij} n_{ij} \log(p_{ij}).
$$

Under the assumption of independence between rows and columns, the cell
probabilities have the form, (see e.g.\ [REFERENCE], chap. XXX)
$$
p_{ij}=u r_i s_j.
$$ 

To make the parameters $(u, r_i, s_j)$ identifiable, constraints
must be imposed. One possibility is to require that $r_1=s_1=1$. The
task is then to estimate $u$, $r_2$, $s_2$ by maximizing the log likelihood
under the constraint that $\sum_{ij} p_{ij} = 1$.  This can be
achieved using a Lagrange multiplier where we instead solve the
unconstrained optimization problem $\max_p Lag(p)$ where
\begin{align}
  Lag(p) &= -l(p) + \lambda g(p) \quad \text{under the constraint that} \\
  g(p) &= \sum_{ij} p_{ij} - 1 = 0.
\end{align}
where $\lambda$ is a Lagrange multiplier.

```{r}
n_ <- c("n11", "n21", "n12", "n22")
n  <- as_sym(n_)
def_sym(u, r2, s2, lam)
p <- as_sym(c("u", "u*r2", "u*s2", "u*r2*s2"))
logL  <- sum(n * log(p))
Lag  <- -logL + lam * (sum(p) - 1) 
vars <- list(u, r2, s2, lam)
gLag <- der(Lag, vars)
sol <- solve_sys(to_vector(gLag), vars)
print(sol, method = "ascii")
sol <- sol[[1]]
```

There is only one critical point. Fitted cell probabilities $\hat p_{ij}$ are:
```{r}
p11 <- sol$u
p21 <- sol$u * sol$r2
p12 <- sol$u * sol$s2
p22 <- sol$u * sol$r2 * sol$s2
p.hat <- matrix_(c(p11, p21, p12, p22), nrow = 2)
```

\[
\hat p = `r tex(mat_factor_div(p.hat, denominator(p.hat[1,1])))`
\]

To verify that the maximum likelihood estimate has been found, we compute the Hessian matrix which is negative definite (the Hessian matrix is diagonal so the eigenvalues are the diagonal entries and these are all negative), output omitted:

```{r}
H <- hessian(logL, list(u, r2, s2)) |> simplify()
```

<!---
\[
  H = `r texr(H)`, \quad
\]
--->

<!--
### Possible topics for students 

1. A simple task is to consider a multinomial distribution with three categories, counts $y_i$ and cell probabilities $p_i$, $i=1,2,3$ where $\sum_i p_i=1$. For this model, find the maximum likelihood estimate for $p_i$. 

1. Above, identifiability of the parameters was handled by not including $r_1$ and $s_1$ in the specification of $p_{ij}$. An alternative is to impose the restrictions $r_1=1$ and $s_1=1$, and this can also be handled via Lagrange multipliers.
-->

```{r, echo=F}
rv <- vector_sym(2, "r")
sv <- vector_sym(2, "s")
m <- rv %*% t(sv)
p <- u*m |> vec()
l <- sum(n * log(p))
```


```{r, echo=F}
def_sym(a, a1, a2, u, r1, r2, s1, s2)
p <- as_sym(c("u*r1*s1", "u*r2*s1", "u*r1*s2", "u*r2*s2"))
l  <- sum(n * log(p))
L  <- -l + a * (sum(p) - 1) + a1 * (r1 - 1) + a2 * (s1 - 1)
vars <- list(u, r2, s2, a, a1, a2)
gL <- der(L, vars)
sol <- solve_sys(to_vector(gL), vars)
sol <- sol[[1]]
```



## A compound symmetry covariance structure {#sec:cov_fun}

<!-- As mentioned in Sec.\ \ref{sec:introduction}, `caracas` is not as -->
<!-- powerful as some of the large commercial computer algebra systems -->
<!-- (CAS). One obvious restriction is that `caracas` can not operate on -->
<!-- general $n \times m$ matrices. Actually, \sympy\ has support for  -->
<!-- symbolic matrices, the best way to implement  -->
<!-- this in \pkg{caracas} is not yet found.  -->
<!-- Instead, $n$ and $m$ must be set to -->
<!-- some integer number. Hence it is difficult to study the asymptotic -->
<!-- behaviour when a matrix dimension goes to infinity.   -->


```{r echo=F}
n <- 3
```

```{r echo=F}
R <- as_sym(toeplitz(c(1, rep("r", n-1))))
```

<!-- To be specific we consider the following example:  -->

Consider random
variables $x_1,\dots, x_n$ where $\var(x_i)=v$ and $\cov(x_i,
x_j)=v r$ for $i\not = j$, where $0 \le r| \le1$. 
For $n=`r n`$, the covariance matrix of $(x_1,\dots, x_n)$ is therefore

\begin{equation}
  \label{eq:1}
  V= v R = v `r tex(R)`. 
\end{equation}


Let $\bar x = \sum_i x_i / n$ denote the average.  Suppose interest is
in the variance of the average, $\var(\bar x)$, when $n$ goes to
infinity. One approach is as follow: Let $1$ denote an $n$-vector of
$1$'s and let $V$ be an $n \times n$ matrix with $v$ on the diagonal
and $v r$ outside the diagonal.  Then $\var(\bar x)=\frac 1 {n^2}
1\transp V 1$. The answer lies in studying the limiting behaviour of
this expression when $n\rightarrow \infty$ and `caracas` can not
handle this directly.

What can be done in `caracas` is the following: The variance of a sum $x. = \sum_i x_i$ 
is $\var(x.) = \sum_i \var(x_i) + 2 \sum_{ij:i<j} \cov(x_i,
x_j)$. For the specific model, one must by hand find that 
$$
\var(x.) = n v + 2 v r n (n-1) / 2 = n v (1 + r (n-1)),
\quad
  \var(\bar x) = v (1 + (n-1)r)/n.
$$
<!-- which follows from that the sum of the upper -->
<!-- triangle of the covariance matrix is $vrn(n-1)/2$.  -->


```{r}
def_sym(v, r, n) 
var_sum <- n * v * ( 1 + r * (n - 1))
var_avg <- var_sum / n^2
var_avg %>% simplify()
```

Now we can study the limiting behaviour of the variance $\var(\bar x)$ in different situations:

```{r}
l_1 <- lim(var_avg, n, Inf)         ## When sample size n goes to infinity
l_2 <- lim(var_avg, r, 0, dir='+')  ## When correlation r goes to zero
l_3 <- lim(var_avg, r, 1, dir='-')  ## When correlation r goes to one
```

For a given correlation $r$ it is instructive to investigate how many
independent variables $k$ the $n$ correlated variables correspond to
(in the sense of the same variance of the average), because the $k$
can be seen as a measure of the amount of information in data. 
Moreover, one might study how $k$ behaves as function of $n$ when $n \rightarrow
\infty$. That is we must (1) solve $v (1 + (n-1)r)/n = v/k$ for $k$
and (2) find $\lim_{n\rightarrow\infty} k$:

```{r}
def_sym(k)
k <- solve_sys(var_avg - v / k, k)[[1]]$k
l_k <- lim(k, n, Inf)
```

The findings above are:
\[
l_1 = `r tex(l_1)`, \quad
l_2 = `r tex(l_2)`, \quad
l_3 = `r tex(l_3)`, \quad
k = `r tex(k)`, \quad 
l_k = `r tex(l_k)`
\]

With respect to $k$, it is illustrate to supplement the symbolic
computations above with numerical evaluations:

<!--
```{r}
dat <- expand.grid(r=c(.1, .2, .5), n=c(10, 50))
k. <- apply(dat, 1,
            FUN=function(row){
                subs(k, c("r", "n"), row) |> as_expr()
            })
dat$ri <- 1/dat$r
dat$k <- k.
dat
```
-->



```{r}
k_fun <- as_func(k)
dat$k <- k_fun(r=dat$r, n=dat$n)
dat$ri <- 1/dat$r
dat
```



<!--

### Possible topics for students

It is illustrative to study such behaviours for other covariance
functions. For example (1) $\cov(x_i,
x_j)=v r^{|i-j|}$ and (2)   $\cov(x_i,
x_j)=v r$ if $|i-j| = 1$ and $\cov(x_i,
x_j)=0$ if $|i-j| > 1$. 

-->




# Possible topics and smaller projects for students {#sec:topicsstudents}


1. Related to Sec. \ref{one-way}: Verify that the residuals $r=(I-P)y$ are not all
	independent and that the correlation between is small and becomes
	smaller as the number of subjects per group increase. Verify that
	$P X = X$ and thus $(I - P)X = 0$. Verify also that the rank of
	$P$ equals the number of groups, which is `r n_grp`. A model matrix
	also spanning $L$ is `X_2 = model.matrix(~ -1 + f)`. Investigate how
	the quantities above look for this choice of model matrix.
	Construct a balanced two way analysis of variance (two-way anova), 
	first only with main effects and then with an interaction and compare the 
	estimates.


1. Related to Sec.\ \ref{sec:logistic-each-component}: Implement
   Newton-Rapson to solve the likelihood equations and compare your
   solution to the output from `glm()`. Related to Sec.\
   \ref{sec:logistic-all-likelihood}: Used the above symbolic
   computations and substitute data in directly.  Compare to `S.` and
   `H.` from Sec. \ref{sec:logistic-each-component}.


1. Related to Sec.\ \ref{sec:cont_tab}: A simple task is to consider a
   multinomial distribution with three categories, counts $y_i$ and cell
   probabilities $p_i$, $i=1,2,3$ where $\sum_i p_i=1$. For this model,
   find the maximum likelihood estimate for $p_i$.  Above,
   identifiability of the parameters was handled by not including $r_1$
   and $s_1$ in the specification of $p_{ij}$. An alternative is to
   impose the restrictions $r_1=1$ and $s_1=1$, and this can also be
   handled via Lagrange multipliers.



1. Related to Sec.\ \ref{sec:ar1}: Find (approximate) standard error
   and confidence interval for the parameter $a$.  Modify the model in
   \eqref{eq:ar1} by setting $x_1 = a x_n + e_1$ and see what happens
   to the pattern of zeros in the concentration matrix. Extend the
   $AR(1)$ model to and $AR(2)$ model and investigate this model along
   the same lines as above.

1. Related to Sec.\ \ref{sec:cov_fun}: It is illustrative to study
   such behaviours for other covariance functions. For example (1)
   $\cov(x_i, x_j)=v r^{|i-j|}$ and (2) $\cov(x_i, x_j)=v r$ if $|i-j|
   = 1$ and $\cov(x_i, x_j)=0$ if $|i-j| > 1$.



# Discussion and future work {#sec:discussion}

We have presented the \pkg{caracas} package and argued that the
package extends the functionality of \proglang{R} significantly with respect to
symbolic mathematics.  One practical virtue of \pkg{caracas} is
that the package integrates nicely with \pkg{Rmarkdown},
@rmarkdown, (e.g. with the `tex()` functionality) and thus
supports creating of scientific documents and teaching material. As
for the usability in practice we await feedback from users.


With respect to freely available resources in a CAS context, we would
like to draw attention to `WolframAlpha`, see
<https://www.wolframalpha.com/>, which provides an online service for
answering (mathematical) queries.

Another related package we mentioned in the introduction is `Ryacas`. 
This package has existed for many years and is still of relevance. 
It probably has fewer features than `caracas`. On the other hand, 
it does not require Python (it is compiled), is faster for some 
computations (like matrix inversion) and the Yacas language is 
extendable (see e.g. the vignette "User-defined yacas rules" in 
the `Ryacas` package).

# Acknowledgements

We would like to thank the R Consortium for financial support for
creating the \pkg{caracas} package, users for pin pointing points
that can be improved in \pkg{caracas} and Ege Rubak (Aalborg
University, Denmark) and Malte Bødkergaard Nielsen (Aalborg
University, Denmark) for comments on this manuscript.



\tableofcontents


\appendix

# A short primer {#sec:primer}

This section provides a brief introduction to \caracas to make this
paper self contained.  Readers are recommended to study the online
documentation at <https://r-cas.github.io/caracas/>.
The \caracas
package provides an interface from \proglang{R} to the
\proglang{Python} package \sympy\ [@sympy]. This means that SymPy is
"running under the hood" of \proglang{R} via the \pkg{reticulate}
package [@reticulate].  The \sympy\ package is mature and robust with
many users and developers.

A  \caracas\ symbol is a list with a `pyobj` slot and the class
`caracas_symbol`.  The `pyobj` is an an object in \proglang{Python}
(often a \sympy\ object).  As such, a symbol (in \proglang{R})
provides a handle to a \proglang{Python} object.  In the design of
\pkg{caracas} we have tried to make this distinction something the
user should not be concerned with, but it is worthwhile being aware of
the distinction.

One way of defining a symbol is with `def_sym()` which declares the
symbol in \proglang{R} and in \proglang{Python}. A symbol can also be 
 defined in terms of other symbols: 
Define symbols `s1` and `s2` and define symbol `s3` in terms of `s1` and `s2`:
```{r}
def_sym(s1, s2) ## Note: 's1' and 's2' exist in both R and Python
s1$pyobj
s3_ <- s1 * s2   ## Note: 's3' is a symbol in R; no corresponding object in Python
s3_$pyobj
```

The underscore in `s3_` indicates that this expression 
is defined in terms of other symbols. This convention is used through out the paper.
Next express `s1` and `s2` in terms of symbols `u` and `v` (which are created on the fly):
```{r}
s4_ <- subs(s3_, c("s1", "s2"), c("u+v", "u-v"))
s4_
```

Another way of creating a `caracas` symbol is from an `R` object:
```{r}
v <- c("v1", "v2")
as_sym(v) ## A 2 x 1 matrix
```

<!-- ## Brindging a gap - From caracas to R and vice versa -->

A `caracas` expression can be coerced to an `R` expression and
subsequently evaluated numerically. Alternatively can be coerced to an
`R` function:

```{r}
s4_expr <- as_expr(s4_)
s4_expr
s4_fun <- as_func(s4_)
s4_fun
```

A numerical evaluation is obtained as (output omitted):
```{r, results='hide'}
eval(s4_expr, list(u=1, v=2))
s4_fun(u=1, v=2)
```






<!-- For a longer time series we get: -->
<!-- ```{r} -->
<!-- N <- 50 -->
<!-- set.seed(20221231) -->
<!-- xt <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = N) |> as.numeric() -->
<!-- L <- diff_mat(N, "-a") -->
<!-- def_sym(v) -->
<!-- K <- crossprod_(L) / v -->
<!-- K. <- as_expr(K) -->

<!-- W <- xt %*% t(xt) -->
<!-- logL.func <- function(par) { -->
  <!-- K.par <- eval(K., list(a = par[1], v = par[2])) -->
  <!-- -(log(det(K.par)) - sum(K.par * W)) -->
<!-- } -->
<!-- optim(c(1, 1), logL.func)$par -->
<!-- ``` -->

<!-- Compare this with -->
<!-- ```{r} -->
<!-- a2 <- arima(xt, order = c(1, 0, 0), include.mean = FALSE, method = "ML") -->
<!-- a2 -->
<!-- ``` -->



<!-- One \caracas\ symbol can be substituted with another with (which here -->
<!-- has the side effect that new \caracas\ symbols `u` and `v` are -->
<!-- created): -->




<!-- The \caracas\ object `s3` is a list with a `pyobj` slot and the class `caracas_symbol`: -->
<!-- ```{r} -->
<!-- str(s3) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- def_sym(u, v) -->
<!-- subs(p, x, u+v) -->
<!-- subs(p, x, u+v) |> expand() |> factor_() -->

<!-- ``` -->

<!-- One \caracas\ symbol can be substituted with another with (which here -->
<!-- has the side effect that new \caracas\ symbols `u` and `v` are -->
<!-- created): -->

<!-- ```{r} -->
<!-- s4 <- subs(s3, c("s1", "s2"), c("u+v", "u-v")) -->
<!-- s4 -->

<!-- ``` -->

<!-- Coercion from \caracas\ symbols to \r\ expressions and vice versa can be done as follows: -->
<!-- ```{r} -->
<!-- as_expr(s4) -->
<!-- as_sym(as_expr(s4)) -->
<!-- ``` -->

<!-- The call `def_sym(s1, s2)` above is a short-hand for the using  -->
<!-- the `symbol()` function: -->
<!-- ```{r} -->
<!-- s1 <- symbol("s1") # LHS s1: R object; RHS 's1': Python object -->
<!-- s2 <- symbol("s2") -->
<!-- ``` -->




<!---
```{r

```{r}

H.list <- lapply(seq_len(nrow(bud)), function(r){
    vls <- c(1, log2(bud$dose[r]), bud$ndead[r], bud$ntotal[r])
    subs(H_, nms, vls)
})
H. <- Reduce(`+`, H.list)

```



newton_rapson <- function(b., S., H.){
    it <- 0
    repeat{
        S_i <- as_expr(subs(S., b, b.))
        H_i <- as_expr(subs(H., b, b.))  
        b2. <- b. - solve(H_i, S_i)
        if (max(abs(b2. - b.)) < 1e-4) break
        b. <- b2.
        it <- it + 1
    }
    list(b=as.numeric(b.), Vb=-solve(H_i), H=H_i, it=it)
}
```


```{r
nr <- newton_rapson(c(-3, 1), S., H.)
nr$b
nr$Vb
```

```{r
mm <- glm(cbind(ndead, ntotal - ndead) ~ log2(dose), data = bud, family = binomial())
summary(mm) %>% coef()
vcov(mm)
```
--->

