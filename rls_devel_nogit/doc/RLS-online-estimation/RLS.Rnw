\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage{SHmathnot,a4wide} 
%% \usepackage{hyperref}

\title{Recursive least squares for online estimation}
\author{Søren Højsgaard}

\begin{document}
\SweaveOpts{prefix.string=fig/DYNEST} %sæt dir til "fig" og prefix til "bar" for figurer
%\setkeys{Gin}{width=1.0\textwidth, height=8cm} %sæt figurstørrelse i Sweave
\renewenvironment{Schunk}{\linespread{.85}\small}{}
\def\wiki#1{ \href{#1}{[Wikipedia]} }

\parindent0pt\parskip5pt

\maketitle
\tableofcontents

\section{Introduction}
\label{sec:xxx}

This note describes some methods for online estimation and
forecasting. 
We consider observations $y_1,y_2, \dots$ recorded at times $t_1, t_2,
\dots$. Consider a regression problem
\begin{equation}
  \label{eq:rlsregmodel}
  %y_i = x_i'\theta + v_i, \quad i=1,\dots, n  
  y_i = g(x_i,\theta) + v_i, \quad i=1,\dots, n  
\end{equation}
where $v_i$ is a random component with $E(v_i)=0$ and
$V(v_i)=\sigma^2$.  


Given a batch of observations $y_1,\dots,y_n$ let $\hat\theta_{n}$
denote an estimate of $\theta$ based on these data.  When a new
observation $y_{n+1}$ arrives we may find $\hat\theta_{n+1}$ by
refitting the model to $y_1,\dots,y_n,y_{n+1}$ or we may find
$\hat\theta_{n+1}$ by updating $\hat\theta_n$ (which is often
computationally cheaper). This is what we mean by \emph{online
  estimation}.

% The
% \emph{1--step ahead forecast} of the response at time $t_n$ is then
% $f_n = g(x_n,\hat\theta_{n-1})$ so that $e_n= y_n - g(x_n,
% \hat\theta_{n-1})$ is the \emph{1--step ahead prediction error}.


\section{Batch estimation - iterative least squares}
\label{sec:xxx}

For later use, assume that there is a non--negative weight $w_i$
associated with measurement $y_i$ for $i=1,\dots, n$. 
A batch estimate of $\theta$ can
be obtained using for example the least squares method. 

The least squares estimate for
$\theta$ is the value of $\hat\theta$ which minimizes the objective function
\begin{equation}
  \label{eq:rlsobj1}
  l(\theta) = \sum_i w_i (y_i - g(x_i,\theta))^2.  
\end{equation}

When
convenient we write $g_i(\theta)$ instead of $g(x_i, \theta)$. Let
$y=(y_1,\dots, y_n)'$, $g=g(\theta)=(g_1(\theta), \dots,
g_n(\theta))'$. 
Let
$b_i = D_\theta g_i(\theta)$ ($K$--vector) and let $B=[d_1 \dots
d_n]$ ($K\times n$ matrix). Let also $\Lambda=diag(w_1, \dots
w_n)$ ($n \times n$ matrix) and $V=B\Lambda B'$ ($K\times K$ matrix). 

To minimize $l(\theta)$ we must solve $S(\theta) = D_\theta
l(\theta)=0$ (there are $K$ equations). We find
\begin{displaymath}
  S(\theta)_k
  =
  D_{\theta_k} l(\theta) 
  =
  2\sum_i w_i(y_i-g_i(\theta))D_{\theta_k} g_i(\theta)  
\end{displaymath}

Hence
\begin{displaymath}
  S(\theta)
  =
  2\sum_i w_i(y_i-g_i(\theta))D_\theta g_i(\theta)
  =
  2 B \Lambda (y-g)
\end{displaymath}
  
Further let
\begin{eqnarray*}
  j_{rk}
  &=& D_{\theta_r}S(\theta)_k
  = 2 \sum_i w_i\{(y_i-g_i(\theta))D_{\theta_r\theta_k}
  g_i(\theta)- D_{\theta_r} g_i(\theta)D_{\theta_k} g_i(\theta)\}  \\
  u_{rk}
  &=& E(j_{rk})
  = -2 \sum_i w_i D_{\theta_r} g_i(\theta)D_{\theta_k} g_i(\theta)\}
  = -2 (V)_{rk}
\end{eqnarray*}

Then the Newton step becomes as follows: Let
$\theta^*$ denote the current estimate of $\theta$ and iterate the
following scheme until convergence
\begin{enumerate}
\item Set
  $\theta = \theta^* + (B_{\theta^*}\Lambda B_{\theta^*}')\inv B_{\theta^*}\Lambda (y-g(\theta^*)) $  
\item Set
  $\theta^* = \theta$.
\end{enumerate}


\paragraph{The linear case}
In the special case where $g(x_i, \theta)=x_i'\theta$ we have
$b_i=x_i$ and $B=X'$ where $X'=[x_1, \dots, x_n]$. In this case the
Newton step becoms
\begin{displaymath}
  \theta = \theta^* + (X'\Lambda X)\inv X'\Lambda(y-X\theta)=
  (X'\Lambda X)\inv X'\Lambda y
\end{displaymath}
so in this case Newton converges in one iteration. We write this out
in more detail as:
\begin{displaymath}
  \hat \theta 
  = (\sum_i w_i x_i x_i')\inv (\sum_i w_i x_i y_i) 
\end{displaymath}

%%%If the $v_i$s are independent then  the variance of $\hat\theta$ is $\sigma^2(\sum_i x_i x_i')\inv$. 





\section{Online estimation}
\label{sec:xxx}

% By online estimation
% we mean that it is not necessary to refit the model to a whole batch
% of data every time a new observation arrives. 
% Many online
% estimation methods have the form
% \begin{displaymath}
%   \hat\theta_{n+1} = \hat\theta_{n} + g_{n+1}(y_{n+1}-x_{n+1}'\hat\theta_{n})
% \end{displaymath}
% for a suitable choice of the vector $g_n$; that is the current
% estimate is equal to the previous estimate
% plus a multiple of the forecast error. 

In the following we shall assume that $w_i=\lambda^{t_n-t_i}$ where $0
< \lambda \le 1$ is called a \emph{forgetting
  factor}. Hence the weights are $(\lambda^{t_n-t_1},
\lambda^{t_n-t_2}, \dots \lambda^{t_n-t_{n-1}}, 1)$. 
Common choices for $\lambda$ are values slightly smaller than $1$,
say $0.99, 0.95$ or so. 


\subsection{Recursive least squares -- the general case}
\label{sec:xxx}



Suppose now that we have 
$n+1$ observations $y_1, \dots, y_{n+1}$. The objective
function to minimize is
\begin{displaymath}
  \tilde l(\theta)=\sum_{i=1}^{n+1} \lambda^{t_{n+1}-t_i}(y_i - g_i(\theta))^2
\end{displaymath}

We extend the definitions of the previously defined quantities as
follows. Let $\tilde B = [b_1, \dots b_n, b_{n+1}]=[B, b_{n+1}]$,
$\tilde y=(y',y_{n+1})'$, $\tilde g = (g', g_{n+1})'$. Let
$\gamma=\lambda^{t_{n+1}-t_n}$. 

\begin{displaymath}
  \matrxc{
    \gamma \Lambda & 0 \\
    0  & 1
  }
\end{displaymath}

Then 
\begin{eqnarray*}
  \tilde S 
  &=& 2 \tilde B \tilde \Lambda (\tilde y - \tilde g)\\
  &=& 2 [B, b_{n+1}] \matrxc{\gamma \Lambda (y-g) \\ y_{n+1}-g_{n+1}}\\
  &=& 2 [\gamma B \Lambda (y-g)+ b_{n+1}(y_{n+1}-g_{n+1})\\
  &=& 2 \gamma S + 2 b_{n+1}(y_{n+1}-g_{n+1})
\end{eqnarray*}

Likewise we get
\begin{eqnarray*}
  \tilde V 
  &=& \tilde B \tilde \Lambda \tilde B' \\
  &=& [\gamma B \Lambda : b_{n+1}] \tilde B'\\
  &=& [\gamma B \Lambda : b_{n+1}]
  \matrxc{B'\\b_{b+1}'} \\
  &=& \gamma B \Lambda B' +  b_{n+1}b_{n+1}'\\
  &=& \gamma V +  b_{n+1}b_{n+1}'\\
\end{eqnarray*}

So now the Newton step is: Let $\theta^*$ be the current estimate and set
\begin{enumerate}
\item Set 
  $
  \theta = \theta^* + \tilde V\inv S
  $
\item Set $\theta^*=\theta$. 
\end{enumerate}


When $y_{t+1}$ is observed we have an estimate $\hat \theta_n$ based
on data $y_1,\dots,y_n$ and a corresponding matrix $V$. Hence $\hat
\theta_n$ is an obvious initial value for the Newton iteration and -
since $V$ is known - $\tilde V$ can be inverted easily using the
Woodbury identity. 

In subsequent iterations $\tilde V\inv$ needs to be evaluated, but if
$\tilde V$ does not change much between subsequent iterations we can
use the following approximation: If $A$ is invertible and $R$ is a
small relative to $A$ then $(A+R)\inv \approx (A\inv - A\inv R
A\inv)$. Hence if $\tilde V\inv$ is found in the first iteration and
$\check V$ is obtained in the next iteration then
\begin{displaymath}
  \check V\inv = (V  + (\check V-V))\inv 
  \approx V\inv - V\inv (\check V-V) V\inv
  = 2 V\inv - V\inv \check V V\inv
\end{displaymath}






\subsection{Recursive least squares  -- the linear case}
\label{sec:xxx}

In the linear case the objective function to minimize is
$
    l(\theta) = \sum_i \lambda^{t_n-t_i} (y_i - x_i'\theta)^2.  
$

Let 
\begin{displaymath}
  Z_n = \sum_{i=1}^n \lambda^{t_n-t_i} x_i x_i',
  \quad 
  P_n = Z_n\inv,
  \quad 
  \psi_n = \sum_{i=1}^n \lambda^{t_n-t_i} x_i y_i
\end{displaymath}

Then the least squares estimate minimizing (\ref{eq:rlsobj1}) based on $n$ observations is
\begin{eqnarray*}
  \label{eq:xxx}
  \hat \theta_n 
  &=&  
  (\sum_i \tilde x_i \tilde x_i')\inv (\sum_i \tilde x_i  \tilde y_i) 
  =
  (\sum_i \lambda^{t_n-t_i} x_i x_i')\inv (\sum_i \lambda^{t_n-t_i}
  x_i  y_i) = Z_n\inv \psi_n = P_n \psi_n .
\end{eqnarray*}

The Recursive Least Squares (or RLS) algorithm goes as follows:
Given $\hat\theta_{n-1}$ and $P_{n-1}$ define
\begin{eqnarray}
  \gamma &=& \lambda^{t_n-t_{n-1}} \nonumber \\
  k %&=& \frac 1 {1+\gamma\inv x_n'P_{n-1} x_n} \gamma\inv P_{n-1} x_n  
  &=& \frac 1 {\gamma +  x_n'P_{n-1}  x_n} P_{n-1} x_n    \label{eq:rlsk}
\end{eqnarray}

Then 
\begin{eqnarray}
  \hat \theta_n &=&   
  \hat\theta_{n-1} + k (y_n - x_n' \hat\theta_{n-1})    \label{eq:rls1} \\
  P_n &=& (I- k x_n')\gamma\inv P_{n-1}   \label{eq:rls2}  
\end{eqnarray}

A proof of \eqref{eq:rls1} and \eqref{eq:rls2} is given in
Section~\ref{sec:rlsproof}.  


\subsection{Initialization}
One way of starting the RLS algorithm is to first process a batch of the
$B$ first observations to obtain $\hat\theta_B$ and $P_B$ and then
continue the RLS algorithm (\ref{eq:rls1}) and (\ref{eq:rls2}) from
there. Here $B$ must be chosen large enough to ensure that $P_B$ is
invertible. 

An approximate initialization is based on the following: Classical
least squares theory gives $\var(\hat\theta_n) = \sigma^2 P_n$
meaning that the covariance of $\hat\theta_n$ is proportional to
$P_n$. At $n=0$, the prior knowledge of $\theta$ is often sparse
and this suggests to take $P_0$ to be ``large'', for example
$P_0=\delta I$ for some large value $\delta$. For a large dataset,
the initial value of $P_0$ is not important because it will be
downweighted because of the exponential forgetting factor
$\lambda$. If a prior choice of $\theta$ is available, then this value
can of course be used in the initialization. Otherwise a typical
initialization would be to set $\theta_0 = 0$. 


\subsection{Comments on the RLS}
\label{sec:xxx}


Notice that the introduction of the forgetting factor corresponds to
making the assumption that $\var(y_i) =
\frac{\sigma^2}{\lambda^{t_n-t_i}}= \frac{\lambda^{t_i}
  \sigma^2}{\lambda^{t_n}}$ such that 1) observations in the far past
will have a much larger variance than those close to $t_n$ and 2) the
variance of previous observations will increase as $n$ increases. The
latter point is less appealing. 

Notice also that if $\hat\theta_{n-1}$ is an
unbiased estimate of $\theta$ then $\hat\theta_n$ is also unbiased. 
However $\hat\theta_n$ is not in general consistent. Consider for
example the model $y_i=\theta + e_i$. In this case $Z_n=\sum_{i=1}^n
\lambda^{t_n-t_i}.$ If $t_i = i$ for all $t_i$ then $Z_n$ is a
convergent series when $\lambda<1$ which implies that $P_n= Z_n\inv$
does not go to zero as $n$ goes to infinity.

% Recalling that $C_n=\sigma^2P_n=\var(\hat\theta_n))$ 
% we can write  $k$ as 
% \begin{equation}
%   \label{eq:rlsk2}
%   k = \frac {\sigma^2} {\sigma^2\gamma +  \sigma^2x_n'P_{n-1}  x_n} P_{n-1} x_n   
%   = \frac 1 {\sigma^2\gamma +  x_n'C_{n-1}  x_n} C_{n-1} x_n     
% \end{equation}
% Notice that $\sigma^2\gamma +  x_n'C_{n-1}  x_n =
% \var(x_n'\hat\theta_{n-1} + \sqrt{\gamma} v_n)$ so this quantity has
% almost the form of a prediction variance.
% The update equations can then be written as
% \begin{eqnarray}
%   \hat \theta_n &=&   
%   \hat\theta_{n-1} + k (y_n - x_n' \hat\theta_{n-1})  
%   \label{eq:rlsb2} \\
%   C_n &=& (I- \frac 1 {\sigma^2\gamma +  x_n'C_{n-1}  x_n} C_{n-1} x_n x_n')\gamma\inv C_{n-1}   
%   \label{eq:rlsp2}  
% \end{eqnarray}



\section{Least Mean Squares Algorithm}
\label{sec:xxx}

An approximation to the RLS algorithm can be obtained by defining
$\tilde k = \rho x_n$ for a small value of $\rho$. Given
$\hat\theta_{n-1}$ set $\hat\theta_n^0 = \hat\theta_{n-1}$ and do the
following iteration until (hopefully) convergence
\begin{equation}
  \label{eq:lms}
  \hat\theta_n^j =   \hat\theta_n^{j-1} + \rho x_n' (y_n - x_n' \hat\theta_n^{j-1}).  
\end{equation}

The scheme (\ref{eq:lms}) is called a \emph{least mean squares} (or \emph{LMS})
algorithm. A problem with the LMS algorithm is that the step sizes
depends on the scaling of the covariates $n_n$. The \emph{normalized
  least mean squares} (or \emph{NLMS}) algorithm tries to remedy this
problem by normalizing the covariates to have length $1$:
\begin{equation}
  \label{eq:nlms}
  \hat\theta_n^j =   \hat\theta_n^{j-1} + \rho \frac 1 {x_n'x_n} x_n' (y_n - x_n' \hat\theta_n^{j-1}).
\end{equation}

\appendix

\section{Proof of RLS updates}
\label{sec:rlsproof}

To prove (\ref{eq:rls1}) first notice that we get the following
recurrence relations for $W_n$ and $\psi_n$:
\begin{eqnarray}
  \label{eq:xxx}
  W_n 
  &=&
  \sum_{i=1}^n \lambda^{t_n-t_i} x_i x_i' 
  = 
  x_n x_n' + \sum_{i=1}^{n-1} \lambda^{t_n-t_i} x_i x_i' \nonumber \\
  &=&
  x_n x_n' + \lambda^{t_n-t_{n-1}} \sum_{i=1}^{n-1}
  \lambda^{t_{n-1}-t_i} x_i x_i' 
  =
  x_n x_n + \lambda^{t_n-t_{n-1}} W_{n-1}
\end{eqnarray}
and similarly
\begin{eqnarray}
  \label{eq:xxx}
  \psi_n = x_n y_n + \lambda^{t_n-t_{n-1}} \psi_{n-1}
\end{eqnarray}

We need a recurrence relation for $P_n = W_n\inv$ as well. 
Let $\gamma=\lambda^{t_n-t_{n-1}}$. Using the Woodbury identity (\ref{eq:wood}) we find
\begin{eqnarray}
  \label{eq:xxx}
  P_n
  &=& 
  (\gamma W_{n-1} + x_n x_n')\inv  \nonumber\\
  &=&
  \gamma\inv W_{n-1}\inv - \frac 1 {1+\gamma\inv x_n'W_{n-1}\inv x_n} 
  \gamma\inv W_{n-1}\inv x_n x_n' \gamma\inv W_{n-1}\inv \nonumber \\
  &=&
  \gamma\inv P_{n-1} - \frac 1 {1+\gamma\inv x_n'P_{n-1} x_n} 
  \gamma\inv P_{n-1} x_n x_n' \gamma\inv P_{n-1} 
\end{eqnarray}



To avoid making the notation too cumbersome let  $Q=P_{n-1}$ and
$\phi=\psi_{n-1}$. Then 
\begin{displaymath}
  P_n
  = 
  \gamma\inv Q - \frac 1 {1+\gamma\inv x_n'Q x_n} 
  \gamma\inv Q x_n x_n' \gamma\inv Q.
\end{displaymath}

The least squares estimate of $\theta$
based on the first $n-1$ observations is consequently
\begin{displaymath}
  \hat\theta_{n-1} = P_{n-1} \psi_{n-1} = Q \phi.
\end{displaymath}


For later use define $c=-\frac 1 {1+\gamma\inv x_n'Q x_n}$ and notice that
$\gamma\inv x_n'Q x_n = -(1 + \frac 1 c)$. 
Now it is all straight forward:
\begin{eqnarray*}
  \label{eq:xxx}
  \hat \theta_n 
  &=& 
  P_n  \psi_n \\
  &=&
  (\gamma\inv Q + c \gamma\inv Q x_n x_n' \gamma\inv Q )
  (\gamma \phi + x_n y_n) \\
  &=&
  \gamma\inv Q \gamma \phi
  + 
  c \gamma\inv Q x_n x_n' \gamma\inv Q \gamma \phi + 
  \gamma\inv Q x_n y_n
  + 
  c \gamma\inv Q x_n x_n' \gamma\inv Q x_n y_n \\
  &=&
  \hat\theta_{n-1} 
  + 
  c \gamma\inv Q x_n x_n' \hat\theta_{n-1} + 
  \gamma\inv Q x_n y_n
  + 
  c \gamma\inv Q x_n x_n' \gamma\inv Q x_n y_n \\  
  &=&
  \hat\theta_{n-1} 
  + 
  c \gamma\inv Q x_n x_n'  \hat\theta_{n-1} + 
  \gamma\inv Q x_n y_n
  - 
  c \gamma\inv Q x_n  (1+\frac 1 c) y_n \\
  &=&
  \hat\theta_{n-1} 
  + 
  c \gamma\inv Q x_n x_n' \hat\theta_{n-1} 
  -
  c \gamma\inv Q x_n y_n \\
  &=&
  \hat\theta_{n-1} 
  - 
  c \gamma\inv Q x_n (y_n - x_n' \hat\theta_{n-1})  
\end{eqnarray*}

% Defining
% $  
% k 
% = \frac 1 {1+\gamma\inv x_n'Q x_n} \gamma\inv Q x_n
% = \frac 1 {1+\gamma\inv x_n'P_{n-1} x_n} \gamma\inv P_{n-1} x_n
% $

With $k$ defined as in \eqref{eq:rlsk} we get
\begin{eqnarray}
  \label{eq:xxx}
  \hat\theta_n = \hat\theta_{n-1} + k (y_n - x_n' \hat\theta_{n-1})
\end{eqnarray}
which proves (\ref{eq:rls1}). 
%Notice that $f_n = x_n' \hat\theta_{n-1}$ can be regarded as a
%\emph{1--step ahead forecast}. 
To prove (\ref{eq:rls2}) notice that 
\begin{eqnarray*}
  P_n
  &=& 
  \gamma\inv Q - \frac 1 {1+\gamma\inv x_n'Q x_n} 
  \gamma\inv Q x_n x_n' \gamma\inv Q
  = 
  \gamma\inv Q - k x_n' \gamma\inv Q \\
  &=&
  (I-k x_n') \gamma\inv Q
  = (I-k x_n') \gamma\inv P_{n-1}.  
\end{eqnarray*}


\section{Woodbury identity}
\label{sec:xxx}



\emph{Woodbury identity} Let $A$ be and invertible matrix and $a$ a
vector.  Then
\begin{equation}
  \label{eq:wood}
  (A+aa')\inv = A\inv - \frac 1 {1+a'A\inv a} A\inv a a' A\inv.  
\end{equation}





% \section{State space models and the Kalman filter}
% \label{sec:kfoverview}

% Linear state space models (SSMs) are traditionally specified as consisting of 
% \begin{eqnarray}
% y_i &=& F^\top_i \theta_i + v_i, \ \ v_i \sim N(0,V_i)  \\
% \theta_i &=& G_i \theta_{i-1} + w_i, \ \ w_i \sim N(0, W_i)\\
% \theta_0 &\sim& N(m_0,C_0)
% \label{eq:ssm1}
% \end{eqnarray}
% known as respectively  {\em
%   observation equation},
% the {\em system equation}
% and the 
% {\em initial distribution} (of $\theta_0$).
% Based on observed data we can then calculate  (the distribution of)
% the parameters $\theta_i$. 
% The Kalman filter works by ``updating $\theta_n$ as new observations
% arrive'': Let $D_n$ denote the observations
% made up to time $t_n$. Given is that
% $\theta_{n-1}|D_{n-1}\sim N(m_{n-1}, C_{n-1})$. Then
% \begin{eqnarray}
% \theta_n | D_{n-1} &\sim& 
% N(
% \overbrace{G_nm_{n-1}}^{a_n}, 
% \overbrace{G_n C_{n-1}G_n^\top + W_n}^{R_n}
% ) \\
% y_n|D_{n-1} &\sim&
% N(
% \overbrace{F_n^\top a_n}^{f_n},
% \overbrace{F_n^\top R_n F_n + V_n}^{Q_n}
% ) \\
% \theta_n|D_{i} &\sim&
% N(
% \overbrace{a_n+ \underbrace{R_n F_n
%     Q_n^{-1}}_{A_n}\underbrace{(y_n-f_n)}_{e_n}}^{m_n},
% \overbrace{R_n - A_n Q_n A_n^\top}^{C_n}
% ) \label{eq:kf3}
% \end{eqnarray}

% Hence, the only thing needed to be stored from time $t-1$ to time $t$
% is $(m_{t-1}, C_{t-1})$. 
% Here $f_t$ is the {1--step forecast}, $e_t$ is the {\em 1--step forecast
%   error} while $A_t$ is the {\em Kalman gain}.


% Returning to the regression setting \eqref{eq:rlsregmodel} we consider
% a special instance of the state--space model, namely
% \begin{eqnarray}
% y_n &=& x_n' \theta_n + v_n, \ \ v_n \sim N(0,V)  \\
% \theta_n &=& \theta_{n-1} + w_n, \ \ w_n \sim N(0, W)\\
% \theta_0 &\sim& N(m_0,C_0)
% \label{eq:ssm2}
% \end{eqnarray}

% In this case, $a_n=m_{n-1}$, $R_n=C_{n-1} + W$, $f_n = x_n' m_{n-1}$
% and $Q_n = x_n'R_n x_n + V$. Hence the updated posterior mean of $m$ becomes
% \begin{equation}
%   \label{eq:kfm}
%   m_n = m_{n-1} + \frac{1}{x_n'R_n x_n + V} R_n x_n (y_n - x_n' m_{n-1})  
% \end{equation}
% Furthermore, notice that
% $R_n - A_n Q_n A_n^\top  = R_n - R_n F_n Q_n \inv F_n'R_n = (I-R_n F_n
% Q_n \inv F_n')R_n$ which in our special case becomes
% \begin{equation}
%   \label{eq:kfc}
%   C_n = (I-\frac{1}{x_n'R_n x_n + V} R_n x_n x_n')R_n   
% \end{equation}

% Notice the similarity between \eqref{eq:rlsb2}  and \eqref{eq:kfm} and
% between \eqref{eq:rlsp2} and \eqref{eq:kfc}. In fact, if $W=0$ and $\lambda=1$
% then the expressions are identical.  















% \section{Example -- The Nile data}
% \label{sec:xxx}

% As an example consider the Nile data from R. 
% The \texttt{RLS} package (which in NOT on CRAN) allows for online estimation.
% Fit a 0, 1. and 2. order
% polynomial to data (while estimating the forgetting factor on the fly)

% @ 
% <<>>=
% library(RLS)
% yy <- as.numeric(Nile)
% xx <- seq_along(yy)
% @ %def 

% @ 
% <<print=T>>=
% m0e <- rls(xx,yy, degree=0)
% m1e <- rls(xx,yy, degree=1)
% m2e <- rls(xx,yy, degree=2)
% @ %def 


% Rather than estimating the forgetting factor we can fix it at some
% value: 
% @ 
% <<>>=
% m0f <- rls(xx,yy, degree=0, lambda=0.99, optim=F)
% m1f <- rls(xx,yy, degree=1, lambda=0.99, optim=F)
% m2f <- rls(xx,yy, degree=2, lambda=0.99, optim=F)
% @ %def 

% @ 
% <<rlsfig1, fig=T, include=F, echo=F, width=8, height=5>>=
% par(mfrow=c(1,2))
% plot(xx,yy,cex=.5)
% lines(xx, fitted(m0e), col=3,lty=2,lwd=2)
% lines(xx, fitted(m1e), col=4,lty=3,lwd=2)
% lines(xx, fitted(m2e), col=6,lty=4,lwd=2)
% plot(xx,yy,cex=.5)
% lines(xx, fitted(m0f), col=3,lty=2,lwd=2)
% lines(xx, fitted(m1f), col=4,lty=3,lwd=2)
% lines(xx, fitted(m2f), col=6,lty=4,lwd=2)
% legend(-1, 630, c("0 degree", "1 degree", "2 degree"), col = c(3,4,6),
%        lty = c(2, 3, 4), 
%        merge = TRUE)

% @ %def 

% \begin{figure}[h]
%   \centering
%   \includegraphics{fig/DYNEST-rlsfig1}
%   \caption{Fitted values for Nile data. Left: $\lambda$ estimated from
%   data. Right: $\lambda$ fixed at $0.99$.}
%   \label{fig:rlsfig1}
% \end{figure}


% Some of th regression coefficients are 
% @ 
% <<>>=
% head(m1e$coef)
% @ %def 

% We can make predictions at a specific time point
% @ 
% <<>>=
% at <- 70
% newxx <- xx[-(1:at)]
% p0e <- predict(m0e, newdata=newxx, at=at)
% p1e <- predict(m1e, newdata=newxx, at=at)
% p2e <- predict(m2e, newdata=newxx, at=at)

% p0f <- predict(m0f, newdata=newxx, at=at)
% p1f <- predict(m1f, newdata=newxx, at=at)
% p2f <- predict(m2f, newdata=newxx, at=at)
% @ %def 

% @ 
% <<rlsfig2, fig=T, include=F, echo=F, width=8, height=5>>=
% par(mfrow=c(1,2))
% plot(xx,yy,cex=.5)
% abline(v=at)
% lines(newxx,  p0e, col=3,lty=2,lwd=2)
% lines(newxx,  p1e, col=4,lty=3,lwd=2)
% lines(newxx,  p2e, col=6,lty=4,lwd=2)
% plot(xx,yy,cex=.5)
% abline(v=at)
% lines(newxx,  p0f, col=3,lty=2,lwd=2)
% lines(newxx,  p1f, col=4,lty=3,lwd=2)
% lines(newxx,  p2f, col=6,lty=4,lwd=2)
% legend(-1, 630, c("0 degree", "1 degree", "2 degree"), col = c(3,4,6),
%        lty = c(2, 3, 4), 
%        merge = TRUE)

% @ %def 


% \begin{figure}[h]
%   \centering
%   \includegraphics{fig/DYNEST-rlsfig2}
%   \caption{Predicted values  for Nile data. Left: $\lambda$ estimated from
%   data. Right: $\lambda$ fixed at $0.99$.}
%   \label{fig:rlsfig2}
% \end{figure}




% @ 
% <<rlsfig3, fig=T, include=F, echo=F, width=8, height=5>>=
% par(mfrow=c(1,2))
% m1fa <- rls(xx,yy, degree=1, lambda=0.99, optim=F)
% m1fb <- rls(xx,yy, degree=1, lambda=0.95, optim=F)
% zzza <- sqrt(m1fa$var)[-(1:2),]
% matplot(zzza, type='l')
% zzzb <- sqrt(m1fb$var)[-(1:2),]
% matplot(zzzb, type='l')
% @ %def 



% \begin{figure}[h]
%   \centering
%   \includegraphics{fig/DYNEST-rlsfig3}
%   \caption{Standard errors of parameter estimates for 1. degree
%     polynomials: Left: $\lambda=0.99$, right: $\lambda=0.95$.}
%   \label{fig:rlsfig2}
% \end{figure}



\end{document}
