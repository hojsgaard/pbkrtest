---
title: Inference in mixed models in R - beyond the usual asymptotic likelihood ratio
  test
author: |
  | Søren Højsgaard^[University of Aalborg, Denmark]
  | Ulrich Halekoh^[University of Southern Denmark, Denmark]
date: "June 12, 2018"
output:
#  html_document:
#    standalone: true
#    self-contained: true
#  pdf_document:
#    includes:
#      in_header: preamble.txt
#    toc: true
#    toc_depth: 3
#    keep_tex: true
  beamer_presentation:
    includes:
      in_header: preamble.txt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, size="scriptsize", message=FALSE, cache=FALSE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
library(lme4)
library(doBy)
library(broom)
library(magrittr)
library(pbkrtest)
options("digits"=4)
options("show.signif.stars"=FALSE)
```

## Outline and take-home message

* Mixed models (random effects, random regression etc.) models handled
	by `lme4` package in R.

* Tests are based on $\chi^2$ approximation of LR test statistic.

	+ Works fine with "large samples" / "large dataset"

	+ But a dataset can be large with respect to some aspect of a
    model while small with respect to other.

* Package `pbkrtest` provides some remedies:

	+ Base test on F-statistic, where denominator degrees of freedom
      are estimated from data.

	+ Base test of parametric bootstrap where data are simulated under
      the model.

	+ Parametric bootstrap carries over to e.g. generalized linear
      mixed models.

* Look at simulated and real data

* Shortcomings of `pbkrtest`


## History: The degree-of-freedom police

* Years ago, Ulrich Halekoh and SH colleagues at ``Danish Institute
  for Agricultural Sciences''

	+ Main concern: Help protect researcher colleagues from reporting
	  effects to be "more significant than they really are".

	+ Many studies called for random effects models - and for `PROC MIXED`
	(from SAS)

	+ `PROC MIXED` reports (by default) $p$--values from asymptotic
    likelihood ratio test.

	+ Common advice: Account for uncertainty in estimate of variance by doing
	$F$-test instead. Use  Satterthwaite or Kenward-Roger approximation of
		denominator degrees of freedom in $F$-test -- in an attempt not to get
			things ``too wrong''.

* Then R became popular;

	+ Mixed models fitted with nlme and lme4 package

	+ No Satterthwaite or Kenward-Roger approximation, so
	our common advice fell apart.

---


* SH raised the issue on R-help - 2006: \texttt{[R] how calculation degrees freedom} [see](https://stat.ethz.ch/pipermail/r-help/2006-January/087013.html):

    + SH: Along similar lines ...  probably in recognition of the
      degree of freedom problem. It could be nice, however, if anova()
      produced ...

	+ Doug Bates: I don't think the "degrees of freedom police" would
      find that to be a suitable compromise. :-)

* In reply to related question:

	+ Doug Bates: I will defer to any of the "degrees of freedom
	police" who post to this list to give you an explanation of why there
	should be different degrees of freedom.

* The point being:

	+ Quite different views on whether the degree-of-freedom issue
      really is an issue or not.


## Example: Double registration in labs 


```{r echo=FALSE}
dub=
structure(list(y1 = c(0.7, 1.01, -0.35, 0.39, -0.69, -0.06, -2.45, 
-1.8, 1.49, 1.53, 0.94, 1.02), y2 = c(0L, 0L, 0L, 2L, 1L, 0L, 
0L, 2L, 4L, 5L, 2L, 0L), trt = structure(c(1L, 1L, 1L, 1L, 1L, 
1L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("ctrl", "trt1"), class = "factor"), 
    subj = structure(c(1L, 1L, 2L, 2L, 3L, 3L, 4L, 4L, 5L, 5L, 
    6L, 6L), .Label = c("subj1", "subj2", "subj3", "subj4", "subj5", 
    "subj6"), class = "factor")), .Names = c("y1", "y2", "grp", 
"subj"), row.names = c(NA, -12L), class = "data.frame")
dub$y1 <- round(dub$y1  + 1, 2)
dub$y1[-(1:6)] <- dub$y1[-(1:6)] + 2 
```

![](img/bacteria-growing-in-petri-dishes.jpg){width=120px}


Clustered data: 

* Compare two groups (treatment with a control); 

* M units (petri plates, persons, animals...) per group; 

* Each unit is measured R times. Measurements on same unit are positively correlated.


---

Simulated data: $N=3$ subjects per group, $R=2$ replicated measurements per subject.
```{r}
dub
```




---

Problem/issues: If we ignore  clustering/positive correlation:

* pretending to have more information than we have
* standard errors of estimates become too small
* $p$ values become too small
* effects appear stronger than they really are.

Notice:

* Measuring the same unit many many times will make the dataset
  larger, but will not really add many more chunks of information
  (depending on the size of the within-subject correlation, of
  course).
* Instead, more units are needed.

---

```{r}
lg1 <- lm(y1 ~ grp, data=dub)
lg1 %>% summary %>% coef %>% as.data.frame -> tb1
tb1$"Pr(>|X^2|)" = 1 - pchisq(tb1[,3]^2, df=1)
tb1
```

Notice: the $t$-test "accounts for" the uncertainty in the estimate of the standard error.

---

### Alternative: Analyze average

<!-- ```{r} -->
<!-- dub$E <- with(dub, interaction(grp, subj)) -->
<!-- lg <- aov(y1 ~ grp + Error(E), data=dub) -->
<!-- summary(lg)[[1]] -->
<!-- ``` -->


```{r}
duba <- aggregate(y1 ~ grp + subj, FUN=mean, data=dub)
lm(y1 ~ grp, data=duba) %>% summary %>% coef
```

* Works fine (gives the correct test) in (nearly) balanced cases.

* Does not provide estimate of between and within subject variation (not necessarily severe problem here).

* Analyzing-the-average is often not a feasible strategy.

---

### Alternative: Random effects model

```{r}
lg2 <- lmer(y1 ~ grp + (1|subj), data=dub)
tidy(lg2)
```

```{r}
sm2 <- update(lg2, .~. -grp)
as.data.frame(anova(lg2, sm2))
```

Notice: Test is based in the $\chi^2$ distribution (i.e.\ that the variance is known)

---

### Alternatives in the pbkrtest package:
```{r cache=TRUE}
KRmodcomp(lg2, sm2)
PBmodcomp(lg2, sm2)
```

Notice: Same $p$-value as when analyzing average.
	

---

## The Kenward--Roger approach

### The Kenward--Roger modification of the F--statistic

For multivariate normal data
\[
  Y_{n\times 1} \sim  N( X_{n\times p} \beta_{p\times 1}, \Sigma)
\]
we consider the test of the hypothesis
\[
  \Lb_{d \times p} (\betab -\bm \beta_0) = 0
\]
<!-- where $\Lb$ is a regular matrix of estimable functions of $\bm \beta$. -->

With $\hat \betab \sim N_d(\betab, \bm\Phi)$, a Wald statistic is

$$
  W = [\Lb(\hat\betab - \betab_0)]\transp [L\bm\Phi L\transp]\inv [\Lb(\hat\betab - \betab_0)]
$$
which is asymptotically $W \sim \chi^2_d$ under the null hypothesis.

---

A scaled version of $W$ is
\begin{displaymath}
  F = \frac{1}{d} W
\end{displaymath}


* Asymptotically $F \sim \frac{1}{d} \chi^2_d$
under the null hypothesis

* Think of as the limiting distribution
of an $F_{d,m}$--distribution as $m\rightarrow \infty$

* To account for the fact that $\bm\Phi=\var(\hat\beta)$
is estimated from data, we must come up with a better estimate of the
denominator degrees of freedom $m$ (better than $m=\infty$).

* That was what Kenward and Roger worked on...

---

The linear hypothesis $\Lb \betab = \betab_0$
can be tested via the  Wald-type  statistic
\begin{gather*}
F= \frac{1}{r}(\hat \betab - \betab_0)^\top \Lb^\top   (\Lb^\top \bm \Phi(\ssb) \Lb)^{-1}
 \Lb (\hat \betab - \betab_0)
\end{gather*}

\begin{itemize}
\item $\bm \Phi (\sigmab) = (\bm X^\top \bm \Sigma(\sigmab) \bm X)^{-1} \approx
\cov(\hat \betab)$, $\hat  \betab$ REML estimate of $\betab$
\item $\ssb$: vector of REML estimates of the elements of $\Sigmab=\var(Y)$
\end{itemize}

Kenward and Roger (1997)

* replaced $\bm \Phi$ by
an improved small sample approximation $\bm \Phi_A$

* scaled $F$ by a factor $\lambda$

* determined denominator degrees of freedom $m$ by matching moments of $F/\lambda$ with an $F_{d,m}$ distribution.


<!-- ## Kenward and Roger's modification -->

<!-- Kenward and Roger (1997) modify the test statistic -->
<!-- \begin{itemize} -->
<!-- \item $\bm \Phi$ is replaced by an improved small sample approximation $\bm \Phi_A$ -->
<!-- \end{itemize} -->

<!-- Furthermore -->
<!-- \begin{itemize} -->
<!-- \item the statistic $F$ is scaled by a factor $\lambda$, -->
<!-- \item denominator degrees of freedom $m$ are determined -->
<!-- \end{itemize} -->
<!-- such that the approximate expectation and variance are those of a $F_{d,m}$ distribution. -->

<!-- ## Restriction on covariance, some details -->

<!-- \begin{itemize} -->

<!-- \item Consider only situations where -->
<!-- \begin{gather*} -->
<!-- \Sigmab= \sum_i \sigma_i \bm G_i, \quad \bm G_i \, \text{known matrices} -->
<!-- \end{gather*} -->

<!-- \item Variance component and random coefficient models satisfy this -->
<!-- restriction. -->

<!-- \item $\bm \Phi_A(\ssb)$  depends now only on the first  partial derivatives of $\bm \Sigma^{-1}$: -->
<!-- \begin{displaymath} -->
<!-- \frac{\partial \bm \Sigmab^{-1}}{\partial \sigma_i} = - \bm \Sigma^{-1} -->
<!-- \frac{\partial \bm \Sigmab}{\partial \sigma_i} -->
<!-- \bm \Sigma^{-1}. -->
<!-- \end{displaymath} -->

<!-- \item $\bm \Phi_A(\ssb)$  depends also on $\var(\ssb)$. -->

<!-- \item Kenward and Roger propose to estimate -->
<!--   $\var(\ssb)$ via the  inverse expected information matrix. -->
<!-- \end{itemize} -->


## Shortcommings of Kenward-Roger

\begin{itemize}
\item The Kenward--Roger approach is no panacea.
\item In the computations of the degrees of freedom we need to compute
  \begin{displaymath}
    G_j {\bm \Sigma}^{-1} G_j
  \end{displaymath}
  where $\Sigmab= \sum_i \sigma_i \bm G_i$.
  Can be space and time consuming!
\item An alternative is a Sattherthwaite--kind approximation which is faster to
  compute. Will come out in next release of \pkg{pbkrtest} (code not tested yet).
  Way faster...

\item What to do with generalized linear mixed models -- or even
  with generalized linear models.
  
\item
  \pkg{pbkrtest} also provides the parametric bootstrap $p$-value.
  
  Computationally somewhat demanding, but can be parallelized.
\end{itemize}

  
## Parametric bootstrap



We have two competing models; a large model $f_1(y; \theta)$
and a null model $f_0(y; \theta_0)$; the null model is a submodel of the large model.

* The $p$ value for a composite hypothesis is
$$
 p = \sup_{\theta \in \Theta_0} Pr_{\theta}(T \ge t_{obs})
$$
where the $\sup$ is taken under the hypothesis.

* We can (usually) not evaluate the $\sup$ in practice, so instead we do:
$$
 p^{PB} = Pr_{\hat\theta}(T \ge t_{obs})
$$

* In practice we approximate $p^{PB}$ as

	+ Draw $B$ parametric bootstrap samples
$t^1, \dots, t^B$ under the fitted null model $\hat \theta_0$.

	+ Fit the large and the null model to each of these datasets;

	+ Calculate the LR-test statistic for each simulated data; this
      gives reference distribution.

	+ Calculate how extreme the observed statistic is.

---

```{r, cache=TRUE}
lg2 <- update(lg2, REML=FALSE)
sm2 <- update(sm2, REML=FALSE)
# Observed test statistic:
t.obs <- 2 * (logLik(lg2) - logLik(sm2))
t.obs
# Reference distribution
set.seed(121315)
t.sim <- PBrefdist(lg2, sm2, nsim=2000)
# p-value
head(t.sim)
sum(t.sim >= t.obs) / length(t.sim)
# compare with X^2 dist
1 - pchisq(t.obs, df=1)
```

---

Interesting to overlay limiting $\chi^2_1$
distribution and simulated reference distribution.

Bootstrap reference distribution has heavier tail giving larger $p$-value.

```{r echo=FALSE, fig.width=4, fig.height=3}
t.sim2 <- t.sim[t.sim<10]
hist(t.sim2, breaks=40, prob=T, col="green")
abline(v=t.obs, col="red", lwd=3)
f <- function(x){dchisq(x, df=1)}
curve(f, 0, 20, add=TRUE, col="blue", lwd=2)
```



## Speedup I: Sequential $p$-value
\label{sec:seqp}

  Instead of simulating a fixed number of values $t^1, \dots, t^B$ for
  determining the reference distribution used for finding $p^{PB}$
  we may instead introduce a stopping rule saying \emph{simulate until we
  have found, say $h=20$ values $t^j$ larger than $t_{obs}$.} If $J$
  simulations are made then the reported $p$--value is $h/J$.

<!-- --- -->

<!-- The simulation of the reference distribution can be parallelized -->
<!-- onto different processors (happens by default): -->


```{r cache=TRUE}
spb <- seqPBmodcomp(lg2, sm2) 
spb
```

---

## Speedup II: Parallel computations

Parametric bootstrap is computationally demanding, but multiple cores can be exploited. Done by default on linux / mac platforms.

```{r cache=TRUE}
PBmodcomp(lg2, sm2) # Default: Use all cores (4 on my computer)
PBmodcomp(lg2, sm2, cl=1) # Use one core
```

---

On windows (in fact, work on all platforms):



```{r, eval=FALSE}
set.seed(121315)
library(parallel)
nc <- detectCores(); nc
clus <- makeCluster(rep("localhost", nc))
PBmodcomp(lg2, sm2, cl=clus)
```




## Speedup III: Parametric form of reference distribution:

  Estimating tail--probabilities will require more samples than
  estimating the mean (and variance) of the reference
  distribution.

Suggests to approximate simulated reference distribution with a known distribution so that fewer samples will suffice:

```{r cache=TRUE}
pb1 <- PBmodcomp(lg2, sm2, nsim=1000)
pb2 <- PBmodcomp(lg2, sm2, nsim=100)
summary(pb1) %>% as.data.frame
summary(pb2) %>% as.data.frame
```


## Why use parametric bootstrap

* Applies generally; in `pbkrtest` implemented for e.g. generalized
  linear mixed models (hwere random effects are on the linear
  predictor scale).

* Kenward-Roger does not readily scale to larger problems because of the computation of
  \begin{displaymath}
    G_j {\bm \Sigma}^{-1} G_j
  \end{displaymath}
  where $\Sigmab= \sum_i \sigma_i \bm G_i$.
  Can be space and time consuming!

* For example, in random regression models with few relatively long
time series. In this case simulation is faster.




## Simulation study


```{r}
dub
```

---

* Task: Test the hypothesis that there is no effect of treatment. How
  good are the various tests?

* Simulate data $1000$ times with divine insight: there is no effect of treatment.

* Test the hypothesis e.g. at level $5\%$. If test
  has correct nominal level we shall reject about $50$ times.

* If hypothesis is rejected e.g. $100$ times then $p$ values are
  anti-conservative: Effects appear more significant than the really
  are. That is we draw "too strong" conclusions.


```{r echo=FALSE}
sim<-list(c(0.11, 0.24, 0.322), c(0.178, 0.282, 0.342), c(0.044, 0.152, 
0.24), c(0.012, 0.044, 0.114), c(0.008, 0.052, 0.108))
sim <- do.call(rbind, sim)
## 1: lnm + F
## 2: lnm + X^2
## 3: mixed + X^2
## 4: mixed + KR
## 5: mixed + LR

rownames(sim) <-
    c("lm+F", "lm+X2", "mixed+X2", "mixed+F-KR", "mixed+PB")
colnames(sim) <- c("0.010", "0.050", "0.100")
sim <- sim[c(2,1,3,4,5),]

```

```{r echo=FALSE}
kable(sim)
```




## Motivation: Sugar beets - A split--plot experiment

* Model how sugar percentage in sugar beets depends on
   harvest time and sowing time.
* Five sowing times ($s$) and two harvesting times ($h$).
* Experiment was laid out in three blocks ($b$).


Experimental plan for sugar beets experiment


<!-- \begin{Verbatim}[fontsize=\tiny] -->

<!-- Sowing times: -->
<!--  1: 4/4, 2: 12/4, 3: 21/4, 4: 29/4, 5: 18/5 -->
<!-- Harvest times: -->
<!--  1: 2/10, 2: 21/10 -->

<!-- Plot allocation: -->
<!--       |  Block 1           |  Block 2           |  Block 3           | -->
<!--       +--------------------|--------------------|--------------------+ -->
<!-- Plot  | h1  h1  h1  h1  h1 | h2  h2  h2  h2  h2 | h1  h1  h1  h1  h1 | Harvest time -->
<!-- 1-15  | s3  s4  s5  s2  s1 | s3  s2  s4  s5  s1 | s5  s2  s3  s4  s1 | Sowing time -->
<!--       |--------------------|--------------------|--------------------| -->
<!-- Plot  | h2  h2  h2  h2  h2 | h1  h1  h1  h1  h1 | h2  h2  h2  h2  h2 | Harvest time -->
<!-- 16-30 | s2  s1  s5  s4  s3 | s4  s1  s3  s2  s5 | s1  s4  s3  s2  s5 | Sowing time -->
<!--       +--------------------|--------------------|--------------------+ -->
<!-- \end{Verbatim} -->


<!-- Plot allocation: -->
<!--       |  Block 1           |  Block 2           |  Block 3           | -->
<!--       +--------------------|--------------------|--------------------+ -->
<!-- Plot  | h1  h1  h1  h1  h1 | h2  h2  h2  h2  h2 | h1  h1  h1  h1  h1 | Harvest time -->
<!-- 1-15  | s3  s4  s5  s2  s1 | s3  s2  s4  s5  s1 | s5  s2  s3  s4  s1 | Sowing time -->
<!--       |--------------------|--------------------|--------------------| -->
<!-- Plot  | h2  h2  h2  h2  h2 | h1  h1  h1  h1  h1 | h2  h2  h2  h2  h2 | Harvest time -->
<!-- 16-30 | s2  s1  s5  s4  s3 | s4  s1  s3  s2  s5 | s1  s4  s3  s2  s5 | Sowing time -->
<!--       +--------------------|--------------------|--------------------+ -->
<!-- \end{Verbatim} -->






```{r size="scriptsize"}
# Plot allocation:
#       |  Block 1       |  Block 2       |  Block 3       |
#       +----------------|----------------|----------------+
# Plot  | h1 h1 h1 h1 h1 | h2 h2 h2 h2 h2 | h1 h1 h1 h1 h1 | Harvest time
# 1-15  | s3 s4 s5 s2 s1 | s3 s2 s4 s5 s1 | s5 s2 s3 s4 s1 | Sowing time
#       |----------------|----------------|----------------|
# Plot  | h2 h2 h2 h2 h2 | h1 h1 h1 h1 h1 | h2 h2 h2 h2 h2 | Harvest time
# 16-30 | s2 s1 s5 s4 s3 | s4 s1 s3 s2 s5 | s1 s4 s3 s2 s5 | Sowing time
#       +----------------|----------------|----------------+
```






## beets data

```{r}
data(beets, package='pbkrtest')
head(beets)
```



```{r, fig.height=3}
par(mfrow=c(1,2))
with(beets, interaction.plot(sow, harvest, sugpct))
with(beets, interaction.plot(sow, harvest, yield))
```

---

* For simplicity assume  no interaction between sowing
    and harvesting times.

* A typical model for such an experiment would be:
    \begin{equation}
      \label{eq:beetsmodel1}
      y_{hbs} = \mu + \alpha_h + \beta_b + \gamma_s + U_{hb} + \epsilon_{hbs},
    \end{equation}
    where $U_{hb} \sim N(0,\omega^2)$ and $\epsilon_{hbs}\sim
    N(0,\sigma^2)$.
    

* Notice that $U_{hb}$ describes the random variation
between whole--plots (within blocks).



---

Using `lmer()` from lme4 we can
 test for no effect of sowing and harvest time as:


```{r}
beet.lg <- lmer(sugpct ~ block + sow + harvest + 
                      (1 | block:harvest), data=beets, REML=FALSE)
beet.noh <- update(beet.lg, .~. - harvest)
beet.nos  <- update(beet.lg, .~. - sow)
```

Both factors appear highly significant
```{r}
anova(beet.lg, beet.noh)  %>% as.data.frame
anova(beet.lg, beet.nos)  %>% as.data.frame
```

However, the LRT based $p$--values are anti--conservative: the effect
of harvest appears stronger than it is.


---

As the design is balanced we may make F--tests for each of the effects
as:

```{r}
beets$bh <- with(beets, interaction(block, harvest))
summary(aov(sugpct ~ block + sow + harvest + 
                Error(bh), data=beets))
```

Notice: the F--statistics are $F_{1,2}$ for harvest time and $F_{4,20}$ for
sowing time.

---

```{r}
set.seed("260618")
KRmodcomp(beet.lg, beet.noh)
PBmodcomp(beet.lg, beet.noh)

```

```{r}
seqPBmodcomp(beet.lg, beet.noh)
```


## Final remarks

* Satterthwaite approximation of degrees of freedom on its way in
  `pbkrtest`. Faster to compute than Kenward-Roger scales to larger
  problems.

* `pbkrtest` available on CRAN [https://cran.r-project.org/package=pbkrtest](https://cran.r-project.org/package=pbkrtest)

* devel version on github: `devtools::install_github(hojsgaard/pbkrtest)`

* `pbkrtest` described in Ulrich Halekoh and SH (2014) [A Kenward-Roger Approximation and Parametric Bootstrap Methods for Tests in Linear Mixed Models The R Package pbkrtest](https://www.jstatsoft.org/article/view/v059i09); Journal of Statistical Software, Vol 59.



Thanks for your attention!
