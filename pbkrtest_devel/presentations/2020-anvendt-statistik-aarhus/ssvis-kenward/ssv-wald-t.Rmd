---
title: Hvad skal vi med t-testet
author: |
  | Søren Højsgaard^[University of Aalborg, Denmark]
date: "June 12, 2018"
output:
#  html_document:
#    standalone: true
#    self-contained: true
  pdf_document:
    includes:
      in_header: preamble.txt
    toc: true
    toc_depth: 3
    keep_tex: true
#  beamer_presentation:
#    includes:
#      in_header: preamble.txt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, size="scriptsize", message=FALSE, cache=TRUE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
library(lme4)
library(doBy)
library(broom)
library(magrittr)
library(pbkrtest) 
options("digits"=4)
options("show.signif.stars"=FALSE)
```


## Hvad skal vi med t-testet?

Betragt en lineær regressionsmodel
$$
 y_i = \alpha + \beta x_i + e_i
$$

Variansen på estimatet $\hat\beta$ er $\sigma^2_\beta=\sigma^2 c$, hvor 
$\sigma^2$ er residualvariansen, dvs. variansen på $y_i$'erne (eller på $e_i$'erne om man vil, det er det samme) og
$c$ er en konstant hvis værdi er let at regne ud, men vi skal ikke
gøre dette her. 

## $t$--testet og $z$--testet

I introducerende statistikkurser lærer man, at når man skal teste
hypotesen at $\beta=\beta_0$ (hvor $\beta_0$ er et givet tal) så skal man skelne mellem situationerne hvor $\sigma^2$ er kendt og ukendt (i praksis er $\sigma^2$ næsten aldrig kendt).

Hvis $\sigma^2$ er kendt, så ser vi på *teststørrelsen*
$$
 z = \frac{\hat\beta-\beta_0}{\sqrt{\sigma^2_\beta}} = \frac{\hat\beta-\beta_0}{\sigma_\beta}
$$
Dvs. $z$ "måler" hvor mange standardafvigelser estimatet $\hat\beta$ ligger fra $\beta_0$. 
Numerisk store værdier af $z$ bevirker, at man tvivler på hypotesen.


så kan man se på
en $t$--test størrelse

$$
t = \frac{\hat\beta-\beta_0}{\sqrt{\hat\sigma^2_\beta}}
$$

Dvs. $t$ "måler" hvor mange standardafvigelser estimatet $\hat\beta$ ligger fra $\beta_0$. Numerisk store værdier af $t$ bevirker, at man tvivler på hypotesen.

Hvis hypotesen er sand, så skal $t$ vurderes i en
$t$--fordeling med $N-1$ frihedsgrader, hvor $N$ er antal
observationer. Numerisk store værdier af $t$ er får os til at tvivle
på hypotesen.

Hvis $\sigma^2$ er kendt (det er $\sigma^2$ næsten aldrig i praksis, men lad os lige lade som om) så er der ingen grund til at bruge
et estimat for $\sigma^2$ ovenfor. I så fald bliver variansen på $\hat\beta$ givet ved $\sigma^2_\beta=\sigma^2 c$. 
I dette tilfælde bliver $t$--test størrelsens pendant

Hvis hypotesen er sand, så skal $u$ vurderes i en
$N(0,1)$--fordeling. Numerisk store værdier af $t$ er får os til at tvivle
på hypotesen.

I et introducerende statistikkursus vil man ofte lære, at hvis antallet af frihedsgrader $f$ er stort så vil en $t_f$-fordeling (en $t$ fordeling med $f$ frihedsgrader) ligne en standard normal fordeling (en $N(0,1)$ fordeling) så meget, at man simpelthen kan vurdere $t$ i en $N(0,1)$ fordeling. Dette svarer præcist til at lade som om at $\hat\sigma^2_\beta$ er den sand værdi for spredningen på $\hat\beta$. 

Hvis man vurderer $t$ i en $N(0,1)$ fordeling når der er ganske få frihedsgrader, så kan resultaterne derimod blive forkerte - meget forkert endda. 

For at illustrere dette laver vi følgende tankeeksperiement: Vi skal sammenligne en behandling for en given sygdom med en placebo, så vi har to grupper med $M$ patienter i hver. Dette kan håndteres med en lineær regressionsmodel: Lad $x$ være en variabel der er $1$ for de patienter, der har fået behandling og $0$ for dem, der har fået placebo. Så vil middelværdien for patienter med placebo være $\alpha$ mens middelværdien for patienter med behandlingen være $\alpha+\beta$ så behandlingseffekten vil være $\beta$. 
Da vi laver eksperimentet selv, så står det os frit for at vælge $\beta$ og vi sætter $\beta=0$, således der ikke er en behandlingseffekt. Hvis vi tester hypotesen $\beta=0$ på niveau $5\%$ så vil vi med $5\%$ sandsynlighed forkaste hypotesen (der jo er sand, for sådan er eksperimentet lavet). Lad os gentage eksperimentet $1000$ gange. Så ville vi skulle forkaste hypotesen omkring $50$ gange - ellers er der noget helt galt.

Hvis $M$ (antal patienter per gruppe) er lille og vi laver et normalfordelingstest, så vil vi ikke få forkastet hypotesen i $5\%$ af tilfældene men måske i $10\%$ af tilfældene. 

```{r}
M <- 3
beta <-0
mu <- c(rep(0, M), rep(beta, M))
grp <- factor(c(rep("placebo", M), rep("behandling", M)))

y <- rnorm(2 * M, mean=mu)
tb <- summary(lm(y ~ grp))$coef
tb <- as.data.frame(tb)
prt <-2 * (1 - pt(abs(tb$`t value`), df=2 * M - 2))
prn <- 2 * (1 - pnorm(abs(tb$`t value`)))
c(prt[2], prn[2])

do_sim <- function(){
  y <- rnorm(2 * M, mean=mu)
  tb <- summary(lm(y ~ grp))$coef
  tb <- as.data.frame(tb)
  prt <-2 * (1 - pt(abs(tb$`t value`), df=2 * M - 2))
  prn <- 2 * (1 - pnorm(abs(tb$`t value`)))
  c(prt[2], prn[2])
}

do_sim()

Nsim <- 1000
sim <- replicate(Nsim, do_sim())

sum(sim[1,] <= 0.05) / Nsim
sum(sim[2,] <= 0.05) / Nsim


```








## F-test og Wald-test

Et alternativt men ækvivalent test (giver samme resultat) er
et $F$--test:
$$
F = t^2 =\frac{(\hat\beta-\beta_0)^2}{\hat\sigma^2_\beta}
$$
er under hypotesen $F$-fordelt med een tællerfrihedsgrad og $N-1$ nævnerfrihedsgrader. 

$$
  W = u^2 =\frac{(\hat\beta-\beta_0)^2}{\sigma^2_\beta}
$$
Dette kaldes et Wald--test. Forskellen mellem et Wald-test og et $F$-test er altså, at i det første er variansen kendt; i $F$-test tages der højde for at variansen er estimeret fra data og den  usikkerhed i estimatet, der følger dermed. 



    





<!-- ## Example: Double registration in labs  -->


<!-- ```{r echo=FALSE} -->
<!-- dub= -->
<!-- structure(list(y1 = c(0.7, 1.01, -0.35, 0.39, -0.69, -0.06, -2.45,  -->
<!-- -1.8, 1.49, 1.53, 0.94, 1.02), y2 = c(0L, 0L, 0L, 2L, 1L, 0L,  -->
<!-- 0L, 2L, 4L, 5L, 2L, 0L), trt = structure(c(1L, 1L, 1L, 1L, 1L,  -->
<!-- 1L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("ctrl", "trt1"), class = "factor"),  -->
<!--     subj = structure(c(1L, 1L, 2L, 2L, 3L, 3L, 4L, 4L, 5L, 5L,  -->
<!--     6L, 6L), .Label = c("subj1", "subj2", "subj3", "subj4", "subj5",  -->
<!--     "subj6"), class = "factor")), .Names = c("y1", "y2", "grp",  -->
<!-- "subj"), row.names = c(NA, -12L), class = "data.frame") -->
<!-- dub$y1 <- round(dub$y1  + 1, 2) -->
<!-- dub$y1[-(1:6)] <- dub$y1[-(1:6)] + 2  -->
<!-- ``` -->

<!-- ![](img/bacteria-growing-in-petri-dishes.jpg){width=120px} -->


<!-- Clustered data:  -->

<!-- * Compare two groups (treatment with a control);  -->

<!-- * M units (petri plates, persons, animals...) per group;  -->

<!-- * Each unit is measured R times. Measurements on same unit are positively correlated. -->


<!-- --- -->

<!-- Simulated data: $N=3$ subjects per group, $R=2$ replicated measurements per subject. -->
<!-- ```{r} -->
<!-- dub -->
<!-- ``` -->



<!-- --- -->

<!-- Hvis data er korrelerede må man enten kan man vælge forskellige muligheder -->

<!-- * Forsøge, med et gammelt dansk ordvalg, at "komme om ved" afhængigheden. -->

<!-- * Tage højde for afhængigheden ved at lade den indgå i sin model -->

<!-- * Ignorere afhængigheden. -->

<!-- De første to muligheder er helt OK; den sidste er snyd - og det kan gå HELT GALT. -->

<!-- Hvis der er en korrelation, så er den oftest positiv. Problemet med at -->
<!-- ignorere problemet er kort fortalt, at man kommer til at "lade som om" -->
<!-- der er mere information i data end der i virkeligheden er. Det fører til -->

<!-- * standardafvigelser på parameterestimater bliver for små -->
<!-- * $p$-værdier bliver for små -->
<!-- * effekter kommer til at se stærkere ud end de i virkeligheden er. -->

<!-- Notice: -->

<!-- * Measuring the same unit many many times will make the dataset -->
<!--   larger, but will not really add many more chunks of information -->
<!--   (depending on the size of the within-subject correlation, of -->
<!--   course). -->
<!-- * Instead, more units are needed. -->

<!-- ```{r} -->
<!-- lg1 <- lm(y1 ~ grp, data=dub) -->
<!-- lg1 %>% summary %>% coef %>% as.data.frame -> tb1 -->
<!-- tb1$"Pr(>|X^2|)" = 1 - pchisq(tb1[,3]^2, df=1) -->
<!-- tb1 -->
<!-- ``` -->

<!-- Notice: the $t$-test "accounts for" the uncertainty in the estimate of the standard error. -->

<!-- --- -->

<!-- ### Analyser gennemsnit: At "komme om ved" problemet. -->

<!-- <!-- ```{r} --> -->
<!-- <!-- dub$E <- with(dub, interaction(grp, subj)) --> -->
<!-- <!-- lg <- aov(y1 ~ grp + Error(E), data=dub) --> -->
<!-- <!-- summary(lg)[[1]] --> -->
<!-- <!-- ``` --> -->


<!-- ```{r} -->
<!-- duba <- aggregate(y1 ~ grp + subj, FUN=mean, data=dub) -->
<!-- lm(y1 ~ grp, data=duba) %>% summary %>% coef -->
<!-- ``` -->

<!-- * Works fine (gives the correct test) in (nearly) balanced cases. -->

<!-- * Does not provide estimate of between and within subject variation (not necessarily severe problem here). -->

<!-- * Analyzing-the-average is often not a feasible strategy. -->

<!-- --- -->

<!-- ### Alternative: Random effects model -->

<!-- ```{r} -->
<!-- lg2 <- lmer(y1 ~ grp + (1|subj), data=dub) -->
<!-- tidy(lg2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- sm2 <- update(lg2, .~. -grp) -->
<!-- as.data.frame(anova(lg2, sm2)) -->
<!-- ``` -->

<!-- Notice: Test is based in the $\chi^2$ distribution (i.e.\ that the variance is known) -->

<!-- --- -->

<!-- ### Alternatives in the pbkrtest package: -->
<!-- ```{r cache=TRUE} -->
<!-- KRmodcomp(lg2, sm2) -->
<!-- PBmodcomp(lg2, sm2) -->
<!-- ``` -->

<!-- Notice: Same $p$-value as when analyzing average. -->


<!-- --- -->

<!-- ## The Kenward--Roger approach -->

<!-- ### The Kenward--Roger modification of the F--statistic -->

<!-- For multivariate normal data -->
<!-- \[ -->
<!--   Y_{n\times 1} \sim  N( X_{n\times p} \beta_{p\times 1}, \Sigma) -->
<!-- \] -->
<!-- we consider the test of the hypothesis -->
<!-- \[ -->
<!--   \Lb_{d \times p} (\betab -\bm \beta_0) = 0 -->
<!-- \] -->
<!-- <!-- where $\Lb$ is a regular matrix of estimable functions of $\bm \beta$. --> -->

<!-- With $\hat \betab \sim N_d(\betab, \bm\Phi)$, a Wald statistic is -->

<!-- $$ -->
<!--   W = [\Lb(\hat\betab - \betab_0)]\transp [L\bm\Phi L\transp]\inv [\Lb(\hat\betab - \betab_0)] -->
<!-- $$ -->
<!-- which is asymptotically $W \sim \chi^2_d$ under the null hypothesis. -->

<!-- --- -->

<!-- A scaled version of $W$ is -->
<!-- \begin{displaymath} -->
<!--   F = \frac{1}{d} W -->
<!-- \end{displaymath} -->


<!-- * Asymptotically $F \sim \frac{1}{d} \chi^2_d$ -->
<!-- under the null hypothesis -->

<!-- * Think of as the limiting distribution -->
<!-- of an $F_{d,m}$--distribution as $m\rightarrow \infty$ -->

<!-- * To account for the fact that $\bm\Phi=\var(\hat\beta)$ -->
<!-- is estimated from data, we must come up with a better estimate of the -->
<!-- denominator degrees of freedom $m$ (better than $m=\infty$). -->

<!-- * That was what Kenward and Roger worked on... -->

<!-- --- -->

<!-- The linear hypothesis $\Lb \betab = \betab_0$ -->
<!-- can be tested via the  Wald-type  statistic -->
<!-- \begin{gather*} -->
<!-- F= \frac{1}{r}(\hat \betab - \betab_0)^\top \Lb^\top   (\Lb^\top \bm \Phi(\ssb) \Lb)^{-1} -->
<!--  \Lb (\hat \betab - \betab_0) -->
<!-- \end{gather*} -->

<!-- \begin{itemize} -->
<!-- \item $\bm \Phi (\sigmab) = (\bm X^\top \bm \Sigma(\sigmab) \bm X)^{-1} \approx -->
<!-- \cov(\hat \betab)$, $\hat  \betab$ REML estimate of $\betab$ -->
<!-- \item $\ssb$: vector of REML estimates of the elements of $\Sigmab=\var(Y)$ -->
<!-- \end{itemize} -->

<!-- Kenward and Roger (1997) -->

<!-- * replaced $\bm \Phi$ by -->
<!-- an improved small sample approximation $\bm \Phi_A$ -->

<!-- * scaled $F$ by a factor $\lambda$ -->

<!-- * determined denominator degrees of freedom $m$ by matching moments of $F/\lambda$ with an $F_{d,m}$ distribution. -->


<!-- <!-- ## Kenward and Roger's modification --> -->

<!-- <!-- Kenward and Roger (1997) modify the test statistic --> -->
<!-- <!-- \begin{itemize} --> -->
<!-- <!-- \item $\bm \Phi$ is replaced by an improved small sample approximation $\bm \Phi_A$ --> -->
<!-- <!-- \end{itemize} --> -->

<!-- <!-- Furthermore --> -->
<!-- <!-- \begin{itemize} --> -->
<!-- <!-- \item the statistic $F$ is scaled by a factor $\lambda$, --> -->
<!-- <!-- \item denominator degrees of freedom $m$ are determined --> -->
<!-- <!-- \end{itemize} --> -->
<!-- <!-- such that the approximate expectation and variance are those of a $F_{d,m}$ distribution. --> -->

<!-- <!-- ## Restriction on covariance, some details --> -->

<!-- <!-- \begin{itemize} --> -->

<!-- <!-- \item Consider only situations where --> -->
<!-- <!-- \begin{gather*} --> -->
<!-- <!-- \Sigmab= \sum_i \sigma_i \bm G_i, \quad \bm G_i \, \text{known matrices} --> -->
<!-- <!-- \end{gather*} --> -->

<!-- <!-- \item Variance component and random coefficient models satisfy this --> -->
<!-- <!-- restriction. --> -->

<!-- <!-- \item $\bm \Phi_A(\ssb)$  depends now only on the first  partial derivatives of $\bm \Sigma^{-1}$: --> -->
<!-- <!-- \begin{displaymath} --> -->
<!-- <!-- \frac{\partial \bm \Sigmab^{-1}}{\partial \sigma_i} = - \bm \Sigma^{-1} --> -->
<!-- <!-- \frac{\partial \bm \Sigmab}{\partial \sigma_i} --> -->
<!-- <!-- \bm \Sigma^{-1}. --> -->
<!-- <!-- \end{displaymath} --> -->

<!-- <!-- \item $\bm \Phi_A(\ssb)$  depends also on $\var(\ssb)$. --> -->

<!-- <!-- \item Kenward and Roger propose to estimate --> -->
<!-- <!--   $\var(\ssb)$ via the  inverse expected information matrix. --> -->
<!-- <!-- \end{itemize} --> -->


<!-- ## Shortcommings of Kenward-Roger -->

<!-- \begin{itemize} -->
<!-- \item The Kenward--Roger approach is no panacea. -->
<!-- \item In the computations of the degrees of freedom we need to compute -->
<!--   \begin{displaymath} -->
<!--     G_j {\bm \Sigma}^{-1} G_j -->
<!--   \end{displaymath} -->
<!--   where $\Sigmab= \sum_i \sigma_i \bm G_i$. -->
<!--   Can be space and time consuming! -->
<!-- \item An alternative is a Sattherthwaite--kind approximation which is faster to -->
<!--   compute. Will come out in next release of \pkg{pbkrtest} (code not tested yet). -->
<!--   Way faster... -->

<!-- \item What to do with generalized linear mixed models -- or even -->
<!--   with generalized linear models. -->

<!-- \item -->
<!--   \pkg{pbkrtest} also provides the parametric bootstrap $p$-value. -->

<!--   Computationally somewhat demanding, but can be parallelized. -->
<!-- \end{itemize} -->


<!-- ## Parametric bootstrap -->



<!-- We have two competing models; a large model $f_1(y; \theta)$ -->
<!-- and a null model $f_0(y; \theta_0)$; the null model is a submodel of the large model. -->

<!-- * The $p$ value for a composite hypothesis is -->
<!-- $$ -->
<!--  p = \sup_{\theta \in \Theta_0} Pr_{\theta}(T \ge t_{obs}) -->
<!-- $$ -->
<!-- where the $\sup$ is taken under the hypothesis. -->

<!-- * We can (usually) not evaluate the $\sup$ in practice, so instead we do: -->
<!-- $$ -->
<!--  p^{PB} = Pr_{\hat\theta}(T \ge t_{obs}) -->
<!-- $$ -->

<!-- * In practice we approximate $p^{PB}$ as -->

<!-- 	+ Draw $B$ parametric bootstrap samples -->
<!-- $t^1, \dots, t^B$ under the fitted null model $\hat \theta_0$. -->

<!-- 	+ Fit the large and the null model to each of these datasets; -->

<!-- 	+ Calculate the LR-test statistic for each simulated data; this -->
<!--       gives reference distribution. -->

<!-- 	+ Calculate how extreme the observed statistic is. -->

<!-- --- -->

<!-- ```{r, cache=TRUE} -->
<!-- lg2 <- update(lg2, REML=FALSE) -->
<!-- sm2 <- update(sm2, REML=FALSE) -->
<!-- # Observed test statistic: -->
<!-- t.obs <- 2 * (logLik(lg2) - logLik(sm2)) -->
<!-- t.obs -->
<!-- # Reference distribution -->
<!-- set.seed(121315) -->
<!-- t.sim <- PBrefdist(lg2, sm2, nsim=2000) -->
<!-- # p-value -->
<!-- head(t.sim) -->
<!-- sum(t.sim >= t.obs) / length(t.sim) -->
<!-- # compare with X^2 dist -->
<!-- 1 - pchisq(t.obs, df=1) -->
<!-- ``` -->

<!-- --- -->

<!-- Interesting to overlay limiting $\chi^2_1$ -->
<!-- distribution and simulated reference distribution. -->

<!-- Bootstrap reference distribution has heavier tail giving larger $p$-value. -->

<!-- ```{r echo=FALSE, fig.width=4, fig.height=3} -->
<!-- t.sim2 <- t.sim[t.sim<10] -->
<!-- hist(t.sim2, breaks=40, prob=T, col="green") -->
<!-- abline(v=t.obs, col="red", lwd=3) -->
<!-- f <- function(x){dchisq(x, df=1)} -->
<!-- curve(f, 0, 20, add=TRUE, col="blue", lwd=2) -->
<!-- ``` -->



<!-- ## Speedup I: Sequential $p$-value -->
<!-- \label{sec:seqp} -->

<!--   Instead of simulating a fixed number of values $t^1, \dots, t^B$ for -->
<!--   determining the reference distribution used for finding $p^{PB}$ -->
<!--   we may instead introduce a stopping rule saying \emph{simulate until we -->
<!--   have found, say $h=20$ values $t^j$ larger than $t_{obs}$.} If $J$ -->
<!--   simulations are made then the reported $p$--value is $h/J$. -->

<!-- <!-- --- --> -->

<!-- <!-- The simulation of the reference distribution can be parallelized --> -->
<!-- <!-- onto different processors (happens by default): --> -->


<!-- ```{r cache=TRUE} -->
<!-- spb <- seqPBmodcomp(lg2, sm2)  -->
<!-- spb -->
<!-- ``` -->

<!-- --- -->

<!-- ## Speedup II: Parallel computations -->

<!-- Parametric bootstrap is computationally demanding, but multiple cores can be exploited. Done by default on linux / mac platforms. -->

<!-- ```{r cache=TRUE} -->
<!-- PBmodcomp(lg2, sm2) # Default: Use all cores (4 on my computer) -->
<!-- PBmodcomp(lg2, sm2, cl=1) # Use one core -->
<!-- ``` -->

<!-- --- -->

<!-- On windows (in fact, work on all platforms): -->



<!-- ```{r, eval=FALSE} -->
<!-- set.seed(121315) -->
<!-- library(parallel) -->
<!-- nc <- detectCores(); nc -->
<!-- clus <- makeCluster(rep("localhost", nc)) -->
<!-- PBmodcomp(lg2, sm2, cl=clus) -->
<!-- ``` -->




<!-- ## Speedup III: Parametric form of reference distribution: -->

<!--   Estimating tail--probabilities will require more samples than -->
<!--   estimating the mean (and variance) of the reference -->
<!--   distribution. -->

<!-- Suggests to approximate simulated reference distribution with a known distribution so that fewer samples will suffice: -->

<!-- ```{r cache=TRUE} -->
<!-- pb1 <- PBmodcomp(lg2, sm2, nsim=1000) -->
<!-- pb2 <- PBmodcomp(lg2, sm2, nsim=100) -->
<!-- summary(pb1) %>% as.data.frame -->
<!-- summary(pb2) %>% as.data.frame -->
<!-- ``` -->


<!-- ## Why use parametric bootstrap -->

<!-- * Applies generally; in `pbkrtest` implemented for e.g. generalized -->
<!--   linear mixed models (hwere random effects are on the linear -->
<!--   predictor scale). -->

<!-- * Kenward-Roger does not readily scale to larger problems because of the computation of -->
<!--   \begin{displaymath} -->
<!--     G_j {\bm \Sigma}^{-1} G_j -->
<!--   \end{displaymath} -->
<!--   where $\Sigmab= \sum_i \sigma_i \bm G_i$. -->
<!--   Can be space and time consuming! -->

<!-- * For example, in random regression models with few relatively long -->
<!-- time series. In this case simulation is faster. -->




<!-- ## Simulation study -->


<!-- ```{r} -->
<!-- dub -->
<!-- ``` -->

<!-- --- -->

<!-- * Task: Test the hypothesis that there is no effect of treatment. How -->
<!--   good are the various tests? -->

<!-- * Simulate data $1000$ times with divine insight: there is no effect of treatment. -->

<!-- * Test the hypothesis e.g. at level $5\%$. If test -->
<!--   has correct nominal level we shall reject about $50$ times. -->

<!-- * If hypothesis is rejected e.g. $100$ times then $p$ values are -->
<!--   anti-conservative: Effects appear more significant than the really -->
<!--   are. That is we draw "too strong" conclusions. -->


<!-- ```{r echo=FALSE} -->
<!-- sim<-list(c(0.11, 0.24, 0.322), c(0.178, 0.282, 0.342), c(0.044, 0.152,  -->
<!-- 0.24), c(0.012, 0.044, 0.114), c(0.008, 0.052, 0.108)) -->
<!-- sim <- do.call(rbind, sim) -->
<!-- ## 1: lnm + F -->
<!-- ## 2: lnm + X^2 -->
<!-- ## 3: mixed + X^2 -->
<!-- ## 4: mixed + KR -->
<!-- ## 5: mixed + LR -->

<!-- rownames(sim) <- -->
<!--     c("lm+F", "lm+X2", "mixed+X2", "mixed+F-KR", "mixed+PB") -->
<!-- colnames(sim) <- c("0.010", "0.050", "0.100") -->
<!-- sim <- sim[c(2,1,3,4,5),] -->

<!-- ``` -->

<!-- ```{r echo=FALSE} -->
<!-- kable(sim) -->
<!-- ``` -->




<!-- ## Motivation: Sugar beets - A split--plot experiment -->

<!-- * Model how sugar percentage in sugar beets depends on -->
<!--    harvest time and sowing time. -->
<!-- * Five sowing times ($s$) and two harvesting times ($h$). -->
<!-- * Experiment was laid out in three blocks ($b$). -->


<!-- Experimental plan for sugar beets experiment -->


<!-- <!-- \begin{Verbatim}[fontsize=\tiny] --> -->

<!-- <!-- Sowing times: --> -->
<!-- <!--  1: 4/4, 2: 12/4, 3: 21/4, 4: 29/4, 5: 18/5 --> -->
<!-- <!-- Harvest times: --> -->
<!-- <!--  1: 2/10, 2: 21/10 --> -->

<!-- <!-- Plot allocation: --> -->
<!-- <!--       |  Block 1           |  Block 2           |  Block 3           | --> -->
<!-- <!--       +--------------------|--------------------|--------------------+ --> -->
<!-- <!-- Plot  | h1  h1  h1  h1  h1 | h2  h2  h2  h2  h2 | h1  h1  h1  h1  h1 | Harvest time --> -->
<!-- <!-- 1-15  | s3  s4  s5  s2  s1 | s3  s2  s4  s5  s1 | s5  s2  s3  s4  s1 | Sowing time --> -->
<!-- <!--       |--------------------|--------------------|--------------------| --> -->
<!-- <!-- Plot  | h2  h2  h2  h2  h2 | h1  h1  h1  h1  h1 | h2  h2  h2  h2  h2 | Harvest time --> -->
<!-- <!-- 16-30 | s2  s1  s5  s4  s3 | s4  s1  s3  s2  s5 | s1  s4  s3  s2  s5 | Sowing time --> -->
<!-- <!--       +--------------------|--------------------|--------------------+ --> -->
<!-- <!-- \end{Verbatim} --> -->


<!-- <!-- Plot allocation: --> -->
<!-- <!--       |  Block 1           |  Block 2           |  Block 3           | --> -->
<!-- <!--       +--------------------|--------------------|--------------------+ --> -->
<!-- <!-- Plot  | h1  h1  h1  h1  h1 | h2  h2  h2  h2  h2 | h1  h1  h1  h1  h1 | Harvest time --> -->
<!-- <!-- 1-15  | s3  s4  s5  s2  s1 | s3  s2  s4  s5  s1 | s5  s2  s3  s4  s1 | Sowing time --> -->
<!-- <!--       |--------------------|--------------------|--------------------| --> -->
<!-- <!-- Plot  | h2  h2  h2  h2  h2 | h1  h1  h1  h1  h1 | h2  h2  h2  h2  h2 | Harvest time --> -->
<!-- <!-- 16-30 | s2  s1  s5  s4  s3 | s4  s1  s3  s2  s5 | s1  s4  s3  s2  s5 | Sowing time --> -->
<!-- <!--       +--------------------|--------------------|--------------------+ --> -->
<!-- <!-- \end{Verbatim} --> -->






<!-- ```{r size="scriptsize"} -->
<!-- # Plot allocation: -->
<!-- #       |  Block 1       |  Block 2       |  Block 3       | -->
<!-- #       +----------------|----------------|----------------+ -->
<!-- # Plot  | h1 h1 h1 h1 h1 | h2 h2 h2 h2 h2 | h1 h1 h1 h1 h1 | Harvest time -->
<!-- # 1-15  | s3 s4 s5 s2 s1 | s3 s2 s4 s5 s1 | s5 s2 s3 s4 s1 | Sowing time -->
<!-- #       |----------------|----------------|----------------| -->
<!-- # Plot  | h2 h2 h2 h2 h2 | h1 h1 h1 h1 h1 | h2 h2 h2 h2 h2 | Harvest time -->
<!-- # 16-30 | s2 s1 s5 s4 s3 | s4 s1 s3 s2 s5 | s1 s4 s3 s2 s5 | Sowing time -->
<!-- #       +----------------|----------------|----------------+ -->
<!-- ``` -->






<!-- ## beets data -->

<!-- ```{r} -->
<!-- data(beets, package='pbkrtest') -->
<!-- head(beets) -->
<!-- ``` -->



<!-- ```{r, fig.height=3} -->
<!-- par(mfrow=c(1,2)) -->
<!-- with(beets, interaction.plot(sow, harvest, sugpct)) -->
<!-- with(beets, interaction.plot(sow, harvest, yield)) -->
<!-- ``` -->

<!-- --- -->

<!-- * For simplicity assume  no interaction between sowing -->
<!--     and harvesting times. -->

<!-- * A typical model for such an experiment would be: -->
<!--     \begin{equation} -->
<!--       \label{eq:beetsmodel1} -->
<!--       y_{hbs} = \mu + \alpha_h + \beta_b + \gamma_s + U_{hb} + \epsilon_{hbs}, -->
<!--     \end{equation} -->
<!--     where $U_{hb} \sim N(0,\omega^2)$ and $\epsilon_{hbs}\sim -->
<!--     N(0,\sigma^2)$. -->


<!-- * Notice that $U_{hb}$ describes the random variation -->
<!-- between whole--plots (within blocks). -->



<!-- --- -->

<!-- Using `lmer()` from lme4 we can -->
<!--  test for no effect of sowing and harvest time as: -->


<!-- ```{r} -->
<!-- beet.lg <- lmer(sugpct ~ block + sow + harvest +  -->
<!--                       (1 | block:harvest), data=beets, REML=FALSE) -->
<!-- beet.noh <- update(beet.lg, .~. - harvest) -->
<!-- beet.nos  <- update(beet.lg, .~. - sow) -->
<!-- ``` -->

<!-- Both factors appear highly significant -->
<!-- ```{r} -->
<!-- anova(beet.lg, beet.noh)  %>% as.data.frame -->
<!-- anova(beet.lg, beet.nos)  %>% as.data.frame -->
<!-- ``` -->

<!-- However, the LRT based $p$--values are anti--conservative: the effect -->
<!-- of harvest appears stronger than it is. -->


<!-- --- -->

<!-- As the design is balanced we may make F--tests for each of the effects -->
<!-- as: -->

<!-- ```{r} -->
<!-- beets$bh <- with(beets, interaction(block, harvest)) -->
<!-- summary(aov(sugpct ~ block + sow + harvest +  -->
<!--                 Error(bh), data=beets)) -->
<!-- ``` -->

<!-- Notice: the F--statistics are $F_{1,2}$ for harvest time and $F_{4,20}$ for -->
<!-- sowing time. -->

<!-- --- -->

<!-- ```{r} -->
<!-- set.seed("260618") -->
<!-- KRmodcomp(beet.lg, beet.noh) -->
<!-- PBmodcomp(beet.lg, beet.noh) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- seqPBmodcomp(beet.lg, beet.noh) -->
<!-- ``` -->


<!-- ## Final remarks -->

<!-- * Satterthwaite approximation of degrees of freedom on its way in -->
<!--   `pbkrtest`. Faster to compute than Kenward-Roger scales to larger -->
<!--   problems. -->

<!-- * `pbkrtest` available on CRAN [https://cran.r-project.org/package=pbkrtest](https://cran.r-project.org/package=pbkrtest) -->

<!-- * devel version on github: `devtools::install_github(hojsgaard/pbkrtest)` -->

<!-- * `pbkrtest` described in Ulrich Halekoh and SH (2014) [A Kenward-Roger Approximation and Parametric Bootstrap Methods for Tests in Linear Mixed Models The R Package pbkrtest](https://www.jstatsoft.org/article/view/v059i09); Journal of Statistical Software, Vol 59. -->



<!-- Thanks for your attention! -->
